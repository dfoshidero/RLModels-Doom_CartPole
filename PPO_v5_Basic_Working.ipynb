{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0547f-535d-49a1-a0cb-2df24c584a95",
   "metadata": {},
   "source": [
    "## Initialize VizDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cddcc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vizdoom in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from vizdoom) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from vizdoom) (0.29.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from vizdoom) (2.5.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==1.10.1+cu113 (from versions: 1.11.0, 1.11.0+cu113, 1.12.0, 1.12.0+cu113, 1.12.1, 1.12.1+cu113, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0)\n",
      "ERROR: No matching distribution found for torch==1.10.1+cu113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: pyglet==1.5.11 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (1.5.11)\n",
      "Requirement already satisfied: joblib in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "#necessary\n",
    "!pip install vizdoom\n",
    "!pip install opencv-python\n",
    "!pip install pandas\n",
    "!pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip install gym\n",
    "!pip install pyglet==1.5.11\n",
    "!pip install joblib\n",
    "\n",
    "# also need to install pytorch-cpu on anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34143102-2b66-4ee2-9f76-f6db917d2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f8b51-d30c-4c9f-a8b1-8e24b891a576",
   "metadata": {},
   "source": [
    "## Make it a Gym Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df5aed06-301c-43ff-a0ab-54dca32e78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2\n",
    "\n",
    "LEVEL = 'defend_the_center'\n",
    "DOOM_SKILL = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47d02cc2-14b1-4eb2-a032-add0d7ed26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('VizDoom/scenarios/basic.cfg')\n",
    "        \n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "    \n",
    "    def step(self, action, frame_skip=4):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        total_reward = 0\n",
    "        for _ in range(frame_skip):\n",
    "            reward = self.game.make_action(actions[action], 5)  # Increase frame skip value here\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Break the loop if the game ends during frame skipping\n",
    "            if self.game.is_episode_finished():\n",
    "                break\n",
    "        \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            ammo = self.game.get_state().game_variables[0] \n",
    "            info = ammo\n",
    "        # If we don't have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\": info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, total_reward, done, truncated, info\n",
    "\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "            \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer  # Apply Grayscale\n",
    "\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48eacd6-a69d-4d14-b890-83f76c4a5e67",
   "metadata": {},
   "source": [
    "## Custom PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7728b4d-a680-4896-b0ea-a68a6683d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39228483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Algorithm\n",
    "\n",
    "\"\"\"\n",
    "https://www.youtube.com/watch?v=hlv79rcHws0\n",
    "\n",
    "https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/PolicyGradient/PPO/torch/main.py\n",
    "\"\"\"\n",
    "\n",
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, input_dims, alpha, fc1_dims=64, fc2_dims=64, checkpoint_dir='tmp/ppo'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, 'actor_torch_ppo')\n",
    "        os.makedirs(os.path.dirname(self.checkpoint_file), exist_ok=True)\n",
    "        \n",
    "        total_input_size = int(T.prod(T.tensor(input_dims)))  # Flatten the input dimensions\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(total_input_size, fc1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dims, n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        if state.dim() > 1:\n",
    "            state = state.view(state.size(0), -1)  # Flatten the state\n",
    "        \n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "\n",
    "        return dist \n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, alpha, fc1_dims=64, fc2_dims=64, checkpoint_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, 'critic_torch_ppo')\n",
    "        os.makedirs(os.path.dirname(self.checkpoint_file), exist_ok=True)\n",
    "        \n",
    "        total_input_size = int(T.prod(T.tensor(input_dims)))  # Flatten the input dimensions\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(total_input_size, fc1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dims, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        if state.dim() > 1:\n",
    "            state = state.view(state.size(0), -1)  # Flatten the state\n",
    "        \n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, n_actions, input_dims, gamma, alpha, gae_lambda,\n",
    "                 policy_clip, batch_size, N, n_epochs, entropy_coefficient):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_coefficient = entropy_coefficient\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
    "        self.critic = CriticNetwork(input_dims, alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []\n",
    "\n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('...saving models...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        print('...loading models...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "\n",
    "        dist = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def learn(self):\n",
    "        episode_actor_losses = []\n",
    "        episode_critic_losses = []\n",
    "        episode_values = []\n",
    "        \n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_probs_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "            \n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount * (reward_arr[k] + self.gamma * values[k+1] * (1 - int(dones_arr[k])) - values[k])\n",
    "                advantage[t] = a_t\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(old_probs_arr[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip, 1+self.policy_clip) * advantage[batch]\n",
    "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                # Calculate entropy bonus\n",
    "                entropy = dist.entropy().mean()\n",
    "                actor_loss -= self.entropy_coefficient * entropy  # Adding entropy bonus\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns - critic_value) ** 2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5 * critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "            # Collect losses for each batch\n",
    "            episode_actor_losses.append(actor_loss.item())\n",
    "            episode_critic_losses.append(critic_loss.item())\n",
    "            episode_values.append(critic_value.mean().item())\n",
    "\n",
    "        # Store average loss and value for the episode\n",
    "        self.actor_losses.append(np.mean(episode_actor_losses))\n",
    "        self.critic_losses.append(np.mean(episode_critic_losses))\n",
    "        self.values.append(np.mean(episode_values))\n",
    "\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "    # Reset stored data after each episode or training session\n",
    "    def reset_learning_debug_data(self):\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []\n",
    "\n",
    "    def reset_learning_debug_data(self):\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e7b4f",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d518ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hyperparameters(env, agent_params, n_games=10):\n",
    "    agent = PPOAgent(**agent_params)\n",
    "    total_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _, _ = agent.choose_action(observation)\n",
    "            observation, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    return avg_reward\n",
    "\n",
    "def hyperparameter_tuning():\n",
    "    #env = gym.make('CartPole-v1')\n",
    "    env = VizDoomGym(render=False)\n",
    "    learning_rates = [0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "    gammas = [0.99, 0.95]\n",
    "    policy_clips = [0.2, 0.4, 0.6, 0.8]\n",
    "    entropy_coefficients = [0.01, 0.05, 0.001, 0.005]\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for entropy_coeff in entropy_coefficients:\n",
    "        for alpha in learning_rates:\n",
    "            for gamma in gammas:\n",
    "                for policy_clip in policy_clips:\n",
    "                    agent_params = {\n",
    "                        'n_actions': env.action_space.n,\n",
    "                        'input_dims': env.observation_space.shape,\n",
    "                        'alpha': alpha,\n",
    "                        'gamma': gamma,\n",
    "                        'gae_lambda': 0.95,\n",
    "                        'policy_clip': policy_clip,\n",
    "                        'batch_size': 5,\n",
    "                        'N': 16,\n",
    "                        'n_epochs': 4,\n",
    "                        'entropy_coefficient': entropy_coeff\n",
    "                    }\n",
    "                    avg_reward = evaluate_hyperparameters(env, agent_params)\n",
    "                    results.append((avg_reward, agent_params))\n",
    "                    print(f'Tested {agent_params} -> Avg Reward: {avg_reward}')\n",
    "    \n",
    "    # Normalize rewards to sum to 1 to use as weights\n",
    "    total_reward = sum([result[0] for result in results])\n",
    "    weights = [result[0] / total_reward for result in results]\n",
    "\n",
    "    # Weighted average of parameters\n",
    "    avg_params = {}\n",
    "    for key in results[0][1].keys():\n",
    "        param_values = [params[key] for _, params in results]\n",
    "        if isinstance(param_values[0], float):  # Check if the parameter is float\n",
    "            avg_params[key] = sum(weight * params[key] for weight, (_, params) in zip(weights, results))\n",
    "        elif isinstance(param_values[0], int):  # Check if the parameter is integer\n",
    "            # Use weighted average and round it to get an integer\n",
    "            weighted_sum = sum(weight * params[key] for weight, (_, params) in zip(weights, results))\n",
    "            avg_params[key] = round(weighted_sum)\n",
    "        else:\n",
    "            # For non-numeric parameters, take the value from the best-performing configuration\n",
    "            avg_params[key] = results[0][1][key]  # Assumes results is sorted by performance, best first\n",
    "\n",
    "    print(f\"Weighted Average of Best Hyperparameters: {avg_params}\")\n",
    "    return avg_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7664881-22e2-42a6-8ab9-693188f1d2c3",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1566fc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 18.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 23.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 20.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 20.0\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 21.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 18.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 23.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 24.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 21.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 23.8\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 22.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 20.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 19.8\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 17.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 23.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 17.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 16.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 24.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 17.4\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 17.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 23.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 21.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 18.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 30.0\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 27.4\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 22.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 23.4\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 26.2\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 19.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 18.0\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 21.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 25.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 14.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 31.4\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 18.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 22.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 15.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 20.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 29.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01} -> Avg Reward: 21.0\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 18.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 21.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 22.0\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 18.2\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 22.0\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 23.2\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 26.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 21.0\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 21.2\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 19.0\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 23.4\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 28.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 26.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 20.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 23.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 17.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 26.2\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 28.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 24.2\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 19.8\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 22.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 22.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 20.8\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 19.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 19.2\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 16.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 20.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 20.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 23.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 17.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 23.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 19.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 20.2\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 19.8\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 14.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 23.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 21.8\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 17.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 27.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.05} -> Avg Reward: 30.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 17.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 22.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 19.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 26.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 21.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 22.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 24.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 17.0\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 16.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 23.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 25.0\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 22.8\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 22.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 25.2\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 19.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 16.8\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 16.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 26.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 23.8\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 22.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 24.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 23.4\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 21.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 16.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 23.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 20.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 22.8\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 22.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 19.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 14.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 26.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 21.4\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 17.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 24.2\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 18.8\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 18.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 21.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 18.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 31.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.001} -> Avg Reward: 21.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 18.8\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 23.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 17.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 26.4\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 21.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 19.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 20.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 21.0\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 15.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 17.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 20.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 22.0\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 21.0\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 20.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 19.4\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 21.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 21.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 22.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 25.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 30.2\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 21.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 20.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 17.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 21.2\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 23.4\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 16.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 25.4\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 19.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 20.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 24.7\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 19.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 5e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 26.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 18.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 19.4\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 19.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 18.1\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 24.9\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.4, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 19.6\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.6, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 19.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 1e-05, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.8, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.005} -> Avg Reward: 16.1\n",
      "Weighted Average of Best Hyperparameters: {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0003299892347977889, 'gamma': 0.9699371544951995, 'gae_lambda': 0.9500000000000003, 'policy_clip': 0.5046668606342741, 'batch_size': 5, 'N': 16, 'n_epochs': 4, 'entropy_coefficient': 0.01669252254873437}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x16000 and 4x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m n_steps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m---> 59\u001b[0m     action, prob, val \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     observation_, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     61\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[22], line 155\u001b[0m, in \u001b[0;36mPPOAgent.choose_action\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation):\n\u001b[0;32m    153\u001b[0m     state \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mtensor([observation], dtype\u001b[38;5;241m=\u001b[39mT\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 155\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(state)\n\u001b[0;32m    157\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 77\u001b[0m, in \u001b[0;36mActorNetwork.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     75\u001b[0m     state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mview(state\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the state\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m dist \u001b[38;5;241m=\u001b[39m Categorical(dist)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x16000 and 4x64)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def plot_learning_curves(timesteps, scores, figure_file):\n",
    "    os.makedirs(os.path.dirname(figure_file), exist_ok=True)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    cumulative_avg = np.cumsum(scores) / (np.arange(len(scores)) + 1)  # Compute the cumulative average\n",
    "\n",
    "    ax.plot(timesteps, scores, label='Reward per Episode', alpha=0.3)  # Plot raw scores\n",
    "    ax.plot(timesteps, cumulative_avg, label='Cumulative Average', color='red')  # Plot cumulative average\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.legend()\n",
    "    plt.title('Training Curve')\n",
    "    plt.savefig(figure_file)\n",
    "    plt.show()\n",
    "\n",
    "def plot_curve_smooth(x, scores, figure_file):\n",
    "    os.makedirs(os.path.dirname(figure_file), exist_ok=True)\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "\n",
    "    fig, ax = plt.subplots()  # Using subplots for consistency\n",
    "    ax.plot(x, running_avg, label='Running Average')\n",
    "    ax.set_xlabel('Episode')  # Align the x label\n",
    "    ax.set_ylabel('Reward')  # Align the y label\n",
    "    ax.legend()\n",
    "    plt.title('Running Average of Previous 100 Scores')\n",
    "    plt.savefig(figure_file)\n",
    "    plt.show()\n",
    "\n",
    "def smooth_curve(data, window=100):\n",
    "    \"\"\"Calculate the running average over a fixed window.\"\"\"\n",
    "    running_avg = np.zeros(len(data))\n",
    "    for i in range(len(data)):\n",
    "        running_avg[i] = np.mean(data[max(0, i-window):(i+1)])\n",
    "    return running_avg\n",
    "\n",
    "def plot_additional_metrics(episodes, actor_losses, critic_losses, values, figure_file_prefix):\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(7, 14))\n",
    "\n",
    "    # Ensure 'episodes' is a list or array of the right size\n",
    "    episodes = list(episodes) if len(episodes) == len(actor_losses) else list(range(len(actor_losses)))\n",
    "\n",
    "    # Smoothed data\n",
    "    smoothed_actor_losses = smooth_curve(actor_losses)\n",
    "    smoothed_critic_losses = smooth_curve(critic_losses)\n",
    "    smoothed_values = smooth_curve(values)\n",
    "\n",
    "    # Actor Loss\n",
    "    axs[0].plot(episodes, smoothed_actor_losses, label='Actor Loss', color='blue')\n",
    "    axs[0].set_title('Actor Loss Over Time (Smoothed)')\n",
    "    axs[0].set_xlabel('Episode')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Critic Loss\n",
    "    axs[1].plot(episodes, smoothed_critic_losses, label='Critic Loss', color='green')\n",
    "    axs[1].set_title('Critic Loss Over Time (Smoothed)')\n",
    "    axs[1].set_xlabel('Episode')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Value Estimates\n",
    "    axs[2].plot(episodes, smoothed_values, label='Value Estimates', color='red')\n",
    "    axs[2].set_title('Value Estimates Over Time (Smoothed)')\n",
    "    axs[2].set_xlabel('Episode')\n",
    "    axs[2].set_ylabel('Value')\n",
    "    axs[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{figure_file_prefix}_additional_metrics.png\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #env = gym.make('CartPole-v1')\n",
    "    env = VizDoomGym(render=True)\n",
    "    best_params = hyperparameter_tuning()\n",
    "\n",
    "    # Create the agent with the best hyperparameters\n",
    "    agent = PPOAgent(**best_params)\n",
    "    n_games = 100\n",
    "\n",
    "    # Directory for saving plots and model checkpoints\n",
    "    plot_dir = './logs/ppo/plots'\n",
    "    checkpoint_dir = './logs/ppo/checkpoints'\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    figure_file = os.path.join(plot_dir, 'test.png')\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    N = best_params.get('N', 16)\n",
    "\n",
    "    for i in range(n_games):\n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            observation_, reward, done, info, _ = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "            if n_steps % N == 0:\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_models()\n",
    "\n",
    "        print(\"Episode \", i, \": score %.1f\" % score, \" avg score %.1f\" % avg_score,\\\n",
    "                \" timesteps\", n_steps, \" learning steps\", learn_iters)\n",
    "    x = [i+1 for i in range(len(score_history))]\n",
    "    plot_curve_smooth(x, score_history, figure_file)\n",
    "    episodes = range(1, n_games + 1)\n",
    "    plot_additional_metrics(episodes, agent.actor_losses, agent.critic_losses, agent.values, plot_dir)\n",
    "\n",
    "    #report average values per however many runs\n",
    "\n",
    "    # changed to LeakyReLU for cases of negative input\n",
    "    # added entropy bonus to avoid agent converging too early, rewards more exploration - saw much better initial results\n",
    "    # added hp tuning to ensure best params - as model seemed quite sensitive\n",
    "    # updated hp tuning to use weighted average of model vals\n",
    "    # implemented frame skipping to speed up games\n",
    "    # add some information about CATASTROPHE FORGETTING in report. PPO is very susceptible and converges on bad policies often\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
