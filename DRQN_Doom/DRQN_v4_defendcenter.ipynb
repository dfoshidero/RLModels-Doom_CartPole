{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRQN Model for VizDoom Environment\n",
    "\n",
    "**Please expand the cells to view the code!**\n",
    "\n",
    "### Description\n",
    "This notebook implements a Deep Recurrent Q-Learning (DRQN) model for the VizDoom environment using TensorFlow.\n",
    "\n",
    "\n",
    "\n",
    "### Key Components:\n",
    "- **Prioritized Experience Replay**: Implementation of a sum tree structure for efficient sampling, promoting beneficial learning episodes.\n",
    "- **DRQN Model**: Including three convolutional layers and the Recurrent Neural Network. It helps to process the sequential data and hidden states and allows the agent to make decisions based on past and present views.\n",
    "- **Hyperparameters**: Configure the following settings to explore various performances:\n",
    "\n",
    "    - `learning_rate`: Determines the speed at which the model learns from the training data.\n",
    "    -  `batch_size`: Affects the overall training stability and efficiency.\n",
    "    - `gamma`: The reward discount factor, which affects how important future rewards are to the agent when making decisions.\n",
    "    - `explore_start` and `explore_stop`: Adjusting these values can impact the agent's ability to discover optimal policies.\n",
    "    \n",
    "- **Visualisations**: Set up of TensorBoard and pyplot to visualise the results.\n",
    "\n",
    "### How to Run:\n",
    "1. **Setup**: Ensure TensorFlow and necessary libraries are installed. Configure the VizDoom environment and paths appropriately. This was developed in **Python 3.10.14**.\n",
    "2. **Parameter Tuning**: Adjust the parameters in the 'Hyperparameters' section as needed.\n",
    "3. **Execution**: Run the cells sequentially to train the model. Monitor the output for performance metrics and visualisations.\n",
    "\n",
    "\n",
    "### References\n",
    "Simonini, T., 2018. Dueling Deep Q Learning with Doom (+ double DQNs and Prioritized Experience Replay).ipynb [Online]. Gist. Available from: https://gist.github.com/simoninithomas/d6adc6edb0a7f37d6323a5e3d2ab72ec [Accessed 7 May 2024].\n",
    "\n",
    "Ravichandiran, S., 2018. Hands-On-Reinforcement-Learning-With-Python/09. Playing Doom Game using DRQN/9.5 Doom Game Using DRQN.ipynb at 5440811df8da575eb41b131f897ddd8a7ce40d5f · sudharsan13296/Hands-On-Reinforcement-Learning-With-Python [Online]. GitHub. Available from: https://github.com/sudharsan13296/Hands-On-Reinforcement-Learning-With-Python/blob/5440811df8da575eb41b131f897ddd8a7ce40d5f/09.%20Playing%20Doom%20Game%20using%20DRQN/9.5%20Doom%20Game%20Using%20DRQN.ipynb [Accessed 7 May 2024].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up environment:**\n",
    "\n",
    "REWARDS:\n",
    "- death penalty = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "\n",
    "    # Load the correct configuration\n",
    "    game.load_config('defend_the_center.cfg')\n",
    "\n",
    "    # Load the correct scenario (in our case basic)\n",
    "    game.set_doom_scenario_path('defend_the_center.wad')\n",
    "\n",
    "    game.set_window_visible(False) #no pop out window\n",
    "    game.init()\n",
    "\n",
    "    # Here we create an hot encoded version of our actions (3 possible actions)\n",
    "    possible_actions = np.identity(3,dtype=int).tolist()\n",
    "\n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "files_and_directories = os.listdir(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess frame:**\n",
    "\n",
    "- Reduce the complexity of our states to reduce the computation time needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "    Take a frame.\n",
    "    Resize it.\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "\n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    Normalize it.\n",
    "\n",
    "    return preprocessed_frame\n",
    "\n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Crop the screen (remove part that contains no information)\n",
    "    # [Up: Down, Left: right]\n",
    "    cropped_frame = frame[15:-5, 20:-20]\n",
    "\n",
    "    # Check if the cropped frame has non-zero dimensions\n",
    "    if cropped_frame.size == 0:\n",
    "        # If the cropped frame has zero dimensions, return a default frame with zeros\n",
    "        return np.zeros((100, 120), dtype=np.float32)\n",
    "\n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame / 255.0\n",
    "\n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(cropped_frame, [100, 120])\n",
    "\n",
    "    return preprocessed_frame # 100x120x1 frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stack frames:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((100,120), dtype=int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    if state.size == 0:\n",
    "        # Return the existing stacked frames without modification\n",
    "        return np.stack(stacked_frames, axis=2), stacked_frames\n",
    "\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "\n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((100,120), dtype=int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [100,120,4]      # Our input is a stack of 4 frames hence 100x120x4 (Width, height, channels)\n",
    "action_size = game.get_available_buttons_size()              # 3 possible actions\n",
    "learning_rate =  0.00025      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 1000000         # Total episodes for training\n",
    "total_timesteps = 500000     #Total timestesp for training\n",
    "max_steps = 2100             # Max possible steps in an episode\n",
    "batch_size = 64\n",
    "\n",
    "# FIXED Q TARGETS HYPERPARAMETERS\n",
    "max_tau = 10000 #Tau is the C step where we update our target network\n",
    "\n",
    "# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.001            # minimum exploration probability\n",
    "decay_rate = 0.00005            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q LEARNING hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "## If you have GPU change to 1million\n",
    "pretrain_length = 10             # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 10 ##100000                 # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = False\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DRQN model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRQN:\n",
    "    def __init__(self, input_shape, num_actions, initial_learning_rate, name):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = initial_learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "        # Define placeholders\n",
    "        self.inputs_ = tf.placeholder(tf.float32, [None, *input_shape], name=name+\"_inputs\")\n",
    "        self.actions_ = tf.placeholder(tf.float32, [None, num_actions], name=name+\"_actions_\")\n",
    "        self.target_Q = tf.placeholder(tf.float32, [None], name=name+\"_target\")\n",
    "        self.ISWeights_ = tf.placeholder(tf.float32, [None, 1], name=name+\"_IS_weights\")\n",
    "        \n",
    "        # Define convolutional layers\n",
    "        with tf.variable_scope(name):\n",
    "            conv1 = tf.layers.conv2d(inputs=self.inputs_, filters=32, kernel_size=[8, 8], strides=[4, 4], padding=\"VALID\", activation=tf.nn.relu)\n",
    "            conv2 = tf.layers.conv2d(inputs=conv1, filters=64, kernel_size=[4, 4], strides=[2, 2], padding=\"VALID\", activation=tf.nn.relu)\n",
    "            conv3 = tf.layers.conv2d(inputs=conv2, filters=128, kernel_size=[4, 4], strides=[2, 2], padding=\"VALID\", activation=tf.nn.relu)\n",
    "        \n",
    "            # Flatten the output of the convolutional layers\n",
    "            flatten = tf.layers.flatten(conv3)\n",
    "        \n",
    "            # Recurrent Neural Network (RNN)\n",
    "            rnn_input = tf.expand_dims(flatten, axis=0)  # Add batch dimension\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(100)\n",
    "            state = cell.zero_state(batch_size=1, dtype=tf.float32)\n",
    "            outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_input, initial_state=state, dtype=tf.float32)\n",
    "            rnn_output = outputs[-1]\n",
    "        \n",
    "            # Feedforward layers\n",
    "            value_fc = tf.layers.dense(inputs=rnn_output, units=512, activation=tf.nn.elu)\n",
    "            value = tf.layers.dense(inputs=value_fc, units=1)\n",
    "        \n",
    "            advantage_fc = tf.layers.dense(inputs=rnn_output, units=512, activation=tf.nn.elu)\n",
    "            advantage = tf.layers.dense(inputs=advantage_fc, units=num_actions)\n",
    "        \n",
    "            # Aggregating layer\n",
    "            output = value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))\n",
    "            self.output = output  # Save the output as an attribute\n",
    "        \n",
    "            # predicted Q value\n",
    "            self.Q = tf.reduce_sum(tf.multiply(output, self.actions_), axis=1)\n",
    "        \n",
    "            # Loss calculation\n",
    "            self.absolute_errors = tf.abs(self.target_Q - self.Q)  # For updating Sumtree\n",
    "            self.loss = tf.reduce_mean(self.ISWeights_ * tf.square(self.target_Q - self.Q))\n",
    "        \n",
    "            # Optimizer\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DRQN(state_size, action_size, learning_rate, name=\"DQNetwork\")\n",
    "\n",
    "# Instantiate the target network\n",
    "TargetNetwork = DRQN(state_size, action_size, learning_rate, name=\"TargetNetwork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prioritised experience replay:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou:\n",
    "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "\n",
    "    \"\"\"\n",
    "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
    "\n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "\n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
    "        \"\"\"\n",
    "\n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "\n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "tree_index  0 0  0  We fill the leaves from left to right\n",
    "        \"\"\"\n",
    "\n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "\n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "\n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "\n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propagate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "\n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "\n",
    "            \"\"\"\n",
    "            Here we want to access the line above\n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "\n",
    "                0\n",
    "               / \\\n",
    "              1   2\n",
    "             / \\ / \\\n",
    "            3  4 5  [6]\n",
    "\n",
    "            If we are in leaf at index 6, we updated the priority score\n",
    "            We need then to update index 2 node\n",
    "            So tree_index = (tree_index - 1) // 2\n",
    "            tree_index = (6-1)//2\n",
    "            tree_index = 2 (because // round the result)\n",
    "            \"\"\"\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for experiences\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_index = 0\n",
    "\n",
    "        while True: # the while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "\n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "\n",
    "            else: # downward search, always search for a higher priority node\n",
    "\n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "\n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "\n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "\n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree\n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "\n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "\n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "\n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "\n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "\n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "\n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "\n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "\n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "\n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "\n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "\n",
    "            b_idx[i]= index\n",
    "\n",
    "            experience = [data]\n",
    "\n",
    "            memory_b.append(experience)\n",
    "\n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "\n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deal with the empty memory problem:**\n",
    "\n",
    "- Pre-populate our memory by taking random actions and storing the experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "\n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "\n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "\n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "\n",
    "        # Add experience to memory\n",
    "        #experience = np.hstack((state, [action, reward], next_state, done))\n",
    "\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "\n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "\n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "        # Add experience to memory\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "\n",
    "        # Our state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up Tensorboard:**\n",
    "\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/dddqn/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/Users/Lei/Documents/master/semester2/Reinforcement_Learning/cw2/tensorboard/drqn/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train our Agent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With ϵ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "\n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "\n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "\n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "\n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps us to copy one set of variables to another\n",
    "# In our case we use it when we want to copy the parameters of DQN to Target_network\n",
    "# Thanks of the very good implementation of Arthur Juliani https://github.com/awjuliani\n",
    "def update_target_graph():\n",
    "\n",
    "    # Get the parameters of our DQNNetwork\n",
    "    from_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, \"DQNetwork\")\n",
    "\n",
    "    # Get the parameters of our Target_network\n",
    "    to_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, \"TargetNetwork\")\n",
    "\n",
    "    op_holder = []\n",
    "\n",
    "    # Update our target_network parameters with DQNNetwork parameters\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "avg_episode_lengths = 0\n",
    "avg_episode_rewards = 0\n",
    "acc_timesteps = 0\n",
    "episode_lengths = []\n",
    "eepisode_rewards = []\n",
    "\n",
    "plot_dir = './logs/dddqn/plots'\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "figure_file = os.path.join(plot_dir, 'test.png')\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "        \n",
    "        # Set tau = 0\n",
    "        tau = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "        \n",
    "        # Update the parameters of our TargetNetwork with DQN_weights\n",
    "        update_target = update_target_graph()\n",
    "        sess.run(update_target)\n",
    "        while acc_timesteps < total_timesteps:\n",
    "            for episode in range(total_episodes):\n",
    "                try:\n",
    "                    if acc_timesteps > total_timesteps:\n",
    "                        break\n",
    "                    # Set step to 0\n",
    "                    step = 0\n",
    "                    \n",
    "                    # Initialize the rewards of the episode\n",
    "                    episode_rewards = []\n",
    "                    \n",
    "                    # Make a new episode and observe the first state\n",
    "                    game.new_episode()\n",
    "                    \n",
    "                    state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    # Remember that stack frame function also call our preprocess function.\n",
    "                    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "                \n",
    "                    while step < max_steps:\n",
    "                        if acc_timesteps > total_timesteps:\n",
    "                            break\n",
    "                        step += 1\n",
    "                        acc_timesteps += 1\n",
    "                        \n",
    "                        # Increase the C step\n",
    "                        tau += 1\n",
    "                        \n",
    "                        # Increase decay_step\n",
    "                        decay_step +=1\n",
    "                        \n",
    "                        # With ϵ select a random action atat, otherwise select a = argmaxQ(st,a)\n",
    "                        action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                        # Do the action\n",
    "                        reward = game.make_action(action)\n",
    "\n",
    "                        # Look if the episode is finished\n",
    "                        done = game.is_episode_finished()\n",
    "                        \n",
    "                        # Add the reward to total reward\n",
    "                        episode_rewards.append(reward)\n",
    "\n",
    "                        # If the game is finished\n",
    "                        if done:\n",
    "                            # the episode ends so no next state\n",
    "                            next_state = np.zeros((120,140), dtype=int)\n",
    "                            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                            savestep = step\n",
    "\n",
    "                            # Set step = max_steps to end the episode\n",
    "                            step = max_steps\n",
    "\n",
    "                            # Get the total reward of the episode\n",
    "                            total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                            print('Episode: {}'.format(episode), 'Step: {}'.format(savestep), 'Acc Step: {}'.format(acc_timesteps),\n",
    "                                    'Total reward: {}'.format(total_reward),\n",
    "                                    'Training loss: {:.4f}'.format(loss),\n",
    "                                    'Explore P: {:.4f}'.format(explore_probability))\n",
    "                            \n",
    "                            episode_lengths.append(savestep)\n",
    "                            eepisode_rewards.append(total_reward)\n",
    "                            # Add experience to memory\n",
    "                            experience = state, action, reward, next_state, done\n",
    "                            memory.store(experience)\n",
    "\n",
    "                        else:\n",
    "                            # Get the next state\n",
    "                            next_state = game.get_state().screen_buffer\n",
    "                            \n",
    "                            # Stack the frame of the next_state\n",
    "                            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                            \n",
    "\n",
    "                            # Add experience to memory\n",
    "                            experience = state, action, reward, next_state, done\n",
    "                            memory.store(experience)\n",
    "                            \n",
    "                            # st+1 is now our current state\n",
    "                            state = next_state\n",
    "\n",
    "\n",
    "                        ### LEARNING PART            \n",
    "                        # Obtain random mini-batch from memory\n",
    "                        tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "                        \n",
    "                        states_mb = np.array([each[0][0] for each in batch], ndmin=3)\n",
    "                        actions_mb = np.array([each[0][1] for each in batch])\n",
    "                        rewards_mb = np.array([each[0][2] for each in batch]) \n",
    "                        next_states_mb = np.array([each[0][3] for each in batch], ndmin=3)\n",
    "                        dones_mb = np.array([each[0][4] for each in batch])\n",
    "\n",
    "                        target_Qs_batch = []\n",
    "\n",
    "                        \n",
    "                        ### DOUBLE DQN Logic\n",
    "                        # Use DQNNetwork to select the action to take at next_state (a') (action with the highest Q-value)\n",
    "                        # Use TargetNetwork to calculate the Q_val of Q(s',a')\n",
    "                        \n",
    "                        # Get Q values for next_state \n",
    "                        q_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                        \n",
    "                        # Calculate Qtarget for all actions that state\n",
    "                        q_target_next_state = sess.run(TargetNetwork.output, feed_dict = {TargetNetwork.inputs_: next_states_mb})\n",
    "                        \n",
    "                        \n",
    "                        # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma * Qtarget(s',a') \n",
    "                        for i in range(0, len(batch)):\n",
    "                            terminal = dones_mb[i]\n",
    "                            \n",
    "                            # We got a'\n",
    "                            action = np.argmax(q_next_state[i])\n",
    "\n",
    "                            # If we are in a terminal state, only equals reward\n",
    "                            if terminal:\n",
    "                                target_Qs_batch.append(rewards_mb[i])\n",
    "                                \n",
    "                            else:\n",
    "                                # Take the Qtarget for action a'\n",
    "                                target = rewards_mb[i] + gamma * q_target_next_state[i][action]\n",
    "                                target_Qs_batch.append(target)\n",
    "                                \n",
    "\n",
    "                        targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                        \n",
    "                        _, loss, absolute_errors = sess.run([DQNetwork.optimizer, DQNetwork.loss, DQNetwork.absolute_errors],\n",
    "                                            feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                    DQNetwork.target_Q: targets_mb,\n",
    "                                                    DQNetwork.actions_: actions_mb,\n",
    "                                                    DQNetwork.ISWeights_: ISWeights_mb})\n",
    "                    \n",
    "                        \n",
    "                        \n",
    "                        # Update priority\n",
    "                        memory.batch_update(tree_idx, absolute_errors)\n",
    "                        \n",
    "                        \n",
    "                        # Write TF Summaries\n",
    "                        summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                            DQNetwork.target_Q: targets_mb,\n",
    "                                                            DQNetwork.actions_: actions_mb,\n",
    "                                                        DQNetwork.ISWeights_: ISWeights_mb})\n",
    "                        writer.add_summary(summary, episode)\n",
    "                        writer.flush()\n",
    "                        \n",
    "                        if tau > max_tau:\n",
    "                            # Update the parameters of our TargetNetwork with DQN_weights\n",
    "                            update_target = update_target_graph()\n",
    "                            sess.run(update_target)\n",
    "                            tau = 0\n",
    "                            print(\"Model updated\")\n",
    "                        \n",
    "                        # Save model every 16 timesteps (500 for demostrate purpose)\n",
    "                        if acc_timesteps % 16 == 0:\n",
    "                            avg_episode_lengths = [np.mean(episode_lengths[:i + 1]) for i in range(episode + 1)]\n",
    "                            avg_episode_rewards = [np.mean(eepisode_rewards[:i + 1]) for i in range(episode + 1)]\n",
    "                            save_path = saver.save(sess, \"/Users/Lei/Documents/master/semester2/Reinforcement_Learning/cw2/model/model.ckpt\")\n",
    "\n",
    "                            # Clear previous output\n",
    "                            clear_output(wait=True)\n",
    "                            \n",
    "                            # Plotting\n",
    "                            fig, ax1 = plt.subplots()\n",
    "\n",
    "                            ax1.set_xlabel('Number of Episodes')\n",
    "                            ax1.set_ylabel('Mean Episode Length', color='tab:blue')\n",
    "                            ax1.plot(range(1, episode + 2), avg_episode_lengths, color='tab:blue',label='Episode Length')\n",
    "                            ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "                            ax2 = ax1.twinx()\n",
    "                            ax2.set_ylabel('Mean Reward per Episode', color='tab:red')\n",
    "                            ax2.plot(range(1, episode + 2), avg_episode_rewards, color='tab:red',label='Episode Reward')\n",
    "                            ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "                            fig.tight_layout()\n",
    "                            ax1.legend(loc='upper right')\n",
    "                            ax2.legend(loc='upper left')\n",
    "                            plt.title('Training Curve')\n",
    "\n",
    "                            # Save the plot with a filename based on acc_timesteps\n",
    "                            save_filename = f\"performance_{acc_timesteps:06d}.png\"\n",
    "                            save_path = os.path.join(plot_dir, save_filename)\n",
    "                            plt.savefig(save_path)\n",
    "\n",
    "\n",
    "                            plt.show()\n",
    "                except tf.errors.InvalidArgumentError as e:\n",
    "                    print(\"An error occurred:\", e)\n",
    "                    print(\"Skipping the code and continuing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Agent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration (TESTING)\n",
    "    game.load_config('defend_the_center.cfg')\n",
    "    \n",
    "    # Load the correct scenario (in our case deadly_corridor scenario)\n",
    "    game.set_doom_scenario_path('defend_the_center.wad')\n",
    "    \n",
    "    game.init()    \n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"/Users/Lei/Documents/master/semester2/Reinforcement_Learning/cw2/model/model.ckpt\")\n",
    "    game.init()\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            ## EPSILON GREEDY STRATEGY\n",
    "            # Choose action a from state s using epsilon greedy.\n",
    "            ## First we randomize a number\n",
    "            exp_exp_tradeoff = np.random.rand()\n",
    "            \n",
    "\n",
    "            explore_probability = 0.01\n",
    "    \n",
    "            if (explore_probability > exp_exp_tradeoff):\n",
    "                # Make a random action (exploration)\n",
    "                action = random.choice(possible_actions)\n",
    "        \n",
    "            else:\n",
    "                # Get action from Q-network (exploitation)\n",
    "                # Estimate the Qs values state\n",
    "                Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "                # Take the biggest Q value (= the best action)\n",
    "                choice = np.argmax(Qs)\n",
    "                action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "        \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "        \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    \n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=//Users/Lei/Documents/master/semester2/Reinforcement_Learning/cw2/tensorboard/drqn/1/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
