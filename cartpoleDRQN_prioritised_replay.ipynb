{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import random                 # Handling random number generation\n",
    "import time                   # Handling time calculation\n",
    "from skimage import transform # Help us to preprocess the frames\n",
    "\n",
    "from collections import deque # Ordered collection with ends\n",
    "import matplotlib.pyplot as plt  # Display graphs\n",
    "\n",
    "import warnings                  # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Lei/Documents/master/semester2/Reinforcement_Learning/cw2\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/Lei/Documents/master/semester2/Reinforcement_Learning/cw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "learning_rate =  0.00001     # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 100000         # Total episodes for training\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# FIXED Q TARGETS HYPERPARAMETERS\n",
    "max_tau = 1000 #Tau is the C step where we update our target network\n",
    "update_steps = 4\n",
    "\n",
    "# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n",
    "explore_start = 0.1           # exploration probability at start\n",
    "explore_stop = 0.001            # minimum exploration probability        # exponential decay rate for exploration prob\n",
    "explore = 20000\n",
    "\n",
    "clip_norm = 0.00008\n",
    "\n",
    "model_folder = f\"./model/prioritised_replay/CN{clip_norm}_LR{learning_rate}_B{batch_size}\"\n",
    "#os.makedirs(model_folder)\n",
    "#log_path = f\"./logs/prioritised_replay/CN{clip_norm}_LR{learning_rate}_B{batch_size}.txt\"\n",
    "\n",
    "# Q LEARNING hyperparameters\n",
    "gamma = 0.99               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "## If you have GPU change to 1million\n",
    "pretrain_length = 1000             # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 50000 ##100000                 # Number of experiences the Memory can keep\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(4,32,kernel_size=8,stride=4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv2=nn.Conv2d(32,64,kernel_size=4,stride =2)\n",
    "        self.value_fc1 = nn.Linear(64, 2)\n",
    "        \n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2)\n",
    "        self.value_fc2 = nn.Linear(128, 2) \n",
    "        \n",
    "        #RNN layer\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=100, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(100, 1)\n",
    "\n",
    "    #def init_hidden(self, batch_size):\n",
    "        # Initialize the hidden state for LSTM\n",
    "        #hidden_state = torch.zeros(1, batch_size, hidden_size=100)\n",
    "        #cell_state = torch.zeros(1, batch_size, hidden_size=100)\n",
    "        #return (hidden_state, cell_state)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the layers\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        \n",
    "        # Reshape conv3 output to LSTM\n",
    "        x = x.view(x.size(0), -1, 64)\n",
    "        x = x.unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        output = self.fc(lstm_out)\n",
    "\n",
    "        return output\n",
    "        \n",
    "\n",
    "    def select_action(self, x):\n",
    "        with torch.no_grad():\n",
    "            Q = self.forward(x)\n",
    "            action_index = torch.argmax(Q, dim=1)\n",
    "        return action_index.item()\n",
    "\n",
    "    \n",
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou: \n",
    "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "tree_index  0 0  0  We fill the leaves from left to right\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "        \n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "        \n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "        \n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propagate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            \n",
    "            \"\"\"\n",
    "            Here we want to access the line above\n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "            \n",
    "                0\n",
    "               / \\\n",
    "              1   2\n",
    "             / \\ / \\\n",
    "            3  4 5  [6] \n",
    "            \n",
    "            If we are in leaf at index 6, we updated the priority score\n",
    "            We need then to update index 2 node\n",
    "            So tree_index = (tree_index - 1) // 2\n",
    "            tree_index = (6-1)//2\n",
    "            tree_index = 2 (because // round the result)\n",
    "            \"\"\"\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for experiences\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True: # the while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            \n",
    "            else: # downward search, always search for a higher priority node\n",
    "                \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                    \n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "            \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node\n",
    "    \n",
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree \n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        \n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        \n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "        \n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        \n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "    \n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "                                   \n",
    "            b_idx[i]= index\n",
    "            \n",
    "            experience = [data]\n",
    "            \n",
    "            memory_b.append(experience)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)\n",
    "\n",
    "\n",
    "def update_target_params(PolicyNetwork, TargetNetwork):\n",
    "    \"\"\"\n",
    "    Copy parameters of the Policy network to Target network.\n",
    "    \"\"\"\n",
    "    TargetNetwork_state_dict = TargetNetwork.state_dict()\n",
    "    PolicyNetwork_state_dict = PolicyNetwork.state_dict()\n",
    "    for key in PolicyNetwork_state_dict:\n",
    "        TargetNetwork_state_dict[key] = PolicyNetwork_state_dict[key]\n",
    "        TargetNetwork.load_state_dict(TargetNetwork_state_dict)\n",
    "\n",
    "    return PolicyNetwork, TargetNetwork   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Instantiate memory\n",
    "memory = Memory(memory_size)\n",
    "import numpy as np \n",
    "from skimage import transform\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "n_state = env.observation_space.shape[0]  # 4\n",
    "n_action = env.action_space.n\n",
    "\n",
    "state, _ = env.reset()\n",
    "\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    \n",
    "    action = random.randint(0,1)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        next_state = np.zeros(n_state)\n",
    "        experience = state, action, reward, next_state, done\n",
    "        #print(\"shape of a stack frame of dead: \", state.shape)\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Start a new episode\n",
    "        state, _ = env.reset()\n",
    "    else:\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m episode_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     54\u001b[0m total_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 56\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     59\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[56], line 30\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(epsilon, state)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Greedy action\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     tensor_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mPolicyNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "Cell \u001b[0;32mIn[49], line 44\u001b[0m, in \u001b[0;36mDRQN.select_action\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 44\u001b[0m         Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m         action_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(Q, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action_index\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[49], line 27\u001b[0m, in \u001b[0;36mDRQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Forward pass through the layers\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[1;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/PYTHON310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/PYTHON310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/PYTHON310/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/PYTHON310/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 4]"
     ]
    }
   ],
   "source": [
    "# Instantiate the DQNetwork\n",
    "PolicyNetwork = DRQN()\n",
    "\n",
    "# Instantiate the target network\n",
    "TargetNetwork = DRQN()\n",
    "\n",
    "optimizer = optim.RMSprop(PolicyNetwork.parameters(), lr=learning_rate)\n",
    "\n",
    "plot_dir = '/Users/Lei/Documents/master/semester2/Reinforcement_Learning/cw2/model/logs/plots'\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "figure_file = os.path.join(plot_dir, 'test.png')\n",
    "\n",
    "def select_action(epsilon, state):\n",
    "    \"\"\"\n",
    "    This function will do the part\n",
    "    With Ïµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\n",
    "    Input state is 4 stacked states. \n",
    "    \"\"\"\n",
    "\n",
    "    num = random.random()\n",
    "\n",
    "    if (num < epsilon):\n",
    "        action = random.randint(0, 1)\n",
    "        return action\n",
    "    # Greedy action\n",
    "    else:\n",
    "        \n",
    "        tensor_state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action = PolicyNetwork.select_action(tensor_state)\n",
    "        return action\n",
    "\n",
    "epsilon = explore_start\n",
    "# Set Target network params\n",
    "PolicyNetwork, TargetNetwork = update_target_params(PolicyNetwork, TargetNetwork)\n",
    "\n",
    "#out_f = open(log_path, 'w')\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "\n",
    "    episode_step = 0\n",
    "    episode_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    # print(\"state: \", state)\n",
    "    \n",
    "\n",
    "    for time_steps in range(10000):\n",
    "\n",
    "        episode_step += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        action = select_action(epsilon, state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done: \n",
    "            next_state = np.zeros(n_state)\n",
    "            experience = state, action, reward, next_state, done\n",
    "            memory.store(experience)\n",
    "\n",
    "        else: \n",
    "            experience = state, action, reward, next_state, done\n",
    "            # print(\"state: \", state)\n",
    "            memory.store(experience)\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "        tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "        \n",
    "        batch_states = torch.FloatTensor([each[0][0] for each in batch] )\n",
    "\n",
    "        batch_actions = torch.FloatTensor([each[0][1] for each in batch]).unsqueeze(1)\n",
    "        batch_rewards = torch.FloatTensor([each[0][2] for each in batch]).unsqueeze(1)\n",
    "        batch_next_states = torch.FloatTensor([each[0][3] for each in batch] ) # stacked frames of np arrays \n",
    "        batch_dones = torch.FloatTensor([each[0][4] for each in batch]).unsqueeze(1)\n",
    "\n",
    "        actions_index = batch_actions.detach().numpy().flatten()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            policy_q_next = PolicyNetwork(batch_next_states)\n",
    "            target_q_next = TargetNetwork(batch_next_states)\n",
    "            online_max_action = torch.argmax(policy_q_next, dim=1, keepdim=True)\n",
    "            y = batch_rewards + (1 - batch_dones) * gamma * target_q_next.gather(1, online_max_action.long())\n",
    "\n",
    "        loss = F.mse_loss(PolicyNetwork(batch_states).gather(1, batch_actions.long()), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_value_(PolicyNetwork.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(PolicyNetwork.parameters(), clip_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        terminal_np = batch_dones.detach().numpy().flatten()\n",
    "        policy_q_next_np = policy_q_next.detach().numpy()\n",
    "        target_q_next_np = target_q_next.detach().numpy()\n",
    "        rewards_np = batch_rewards.detach().numpy().flatten()\n",
    "\n",
    "        target_qs_batch = []\n",
    "\n",
    "        for i in range(0, len(batch)):\n",
    "\n",
    "            terminal = terminal_np[i]\n",
    "            action = np.argmax(policy_q_next_np[i])\n",
    "\n",
    "            if terminal:\n",
    "                target_qs_batch.append(torch.tensor(rewards_np[i], dtype=torch.float32)) # rewards_mb[i] is a TENSOR\n",
    "\n",
    "            else: \n",
    "                target = rewards_np[i] + gamma * target_q_next_np[i][action]\n",
    "\n",
    "                target_qs_batch.append(torch.tensor(target, dtype=torch.float32))\n",
    "\n",
    "        ACTIONS = [torch.tensor(np.array([0,1]), dtype=torch.float32), torch.tensor(np.array([1,0]), dtype=torch.float32)]\n",
    "        predicted_qs = [torch.sum(torch.mul(policy_q_next[i], ACTIONS[int(actions_index[i])])) for i in range(batch_size)]\n",
    "\n",
    "        absolute_errors = [torch.abs(torch.subtract(predicted_qs[i], target_qs_batch[i])) for i in range(batch_size)]\n",
    "        \n",
    "        absolute_errors_np = np.array([error.item() for error in absolute_errors])\n",
    "        ISWeights_list = ISWeights_mb.tolist()\n",
    "        memory.batch_update(tree_idx, absolute_errors_np)\n",
    "\n",
    "\n",
    "        if epsilon > explore_stop:\n",
    "            epsilon -= (explore_start - explore_stop) / explore\n",
    "\n",
    "        # print(\"POLICY PARAMS: \" , PolicyNetwork.state_dict())\n",
    "        if total_steps % update_steps == 0:\n",
    "            PolicyNetwork, TargetNetwork = update_target_params(PolicyNetwork, TargetNetwork)\n",
    "            # print(\"Model updated\")\n",
    "\n",
    "        if done: \n",
    "            avg_episode_lengths = [np.mean(episode_step[:i + 1]) for i in range(episode + 1)]\n",
    "            avg_episode_rewards = [np.mean(episode_step[:i + 1]) for i in range(episode + 1)]\n",
    "            loss_at_end_of_episode = torch.mean(torch.stack(absolute_errors), dim=0).item()\n",
    "            \n",
    "\n",
    "            # Plotting\n",
    "            fig, ax1 = plt.subplots()\n",
    "\n",
    "            ax1.set_xlabel('Number of Episodes')\n",
    "            ax1.set_ylabel('Mean Episode Length', color='tab:blue')\n",
    "            ax1.plot(range(1, episode + 2), avg_episode_lengths, color='tab:blue', label='Episode Length')\n",
    "            ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.set_ylabel('Mean Reward per Episode', color='tab:red')\n",
    "            ax2.plot(range(1, episode + 2), avg_episode_rewards, color='tab:red', label='Episode Reward')\n",
    "            ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "            fig.tight_layout()\n",
    "            ax1.legend(loc='upper right')\n",
    "            ax2.legend(loc='upper left')\n",
    "            plt.title('Training Curve')\n",
    "\n",
    "            # Save the plot with a filename based on acc_timesteps\n",
    "            save_filename = f\"performance_{total_steps:06d}.png\"\n",
    "            save_path = os.path.join(plot_dir, save_filename)\n",
    "            plt.savefig(save_path)\n",
    "\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "            print(f\"Episode {episode}     |      Reward: {episode_reward}     |     length: {episode_step}      |      Loss: {loss_at_end_of_episode}    |     Total timesteps: {total_steps}\")\n",
    "        \n",
    "            if episode % 100 == 0:\n",
    "                torch.save(PolicyNetwork, f\"{model_folder}/E_{episode}.pt\")\n",
    "                print(f\"======Model saved.======\")\n",
    "            \n",
    "            break\n",
    "\n",
    "    if total_steps > 200000:\n",
    "        break\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
