{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/simoninithomas/d6adc6edb0a7f37d6323a5e3d2ab72ec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "- Now that we imported the libraries/dependencies, we will create our environment.\n",
    "- Doom environment takes:\n",
    "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
    "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
    "- Note: We have 3 possible actions: turn left, turn right, shoot (attack)... so we don't need to do one hot encoding (thanks to <a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out). \n",
    "\n",
    "<br>\n",
    "REWARDS:\n",
    "\n",
    "- death penalty = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "\n",
    "    # Load the correct configuration\n",
    "    game.load_config('VizDoom/scenarios/defend_the_center.cfg')\n",
    "\n",
    "    # Load the correct scenario (in our case basic)\n",
    "    game.set_doom_scenario_path('VizDoom/scenarios/defend_the_center.wad')\n",
    "\n",
    "    game.set_window_visible(False) #no pop out window\n",
    "    game.init()\n",
    "\n",
    "    # Here we create an hot encoded version of our actions (3 possible actions)\n",
    "    possible_actions = np.identity(3,dtype=int).tolist()\n",
    "\n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "files_and_directories = os.listdir(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
    "### preprocess_frame\n",
    "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
    "<br><br>\n",
    "Our steps:\n",
    "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\n",
    "- We normalize pixel values\n",
    "- Finally we resize the preprocessed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "    Take a frame.\n",
    "    Resize it.\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "\n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    Normalize it.\n",
    "\n",
    "    return preprocessed_frame\n",
    "\n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Crop the screen (remove part that contains no information)\n",
    "    # [Up: Down, Left: right]\n",
    "    cropped_frame = frame[15:-5, 20:-20]\n",
    "\n",
    "    # Check if the cropped frame has non-zero dimensions\n",
    "    if cropped_frame.size == 0:\n",
    "        # If the cropped frame has zero dimensions, return a default frame with zeros\n",
    "        return np.zeros((100, 120), dtype=np.float32)\n",
    "\n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame / 255.0\n",
    "\n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(cropped_frame, [100, 120])\n",
    "\n",
    "    return preprocessed_frame # 100x120x1 frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack_frames\n",
    "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
    "\n",
    "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
    "\n",
    "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
    "\n",
    "- First we preprocess frame\n",
    "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
    "- Finally we **build the stacked state**\n",
    "\n",
    "This is how work stack:\n",
    "- For the first frame, we feed 4 frames\n",
    "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
    "- And so on\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
    "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((100,120), dtype=int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    if state.size == 0:\n",
    "        # Return the existing stacked frames without modification\n",
    "        return np.stack(stacked_frames, axis=2), stacked_frames\n",
    "\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "\n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((100,120), dtype=int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
    "\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [100,120,4]      # Our input is a stack of 4 frames hence 100x120x4 (Width, height, channels)\n",
    "action_size = game.get_available_buttons_size()              # 3 possible actions\n",
    "learning_rate =  0.00025      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 1000000         # Total episodes for training\n",
    "total_timesteps = 500000     #Total timestesp for training\n",
    "max_steps = 2100             # Max possible steps in an episode\n",
    "batch_size = 64\n",
    "\n",
    "# FIXED Q TARGETS HYPERPARAMETERS\n",
    "max_tau = 10000 #Tau is the C step where we update our target network\n",
    "\n",
    "# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.001            # minimum exploration probability\n",
    "decay_rate = 0.00005            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q LEARNING hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "## If you have GPU change to 1million\n",
    "pretrain_length = 10             # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 10 ##100000                 # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = False\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Dueling Double Deep Q-learning Neural Network model (aka DDDQN) üß†\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1500/1*FkHqwA2eSGixdS-3dvVoMA.png\" alt=\"Dueling Double Deep Q Learning Model\" />\n",
    "This is our Dueling Double Deep Q-learning model:\n",
    "- We take a stack of 4 frames as input\n",
    "- It passes through 3 convnets\n",
    "- Then it is flatened\n",
    "- Then it is passed through 2 streams\n",
    "    - One that calculates V(s)\n",
    "    - The other that calculates A(s,a)\n",
    "- Finally an agregating layer\n",
    "- It outputs a Q value for each actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQNNet:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "        random_var = 64\n",
    "        \n",
    "        \n",
    "        # We use tf.variable_scope here to know which network we're using (DQN or target_net)\n",
    "        # it will be useful when we will update our w- parameters (by copy the DQN parameters)\n",
    "        with tf.compat.v1.variable_scope(self.name):\n",
    "            \n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            # [None, 100, 120, 4]\n",
    "            self.inputs_ = tf.compat.v1.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            \n",
    "            #\n",
    "            self.ISWeights_ = tf.compat.v1.placeholder(tf.float32, [None,1], name='IS_weights')\n",
    "            \n",
    "            self.actions_ = tf.placeholder(tf.float32, [random_var, action_size], name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.compat.v1.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            # Input is 100x120x4\n",
    "            self.conv1 = Conv2D(\n",
    "                    filters=32,\n",
    "                    kernel_size=[8, 8],\n",
    "                    strides=[4, 4],\n",
    "                    padding=\"VALID\",\n",
    "                    kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "                    name=\"conv1\")(self.inputs_)\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Second convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv2 = Conv2D(\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "                                 name = \"conv2\")(self.conv1_out)\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\")\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Third convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv3 = Conv2D(\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "                                 name = \"conv3\")(self.conv2_out)\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
    "            \n",
    "            \n",
    "            self.flatten = Flatten(data_format='channels_last')(self.conv3)\n",
    "            \n",
    "            \n",
    "            ## Here we separate into two streams\n",
    "            # The one that calculate V(s)\n",
    "            self.value_fc = Dense(\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "                                name=\"value_fc\")(self.flatten)\n",
    "            \n",
    "            self.value = Dense(\n",
    "                                        units = 1,\n",
    "                                        activation = None,\n",
    "                                        kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "                                name=\"value\")(self.value_fc)\n",
    "            \n",
    "            # The one that calculate A(s,a)\n",
    "            self.advantage_fc = Dense(\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "                                name=\"advantage_fc\")(self.flatten)\n",
    "            \n",
    "            self.advantage = Dense(\n",
    "                                        units = self.action_size,\n",
    "                                        activation = None,\n",
    "                                        kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "                                name=\"advantages\")(self.advantage_fc)\n",
    "            \n",
    "            # Agregating layer\n",
    "            # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "            self.output = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "              \n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            # The loss is modified because of PER \n",
    "            self.absolute_errors = tf.abs(self.target_Q - self.Q)# for updating Sumtree\n",
    "            \n",
    "            self.loss = tf.reduce_mean(self.ISWeights_ * tf.math.squared_difference(self.target_Q, self.Q))\n",
    "            \n",
    "            self.optimizer = tf.compat.v1.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"DQNetwork\")\n",
    "\n",
    "# Instantiate the target network\n",
    "TargetNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"TargetNetwork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Prioritized Experience Replay üîÅ\n",
    "Now that we create our Neural Network, **we need to implement the Prioritized Experience Replay method.** <br>\n",
    "\n",
    "As explained in the article, **we can't use a simple array to do that because sampling from it will be not efficient, so we use a binary tree data type (in a binary tree each node has no + than 2 children).** More precisely, a sumtree, which is a binary tree where parents nodes are the sum of the children nodes.\n",
    "\n",
    "If you don't know what is a binary tree check this awesome video https://www.youtube.com/watch?v=oSWTXtMglKE\n",
    "\n",
    "\n",
    "This SumTree implementation was taken from Morvan Zhou in his chinese course about Reinforcement Learning\n",
    "\n",
    "To summarize:\n",
    "- **Step 1**: We construct a SumTree, which is a Binary Sum tree where leaves contains the priorities and a data array where index points to the index of leaves.\n",
    "    <img src=\"https://cdn-images-1.medium.com/max/1200/1*Go9DNr7YY-wMGdIQ7HQduQ.png\" alt=\"SumTree\"/>\n",
    "    <br><br>\n",
    "    - **def __init__**: Initialize our SumTree data object with all nodes = 0 and data (data array) with all = 0.\n",
    "    - **def add**: add our priority score in the sumtree leaf and experience (S, A, R, S', Done) in data.\n",
    "    - **def update**: we update the leaf priority score and propagate through tree.\n",
    "    - **def get_leaf**: retrieve priority score, index and experience associated with a leaf.\n",
    "    - **def total_priority**: get the root node value to calculate the total priority score of our replay buffer.\n",
    "<br><br>\n",
    "- **Step 2**: We create a Memory object that will contain our sumtree and data.\n",
    "    - **def __init__**: generates our sumtree and data by instantiating the SumTree object.\n",
    "    - **def store**: we store a new experience in our tree. Each new experience will **have priority = max_priority** (and then this priority will be corrected during the training (when we'll calculating the TD error hence the priority score).\n",
    "    - **def sample**:\n",
    "         - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "         - Then a value is uniformly sampled from each range\n",
    "         - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "         - Then, we calculate IS weights for each minibatch element\n",
    "    - **def update_batch**: update the priorities on the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou:\n",
    "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "\n",
    "    \"\"\"\n",
    "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
    "\n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "\n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
    "        \"\"\"\n",
    "\n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "\n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "tree_index  0 0  0  We fill the leaves from left to right\n",
    "        \"\"\"\n",
    "\n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "\n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "\n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "\n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propagate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "\n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "\n",
    "            \"\"\"\n",
    "            Here we want to access the line above\n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "\n",
    "                0\n",
    "               / \\\n",
    "              1   2\n",
    "             / \\ / \\\n",
    "            3  4 5  [6]\n",
    "\n",
    "            If we are in leaf at index 6, we updated the priority score\n",
    "            We need then to update index 2 node\n",
    "            So tree_index = (tree_index - 1) // 2\n",
    "            tree_index = (6-1)//2\n",
    "            tree_index = 2 (because // round the result)\n",
    "            \"\"\"\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for experiences\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_index = 0\n",
    "\n",
    "        while True: # the while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "\n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "\n",
    "            else: # downward search, always search for a higher priority node\n",
    "\n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "\n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don't use deque anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "\n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "\n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree\n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "\n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "\n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "\n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "\n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "\n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "\n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "\n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "\n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "\n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "\n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "\n",
    "            b_idx[i]= index\n",
    "\n",
    "            experience = [data]\n",
    "\n",
    "            memory_b.append(experience)\n",
    "\n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "\n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "\n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "\n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "\n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "\n",
    "        # Add experience to memory\n",
    "        #experience = np.hstack((state, [action, reward], next_state, done))\n",
    "\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "\n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "\n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "        # Add experience to memory\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "\n",
    "        # Our state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/dddqn/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"./tensorboard/dddqn/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train our Agent üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Our algorithm:\n",
    "<br>\n",
    "* Initialize the weights for DQN\n",
    "* Initialize target value weights w- <- w\n",
    "* Init the environment\n",
    "* Initialize the decay rate (that will use to reduce epsilon) \n",
    "<br><br>\n",
    "* **For** episode to max_episode **do** \n",
    "    * Make new episode\n",
    "    * Set step to 0\n",
    "    * Observe the first state $s_0$\n",
    "    <br><br>\n",
    "    * **While** step < max_steps **do**:\n",
    "        * Increase decay_rate\n",
    "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "        \n",
    "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
    "        * Set target $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma Q(s',argmax_{a'}{Q(s', a', w), w^-)}$\n",
    "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "        * Every C steps, reset: $w^- \\leftarrow w$\n",
    "    * **endfor**\n",
    "    <br><br>\n",
    "* **endfor**\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With œµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "\n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "\n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "\n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "\n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps us to copy one set of variables to another\n",
    "# In our case we use it when we want to copy the parameters of DQN to Target_network\n",
    "# Thanks of the very good implementation of Arthur Juliani https://github.com/awjuliani\n",
    "def update_target_graph():\n",
    "\n",
    "    # Get the parameters of our DQNNetwork\n",
    "    from_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, \"DQNetwork\")\n",
    "\n",
    "    # Get the parameters of our Target_network\n",
    "    to_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, \"TargetNetwork\")\n",
    "\n",
    "    op_holder = []\n",
    "\n",
    "    # Update our target_network parameters with DQNNetwork parameters\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m q_next_state \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mrun(DQNetwork\u001b[38;5;241m.\u001b[39moutput, feed_dict \u001b[38;5;241m=\u001b[39m {DQNetwork\u001b[38;5;241m.\u001b[39minputs_: next_states_mb})\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Calculate Qtarget for all actions that state\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m q_target_next_state \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTargetNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mTargetNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs_\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states_mb\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma * Qtarget(s',a') \u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(batch)):\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom2/lib/python3.10/site-packages/tensorflow/python/client/session.py:972\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    969\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 972\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    975\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom2/lib/python3.10/site-packages/tensorflow/python/client/session.py:1215\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1215\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom2/lib/python3.10/site-packages/tensorflow/python/client/session.py:1395\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1392\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1398\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom2/lib/python3.10/site-packages/tensorflow/python/client/session.py:1402\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1401\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1404\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom2/lib/python3.10/site-packages/tensorflow/python/client/session.py:1385\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1383\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1384\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1385\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom2/lib/python3.10/site-packages/tensorflow/python/client/session.py:1478\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1477\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1478\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "avg_episode_lengths = 0\n",
    "avg_episode_rewards = 0\n",
    "acc_timesteps = 0\n",
    "episode_lengths = []\n",
    "eepisode_rewards = []\n",
    "\n",
    "plot_dir = './logs/dddqn/plots'\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "figure_file = os.path.join(plot_dir, 'test.png')\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "        \n",
    "        # Set tau = 0\n",
    "        tau = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "        \n",
    "        # Update the parameters of our TargetNetwork with DQN_weights\n",
    "        update_target = update_target_graph()\n",
    "        sess.run(update_target)\n",
    "        while acc_timesteps < total_timesteps:\n",
    "            for episode in range(total_episodes):\n",
    "                try:\n",
    "                    if acc_timesteps > total_timesteps:\n",
    "                        break\n",
    "                    # Set step to 0\n",
    "                    step = 0\n",
    "                    \n",
    "                    # Initialize the rewards of the episode\n",
    "                    episode_rewards = []\n",
    "                    \n",
    "                    # Make a new episode and observe the first state\n",
    "                    game.new_episode()\n",
    "                    \n",
    "                    state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    # Remember that stack frame function also call our preprocess function.\n",
    "                    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "                \n",
    "                    while step < max_steps:\n",
    "                        if acc_timesteps > total_timesteps:\n",
    "                            break\n",
    "                        step += 1\n",
    "                        acc_timesteps += 1\n",
    "                        \n",
    "                        # Increase the C step\n",
    "                        tau += 1\n",
    "                        \n",
    "                        # Increase decay_step\n",
    "                        decay_step +=1\n",
    "                        \n",
    "                        # With œµ select a random action atat, otherwise select a = argmaxQ(st,a)\n",
    "                        action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                        # Do the action\n",
    "                        reward = game.make_action(action)\n",
    "\n",
    "                        # Look if the episode is finished\n",
    "                        done = game.is_episode_finished()\n",
    "                        \n",
    "                        # Add the reward to total reward\n",
    "                        episode_rewards.append(reward)\n",
    "\n",
    "                        # If the game is finished\n",
    "                        if done:\n",
    "                            # the episode ends so no next state\n",
    "                            next_state = np.zeros((120,140), dtype=int)\n",
    "                            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                            savestep = step\n",
    "\n",
    "                            # Set step = max_steps to end the episode\n",
    "                            step = max_steps\n",
    "\n",
    "                            # Get the total reward of the episode\n",
    "                            total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                            print('Episode: {}'.format(episode), 'Step: {}'.format(savestep), 'Acc Step: {}'.format(acc_timesteps),\n",
    "                                    'Total reward: {}'.format(total_reward),\n",
    "                                    'Training loss: {:.4f}'.format(loss),\n",
    "                                    'Explore P: {:.4f}'.format(explore_probability))\n",
    "                            \n",
    "                            episode_lengths.append(savestep)\n",
    "                            eepisode_rewards.append(total_reward)\n",
    "                            # Add experience to memory\n",
    "                            experience = state, action, reward, next_state, done\n",
    "                            memory.store(experience)\n",
    "\n",
    "                        else:\n",
    "                            # Get the next state\n",
    "                            next_state = game.get_state().screen_buffer\n",
    "                            \n",
    "                            # Stack the frame of the next_state\n",
    "                            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                            \n",
    "\n",
    "                            # Add experience to memory\n",
    "                            experience = state, action, reward, next_state, done\n",
    "                            memory.store(experience)\n",
    "                            \n",
    "                            # st+1 is now our current state\n",
    "                            state = next_state\n",
    "\n",
    "\n",
    "                        ### LEARNING PART            \n",
    "                        # Obtain random mini-batch from memory\n",
    "                        tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "                        \n",
    "                        states_mb = np.array([each[0][0] for each in batch], ndmin=3)\n",
    "                        actions_mb = np.array([each[0][1] for each in batch])\n",
    "                        rewards_mb = np.array([each[0][2] for each in batch]) \n",
    "                        next_states_mb = np.array([each[0][3] for each in batch], ndmin=3)\n",
    "                        dones_mb = np.array([each[0][4] for each in batch])\n",
    "\n",
    "                        target_Qs_batch = []\n",
    "\n",
    "                        \n",
    "                        ### DOUBLE DQN Logic\n",
    "                        # Use DQNNetwork to select the action to take at next_state (a') (action with the highest Q-value)\n",
    "                        # Use TargetNetwork to calculate the Q_val of Q(s',a')\n",
    "                        \n",
    "                        # Get Q values for next_state \n",
    "                        q_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                        \n",
    "                        # Calculate Qtarget for all actions that state\n",
    "                        q_target_next_state = sess.run(TargetNetwork.output, feed_dict = {TargetNetwork.inputs_: next_states_mb})\n",
    "                        \n",
    "                        \n",
    "                        # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma * Qtarget(s',a') \n",
    "                        for i in range(0, len(batch)):\n",
    "                            terminal = dones_mb[i]\n",
    "                            \n",
    "                            # We got a'\n",
    "                            action = np.argmax(q_next_state[i])\n",
    "\n",
    "                            # If we are in a terminal state, only equals reward\n",
    "                            if terminal:\n",
    "                                target_Qs_batch.append(rewards_mb[i])\n",
    "                                \n",
    "                            else:\n",
    "                                # Take the Qtarget for action a'\n",
    "                                target = rewards_mb[i] + gamma * q_target_next_state[i][action]\n",
    "                                target_Qs_batch.append(target)\n",
    "                                \n",
    "\n",
    "                        targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                        \n",
    "                        _, loss, absolute_errors = sess.run([DQNetwork.optimizer, DQNetwork.loss, DQNetwork.absolute_errors],\n",
    "                                            feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                    DQNetwork.target_Q: targets_mb,\n",
    "                                                    DQNetwork.actions_: actions_mb,\n",
    "                                                    DQNetwork.ISWeights_: ISWeights_mb})\n",
    "                    \n",
    "                        \n",
    "                        \n",
    "                        # Update priority\n",
    "                        memory.batch_update(tree_idx, absolute_errors)\n",
    "                        \n",
    "                        \n",
    "                        # Write TF Summaries\n",
    "                        summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                            DQNetwork.target_Q: targets_mb,\n",
    "                                                            DQNetwork.actions_: actions_mb,\n",
    "                                                        DQNetwork.ISWeights_: ISWeights_mb})\n",
    "                        writer.add_summary(summary, episode)\n",
    "                        writer.flush()\n",
    "                        \n",
    "                        if tau > max_tau:\n",
    "                            # Update the parameters of our TargetNetwork with DQN_weights\n",
    "                            update_target = update_target_graph()\n",
    "                            sess.run(update_target)\n",
    "                            tau = 0\n",
    "                            print(\"Model updated\")\n",
    "                        \n",
    "                        # Save model every 16 timesteps (500 for demostrate purpose)\n",
    "                        if acc_timesteps % 16 == 0:\n",
    "                            avg_episode_lengths = [np.mean(episode_lengths[:i + 1]) for i in range(episode + 1)]\n",
    "                            avg_episode_rewards = [np.mean(eepisode_rewards[:i + 1]) for i in range(episode + 1)]\n",
    "                            save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "\n",
    "                            # Clear previous output\n",
    "                            clear_output(wait=True)\n",
    "                            \n",
    "                            # Plotting\n",
    "                            fig, ax1 = plt.subplots()\n",
    "\n",
    "                            ax1.set_xlabel('Number of Episodes')\n",
    "                            ax1.set_ylabel('Mean Episode Length', color='tab:blue')\n",
    "                            ax1.plot(range(1, episode + 2), avg_episode_lengths, color='tab:blue',label='Episode Length')\n",
    "                            ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "                            ax2 = ax1.twinx()\n",
    "                            ax2.set_ylabel('Mean Reward per Episode', color='tab:red')\n",
    "                            ax2.plot(range(1, episode + 2), avg_episode_rewards, color='tab:red',label='Episode Reward')\n",
    "                            ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "                            fig.tight_layout()\n",
    "                            ax1.legend(loc='upper right')\n",
    "                            ax2.legend(loc='upper left')\n",
    "                            plt.title('Training Curve')\n",
    "\n",
    "                            # Save the plot with a filename based on acc_timesteps\n",
    "                            save_filename = f\"performance_{acc_timesteps:06d}.png\"\n",
    "                            save_path = os.path.join(plot_dir, save_filename)\n",
    "                            plt.savefig(save_path)\n",
    "\n",
    "\n",
    "                            plt.show()\n",
    "                except tf.errors.InvalidArgumentError as e:\n",
    "                    print(\"An error occurred:\", e)\n",
    "                    print(\"Skipping the code and continuing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Watch our Agent play üëÄ\n",
    "Now that we trained our agent, we can test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[219], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     choice \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Qs)\n\u001b[1;32m     43\u001b[0m     action \u001b[38;5;241m=\u001b[39m possible_actions[\u001b[38;5;28mint\u001b[39m(choice)]\n\u001b[0;32m---> 45\u001b[0m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m done \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mis_episode_finished()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration (TESTING)\n",
    "    game.load_config('VizDoom/scenarios/defend_the_center.cfg')\n",
    "    \n",
    "    # Load the correct scenario (in our case deadly_corridor scenario)\n",
    "    game.set_doom_scenario_path('VizDoom/scenarios/defend_the_center.wad')\n",
    "    \n",
    "    game.init()    \n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            ## EPSILON GREEDY STRATEGY\n",
    "            # Choose action a from state s using epsilon greedy.\n",
    "            ## First we randomize a number\n",
    "            exp_exp_tradeoff = np.random.rand()\n",
    "            \n",
    "\n",
    "            explore_probability = 0.01\n",
    "    \n",
    "            if (explore_probability > exp_exp_tradeoff):\n",
    "                # Make a random action (exploration)\n",
    "                action = random.choice(possible_actions)\n",
    "        \n",
    "            else:\n",
    "                # Get action from Q-network (exploitation)\n",
    "                # Estimate the Qs values state\n",
    "                Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "                # Take the biggest Q value (= the best action)\n",
    "                choice = np.argmax(Qs)\n",
    "                action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "        \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "        \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    \n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
