{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trains CartPole agent with DDDQN REP\n",
    "\n",
    "**Please expand the cells to view the code!**\n",
    "\n",
    "### Description\n",
    "This notebook trains the DDDQN model on a CartPole-v1 agent, using Random experience replay (REP) which is determined using the SumTree data structure as introduced in the main reference. \n",
    "\n",
    "### How to Run:\n",
    "1. **Setup**: Ensure all necessary libraries are installed and the CartPole environment is set up. This was developed in **Python 3.10.14**.\n",
    "2. **Configure Hyperparameters**: Modify the hyperparameters to experiment with different model behaviors and optimization strategies.\n",
    "3. **Train the Model**: Run the code sequentially to train the DQN model, monitor training progress through log outputs, and save model checkpoints periodically.\n",
    "\n",
    "### References\n",
    "Minai, Y., 2023. Deep Q-learning (DQN) Tutorial with CartPole-v0 [Online]. Medium. Available from: https://medium.com/@ym1942/deep-q-learning-dqn-tutorial-with-cartpole-v0-5505dbd2409e [Accessed 8 May 2024].\n",
    "\n",
    "Paszke, A., n.d. Reinforcement Learning (DQN) Tutorial — PyTorch Tutorials 1.8.0 documentation [Online]. pytorch.org. Available from: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html [Accessed 8 May 2024].\n",
    "\n",
    "Perkins, H., 2022. youtube-rl-demos/vizdoom/vizdoom_011.py at vizdoom13 · hughperkins/youtube-rl-demos [Online]. GitHub. Available from: https://github.com/hughperkins/youtube-rl-demos/blob/vizdoom13/vizdoom/vizdoom_011.py [Accessed 8 May 2024].\n",
    "\n",
    "Simonini, T., 2018. Dueling Deep Q Learning with Doom (+ double DQNs and Prioritized Experience Replay).ipynb [Online]. Gist. Available from: https://gist.github.com/simoninithomas/d6adc6edb0a7f37d6323a5e3d2ab72ec#file-dueling-deep-q-learning-with-doom-double-dqns-and-prioritized-experience-replay-ipynb [Accessed 8 May 2024].\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import cv2\n",
    "\n",
    "import random                 \n",
    "import time                   \n",
    "from skimage import transform \n",
    "\n",
    "from collections import deque \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "import warnings                  \n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "learning_rate =  1e-5     \n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 100000         # Total episodes for training\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# FIXED Q TARGETS HYPERPARAMETERS\n",
    "update_steps = 4\n",
    "clip_norm = 0.0001 \n",
    "# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n",
    "explore_start = 0.01           # exploration probability at start\n",
    "\n",
    "\n",
    "explore_stop = 0.0001            # minimum exploration probability        # exponential decay rate for exploration prob\n",
    "explore = 20000\n",
    "\n",
    "model_path = f\"./models/random_replay/explore/EX{explore_stop}_B{batch_size}_CN{clip_norm}_LR{learning_rate}\"\n",
    "os.makedirs(model_path)\n",
    "log_path = f\"./logs/random_replay/explore/EX{explore_stop}_B{batch_size}_CN{clip_norm}_LR{learning_rate}.txt\"\n",
    "\n",
    "# Q LEARNING hyperparameters\n",
    "gamma = 0.99               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "## If you have GPU change to 1million\n",
    "pretrain_length = 1000             # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 50000 ##100000                 # Number of experiences the Memory can keep\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc_value = nn.Linear(64, 256)\n",
    "        self.value = nn.Linear(256, 1)\n",
    "\n",
    "        self.fc_adv = nn.Linear(64, 256)\n",
    "        self.adv = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.relu(self.fc1(x))\n",
    "        value = self.relu(self.fc_value(y))\n",
    "        adv = self.relu(self.fc_adv(y))\n",
    "\n",
    "        value = self.value(value)\n",
    "        adv = self.adv(adv)\n",
    "\n",
    "        output = value + adv - torch.mean(adv, dim=1, keepdim=True)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def select_action(self, x):\n",
    "        with torch.no_grad():\n",
    "            Q = self.forward(x)\n",
    "            action_index = torch.argmax(Q, dim=1)\n",
    "        return action_index.item()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def update_target_params(PolicyNetwork, TargetNetwork):\n",
    "    \"\"\"\n",
    "    Copy parameters of the Policy network to Target network.\n",
    "    \"\"\"\n",
    "    TargetNetwork_state_dict = TargetNetwork.state_dict()\n",
    "    PolicyNetwork_state_dict = PolicyNetwork.state_dict()\n",
    "    for key in PolicyNetwork_state_dict:\n",
    "        TargetNetwork_state_dict[key] = PolicyNetwork_state_dict[key]\n",
    "        TargetNetwork.load_state_dict(TargetNetwork_state_dict)\n",
    "\n",
    "    return PolicyNetwork, TargetNetwork   \n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, memory_size: int) -> None:\n",
    "        self.memory_size = memory_size\n",
    "        self.buffer = deque(maxlen=self.memory_size)\n",
    "\n",
    "    def add(self, experience) -> None:\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size: int, continuous: bool = True):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        if continuous:\n",
    "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
    "            return [self.buffer[i] for i in range(rand, rand + batch_size)]\n",
    "        else:\n",
    "            indexes = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "            return [self.buffer[i] for i in indexes]\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(memory_size)\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "n_state = env.observation_space.shape[0]  # 4\n",
    "n_action = env.action_space.n\n",
    "\n",
    "state, _ = env.reset()\n",
    "\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    \n",
    "    action = random.randint(0,1)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        next_state = np.zeros(n_state)\n",
    "        experience = state, action, reward, next_state, done\n",
    "        #print(\"shape of a stack frame of dead: \", state.shape)\n",
    "        memory.add(experience)\n",
    "        \n",
    "        # Start a new episode\n",
    "        state, _ = env.reset()\n",
    "    else:\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.add(experience)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DQNetwork\n",
    "PolicyNetwork = DDDQNet()\n",
    "\n",
    "# Instantiate the target network\n",
    "TargetNetwork = DDDQNet()\n",
    "\n",
    "optimizer = optim.RMSprop(PolicyNetwork.parameters(), lr=learning_rate)\n",
    "\n",
    "def select_action(epsilon, state):\n",
    "    \"\"\"\n",
    "    This function will do the part\n",
    "    With ϵ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\n",
    "    Input state is 4 stacked states. \n",
    "    \"\"\"\n",
    "\n",
    "    num = random.random()\n",
    "\n",
    "    if (num < epsilon):\n",
    "        action = random.randint(0, 1)\n",
    "        return action\n",
    "    # Greedy action\n",
    "    else:\n",
    "        \n",
    "        tensor_state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action = PolicyNetwork.select_action(tensor_state)\n",
    "        return action\n",
    "\n",
    "epsilon = explore_start\n",
    "# Set Target network params\n",
    "PolicyNetwork, TargetNetwork = update_target_params(PolicyNetwork, TargetNetwork)\n",
    "\n",
    "out_f = open(log_path, 'w')\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "\n",
    "    episode_step = 0\n",
    "    episode_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    # print(\"state: \", state)\n",
    "    \n",
    "\n",
    "    for time_steps in range(10000):\n",
    "\n",
    "        episode_step += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        action = select_action(epsilon, state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done: \n",
    "            next_state = np.zeros(n_state)\n",
    "            experience = state, action, reward, next_state, done\n",
    "            memory.add(experience)\n",
    "\n",
    "        else: \n",
    "            experience = state, action, reward, next_state, done\n",
    "            # print(\"state: \", state)\n",
    "            memory.add(experience)\n",
    "            state = next_state\n",
    "\n",
    "        \n",
    "        batch = memory.sample(batch_size, False)\n",
    "\n",
    "        batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*batch)\n",
    "        # state, action, reward, next_state, done\n",
    "        # \n",
    "        # \n",
    "\n",
    "        batch_states = torch.FloatTensor(batch_states)\n",
    "        batch_next_states = torch.FloatTensor(batch_next_states)\n",
    "        batch_actions = torch.FloatTensor(batch_actions).unsqueeze(1)\n",
    "        batch_rewards = torch.FloatTensor(batch_rewards).unsqueeze(1)\n",
    "        batch_dones = torch.FloatTensor(batch_dones).unsqueeze(1)\n",
    "\n",
    "        actions_index = batch_actions.detach().numpy().flatten()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            policy_q_next = PolicyNetwork(batch_next_states)\n",
    "            target_q_next = TargetNetwork(batch_next_states)\n",
    "            online_max_action = torch.argmax(policy_q_next, dim=1, keepdim=True)\n",
    "            y = batch_rewards + (1 - batch_dones) * gamma * target_q_next.gather(1, online_max_action.long())\n",
    "\n",
    "        loss = F.mse_loss(PolicyNetwork(batch_states).gather(1, batch_actions.long()), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(PolicyNetwork.parameters(), clip_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if epsilon > explore_stop:\n",
    "            epsilon -= (explore_start - explore_stop) / explore\n",
    "\n",
    "        # print(\"POLICY PARAMS: \" , PolicyNetwork.state_dict())\n",
    "        if total_steps % update_steps == 0:\n",
    "            PolicyNetwork, TargetNetwork = update_target_params(PolicyNetwork, TargetNetwork)\n",
    "            # print(\"Model updated\")\n",
    "\n",
    "        if done: \n",
    "            # Write batch stats to log files \n",
    "            loss_at_end_of_episode = loss.item()\n",
    "\n",
    "            out_f.write(json.dumps({\n",
    "                'episode': episode,\n",
    "                'reward': episode_reward,\n",
    "                'total_steps': total_steps,\n",
    "                'length': episode_step,\n",
    "                'loss': loss_at_end_of_episode,\n",
    "                'epsilon': epsilon,\n",
    "            }) + '\\n')\n",
    "\n",
    "            out_f.flush()\n",
    "            \n",
    "\n",
    "            print(f\"Episode {episode}     |      Reward: {episode_reward}     |     length: {episode_step}      |      Loss: {loss_at_end_of_episode}    |     Total timesteps: {total_steps}\")\n",
    "        \n",
    "            if episode % 100 == 0:\n",
    "                torch.save(PolicyNetwork, f'{model_path}/E{episode}.pt')\n",
    "                print(f\"======Model saved.======\")\n",
    "            \n",
    "            break\n",
    "\n",
    "    if total_steps > 100000:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
