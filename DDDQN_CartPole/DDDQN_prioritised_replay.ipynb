{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trains CartPole agent with DDDQN PEP\n",
    "\n",
    "**Please expand the cells to view the code!**\n",
    "\n",
    "### Description\n",
    "This notebook trains the DDDQN model on a CartPole-v1 agent, using prioritised experience replay (PEP) which is determined using the SumTree data structure as introduced in the main reference. \n",
    "\n",
    "### How to Run:\n",
    "1. **Setup**: Ensure all necessary libraries are installed and the CartPole environment is set up. This was developed in **Python 3.10.14**.\n",
    "2. **Configure Hyperparameters**: Modify the hyperparameters to experiment with different model behaviors and optimization strategies.\n",
    "3. **Train the Model**: Run the code sequentially to train the DQN model, monitor training progress through log outputs, and save model checkpoints periodically.\n",
    "\n",
    "### References\n",
    "Minai, Y., 2023. Deep Q-learning (DQN) Tutorial with CartPole-v0 [Online]. Medium. Available from: https://medium.com/@ym1942/deep-q-learning-dqn-tutorial-with-cartpole-v0-5505dbd2409e [Accessed 8 May 2024].\n",
    "\n",
    "Paszke, A., n.d. Reinforcement Learning (DQN) Tutorial — PyTorch Tutorials 1.8.0 documentation [Online]. pytorch.org. Available from: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html [Accessed 8 May 2024].\n",
    "\n",
    "Perkins, H., 2022. youtube-rl-demos/vizdoom/vizdoom_011.py at vizdoom13 · hughperkins/youtube-rl-demos [Online]. GitHub. Available from: https://github.com/hughperkins/youtube-rl-demos/blob/vizdoom13/vizdoom/vizdoom_011.py [Accessed 8 May 2024].\n",
    "\n",
    "Simonini, T., 2018. Dueling Deep Q Learning with Doom (+ double DQNs and Prioritized Experience Replay).ipynb [Online]. Gist. Available from: https://gist.github.com/simoninithomas/d6adc6edb0a7f37d6323a5e3d2ab72ec#file-dueling-deep-q-learning-with-doom-double-dqns-and-prioritized-experience-replay-ipynb [Accessed 8 May 2024].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import random                 # Handling random number generation\n",
    "import time                   # Handling time calculation\n",
    "from skimage import transform # Help us to preprocess the frames\n",
    "\n",
    "from collections import deque # Ordered collection with ends\n",
    "import matplotlib.pyplot as plt  # Display graphs\n",
    "\n",
    "import warnings                  # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "learning_rate =  1e-5    # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 100000         # Total episodes for training\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# FIXED Q TARGETS HYPERPARAMETERS\n",
    "max_tau = 1000 #Tau is the C step where we update our target network\n",
    "update_steps = 4\n",
    "\n",
    "# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n",
    "explore_start = 0.1           # exploration probability at start\n",
    "explore_stop = 0.0001            # minimum exploration probability        # exponential decay rate for exploration prob\n",
    "explore = 20000\n",
    "\n",
    "clip_norm = 0.0001\n",
    "\n",
    "model_folder = f\"./models/prioritised_replay/explore/EX{explore_stop}_B{batch_size}_CN{clip_norm}_LR{learning_rate}\"\n",
    "os.makedirs(model_folder)\n",
    "log_path = f\"./logs/prioritised_replay/explore/EX{explore_stop}_B{batch_size}_CN{clip_norm}_LR{learning_rate}.txt\"\n",
    "\n",
    "# Q LEARNING hyperparameters\n",
    "gamma = 0.99               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "## If you have GPU change to 1million\n",
    "pretrain_length = 1000             # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 50000 ##100000                 # Number of experiences the Memory can keep\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc_value = nn.Linear(64, 256)\n",
    "        self.value = nn.Linear(256, 1)\n",
    "\n",
    "        self.fc_adv = nn.Linear(64, 256)\n",
    "        self.adv = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.relu(self.fc1(x))\n",
    "        value = self.relu(self.fc_value(y))\n",
    "        adv = self.relu(self.fc_adv(y))\n",
    "\n",
    "        value = self.value(value)\n",
    "        adv = self.adv(adv)\n",
    "\n",
    "        output = value + adv - torch.mean(adv, dim=1, keepdim=True)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def select_action(self, x):\n",
    "        with torch.no_grad():\n",
    "            Q = self.forward(x)\n",
    "            action_index = torch.argmax(Q, dim=1)\n",
    "        return action_index.item()\n",
    "    \n",
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou: \n",
    "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "tree_index  0 0  0  We fill the leaves from left to right\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "        \n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "        \n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "        \n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propagate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            \n",
    "            \"\"\"\n",
    "            Here we want to access the line above\n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "            \n",
    "                0\n",
    "               / \\\n",
    "              1   2\n",
    "             / \\ / \\\n",
    "            3  4 5  [6] \n",
    "            \n",
    "            If we are in leaf at index 6, we updated the priority score\n",
    "            We need then to update index 2 node\n",
    "            So tree_index = (tree_index - 1) // 2\n",
    "            tree_index = (6-1)//2\n",
    "            tree_index = 2 (because // round the result)\n",
    "            \"\"\"\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for experiences\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True: # the while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            \n",
    "            else: # downward search, always search for a higher priority node\n",
    "                \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                    \n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "            \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node\n",
    "    \n",
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree \n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        \n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        \n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "        \n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        \n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "    \n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "                                   \n",
    "            b_idx[i]= index\n",
    "            \n",
    "            experience = [data]\n",
    "            \n",
    "            memory_b.append(experience)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)\n",
    "\n",
    "\n",
    "def update_target_params(PolicyNetwork, TargetNetwork):\n",
    "    \"\"\"\n",
    "    Copy parameters of the Policy network to Target network.\n",
    "    \"\"\"\n",
    "    TargetNetwork_state_dict = TargetNetwork.state_dict()\n",
    "    PolicyNetwork_state_dict = PolicyNetwork.state_dict()\n",
    "    for key in PolicyNetwork_state_dict:\n",
    "        TargetNetwork_state_dict[key] = PolicyNetwork_state_dict[key]\n",
    "        TargetNetwork.load_state_dict(TargetNetwork_state_dict)\n",
    "\n",
    "    return PolicyNetwork, TargetNetwork   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(memory_size)\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "n_state = env.observation_space.shape[0]  # 4\n",
    "n_action = env.action_space.n\n",
    "\n",
    "state, _ = env.reset()\n",
    "\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    \n",
    "    action = random.randint(0,1)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        next_state = np.zeros(n_state)\n",
    "        experience = state, action, reward, next_state, done\n",
    "        #print(\"shape of a stack frame of dead: \", state.shape)\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Start a new episode\n",
    "        state, _ = env.reset()\n",
    "    else:\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DQNetwork\n",
    "PolicyNetwork = DDDQNet()\n",
    "\n",
    "# Instantiate the target network\n",
    "TargetNetwork = DDDQNet()\n",
    "\n",
    "optimizer = optim.RMSprop(PolicyNetwork.parameters(), lr=learning_rate)\n",
    "\n",
    "def select_action(epsilon, state):\n",
    "    \"\"\"\n",
    "    This function will do the part\n",
    "    With ϵ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\n",
    "    Input state is 4 stacked states. \n",
    "    \"\"\"\n",
    "\n",
    "    num = random.random()\n",
    "\n",
    "    if (num < epsilon):\n",
    "        action = random.randint(0, 1)\n",
    "        return action\n",
    "    # Greedy action\n",
    "    else:\n",
    "        \n",
    "        tensor_state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        action = PolicyNetwork.select_action(tensor_state)\n",
    "        return action\n",
    "\n",
    "epsilon = explore_start\n",
    "# Set Target network params\n",
    "PolicyNetwork, TargetNetwork = update_target_params(PolicyNetwork, TargetNetwork)\n",
    "\n",
    "out_f = open(log_path, 'w')\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "\n",
    "    episode_step = 0\n",
    "    episode_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    # print(\"state: \", state)\n",
    "    \n",
    "\n",
    "    for time_steps in range(10000):\n",
    "\n",
    "        episode_step += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        action = select_action(epsilon, state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done: \n",
    "            next_state = np.zeros(n_state)\n",
    "            experience = state, action, reward, next_state, done\n",
    "            memory.store(experience)\n",
    "\n",
    "        else: \n",
    "            experience = state, action, reward, next_state, done\n",
    "            # print(\"state: \", state)\n",
    "            memory.store(experience)\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "        tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "        \n",
    "        batch_states = torch.FloatTensor([each[0][0] for each in batch] )\n",
    "\n",
    "        batch_actions = torch.FloatTensor([each[0][1] for each in batch]).unsqueeze(1)\n",
    "        batch_rewards = torch.FloatTensor([each[0][2] for each in batch]).unsqueeze(1)\n",
    "        batch_next_states = torch.FloatTensor([each[0][3] for each in batch] ) # stacked frames of np arrays \n",
    "        batch_dones = torch.FloatTensor([each[0][4] for each in batch]).unsqueeze(1)\n",
    "\n",
    "        actions_index = batch_actions.detach().numpy().flatten()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            policy_q_next = PolicyNetwork(batch_next_states)\n",
    "            target_q_next = TargetNetwork(batch_next_states)\n",
    "            online_max_action = torch.argmax(policy_q_next, dim=1, keepdim=True)\n",
    "            y = batch_rewards + (1 - batch_dones) * gamma * target_q_next.gather(1, online_max_action.long())\n",
    "\n",
    "        loss = F.mse_loss(PolicyNetwork(batch_states).gather(1, batch_actions.long()), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_value_(PolicyNetwork.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(PolicyNetwork.parameters(), clip_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        terminal_np = batch_dones.detach().numpy().flatten()\n",
    "        policy_q_next_np = policy_q_next.detach().numpy()\n",
    "        target_q_next_np = target_q_next.detach().numpy()\n",
    "        rewards_np = batch_rewards.detach().numpy().flatten()\n",
    "\n",
    "        target_qs_batch = []\n",
    "\n",
    "        for i in range(0, len(batch)):\n",
    "\n",
    "            terminal = terminal_np[i]\n",
    "            action = np.argmax(policy_q_next_np[i])\n",
    "\n",
    "            if terminal:\n",
    "                target_qs_batch.append(torch.tensor(rewards_np[i], dtype=torch.float32)) # rewards_mb[i] is a TENSOR\n",
    "\n",
    "            else: \n",
    "                target = rewards_np[i] + gamma * target_q_next_np[i][action]\n",
    "\n",
    "                target_qs_batch.append(torch.tensor(target, dtype=torch.float32))\n",
    "\n",
    "        ACTIONS = [torch.tensor(np.array([0,1]), dtype=torch.float32), torch.tensor(np.array([1,0]), dtype=torch.float32)]\n",
    "        predicted_qs = [torch.sum(torch.mul(policy_q_next[i], ACTIONS[int(actions_index[i])])) for i in range(batch_size)]\n",
    "\n",
    "        absolute_errors = [torch.abs(torch.subtract(predicted_qs[i], target_qs_batch[i])) for i in range(batch_size)]\n",
    "        \n",
    "        absolute_errors_np = np.array([error.item() for error in absolute_errors])\n",
    "        ISWeights_list = ISWeights_mb.tolist()\n",
    "        memory.batch_update(tree_idx, absolute_errors_np)\n",
    "\n",
    "\n",
    "        if epsilon > explore_stop:\n",
    "            epsilon -= (explore_start - explore_stop) / explore\n",
    "\n",
    "        # print(\"POLICY PARAMS: \" , PolicyNetwork.state_dict())\n",
    "        if total_steps % update_steps == 0:\n",
    "            PolicyNetwork, TargetNetwork = update_target_params(PolicyNetwork, TargetNetwork)\n",
    "            # print(\"Model updated\")\n",
    "\n",
    "        if done: \n",
    "            # Write batch stats to log files \n",
    "            loss_at_end_of_episode = torch.mean(torch.stack(absolute_errors), dim=0).item()\n",
    "\n",
    "            out_f.write(json.dumps({\n",
    "                'episode': episode,\n",
    "                'reward': episode_reward,\n",
    "                'total_steps': total_steps,\n",
    "                'length': episode_step,\n",
    "                'loss': loss_at_end_of_episode,\n",
    "                'epsilon': epsilon,\n",
    "            }) + '\\n')\n",
    "\n",
    "            out_f.flush()\n",
    "            \n",
    "\n",
    "            print(f\"Episode {episode}     |      Reward: {episode_reward}     |     length: {episode_step}      |      Loss: {loss_at_end_of_episode}    |     Total timesteps: {total_steps}\")\n",
    "        \n",
    "            if episode % 100 == 0:\n",
    "                torch.save(PolicyNetwork, f\"{model_folder}/E_{episode}.pt\")\n",
    "                print(f\"======Model saved.======\")\n",
    "            \n",
    "            break\n",
    "\n",
    "    if total_steps > 100000:\n",
    "        break\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
