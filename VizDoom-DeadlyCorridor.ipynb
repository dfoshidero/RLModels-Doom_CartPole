{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390adbc2",
   "metadata": {},
   "source": [
    "## 1. Getting VizDoom up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fbd6a2-79b9-4fdb-b138-01ba25aafc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec3301f-9627-44c9-abbe-ff3605e68762",
   "metadata": {},
   "source": [
    "## 2. Converting it to a Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae853175-8aca-4ab6-ac5f-5b0e8de82789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2\n",
    "\n",
    "# Import environment checker\n",
    "from stable_baselines3.common import env_checker\n",
    "# Discrete(3).sample() returns a number from 0, 1, 2 -> used as index to select action\n",
    "# Box(low=0, high=10, shape=(10,10)).sample() -> getting 10x10 array with low=0 and high=10\n",
    "\n",
    "LEVEL = 'deadly_corridor'\n",
    "DOOM_SKILL = 's1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231c158-3c58-4a15-9237-1526e13565b1",
   "metadata": {},
   "source": [
    "### Environment configuration for Reward Shaping \n",
    "#### Additional game variables needed for this level:\n",
    "- DAMAGE_TAKEN (-)\n",
    "- DAMAGECOUNT (+)\n",
    "- SELECTED_WEAPON_AMMO (-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86fd0b97-dfae-4ad4-98ab-7268c8db3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False, config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg'):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(config)\n",
    "        \n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(7)\n",
    "\n",
    "        # Game variables: HEALTH DAMAGE_TAKEN DAMAGECOUNT SELECTED_WEAPON_AMMO \n",
    "        ## We want the change in these variable values, rather than the PiT values\n",
    "        self.damage_taken = 0\n",
    "        self.damagecount = 0\n",
    "        self.ammo = 52\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(7, dtype=np.uint8)\n",
    "        # Movement rewards encapsulates predefined reward in the environment config\n",
    "        movement_reward = self.game.make_action(actions[action], 4) # get action using index -> left, right, shoot\n",
    "\n",
    "        reward = 0\n",
    "        # Get all the other stuff we need to return \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            # ammo = self.game.get_state().game_variables[0] \n",
    "\n",
    "            # Reward shaping\n",
    "            game_variables = self.game.get_state().game_variables # get current PiT game variables\n",
    "            health, damage_taken, damagecount, ammo = game_variables # unpack\n",
    "\n",
    "            # calculate change in damage_taken, damagecount, ammo\n",
    "            damage_taken_delta = -damage_taken + self.damage_taken # disincentivizng us to take damage\n",
    "            self.damage_taken = damage_taken\n",
    "            damagecount_delta = damagecount - self.damagecount # increments by +1: incentivizing more damagecounts (1 damagecount = 1 reward)\n",
    "            self.damagecount = damagecount\n",
    "            ammo_delta = ammo - self.ammo # increments by -1: disincentiving us to take shots that miss\n",
    "                                          # damagecount and ammo will cancel each other out\n",
    "            self.ammo = ammo\n",
    "\n",
    "            # Pack everything into reward function (tuned weights)\n",
    "            reward = movement_reward + damage_taken_delta*10 + damagecount_delta*200 + ammo_delta*5\n",
    "            \n",
    "            info = ammo\n",
    "        # If we dont have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "            \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer  # Apply Grayscale\n",
    "\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18351768-7387-4813-99d0-a2faea4dc26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d7861e-c8cc-4b61-8962-717cbfbb1ff4",
   "metadata": {},
   "source": [
    "Environment checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2e6ac7a-9294-40a3-82d5-cc9d2f6f9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82a3ba49-76d5-4c4a-817b-7f0dbf3ccc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8503e-9e86-404e-9557-36db8c6cf6f2",
   "metadata": {},
   "source": [
    "## 3. Setup Callback\n",
    "Save model at different state of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd3a115d-fbab-449a-bd50-f19a8f3aac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for file nav\n",
    "import os\n",
    "# Import callback class from sb3\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "667e4204-553a-47eb-89d4-572b15f21eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        return True\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94544ad3-aea1-4559-95ab-8c0a30166e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_corridor'\n",
    "LOG_DIR = './logs/log_corridor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2a9d6be-2e1f-410f-9c40-e0e508cf86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of callback\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR) \n",
    "# after every 10000 steps of training the model, weights are saved for the pytorch agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0684619b-6997-4f27-9246-3eb05be026c9",
   "metadata": {},
   "source": [
    "## 5. Train Model using Curriculum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bedf5cea-9605-4739-97fa-719fe962cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PPO for training\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a93b07b6-5e9a-46d9-916c-7d918a5d4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment\n",
    "DOOM_SKILL = 's1'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfb926b1-88b4-469e-973e-0d4d0f7b313b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# n_steps: How many steps/frames the agent is going to take and store in the buffer \n",
    "# before run through training of actor and critique\n",
    "# ideally not too close to end of game (300) but somewhere close\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.00001, n_steps=8192, clip_range=.1, gamma=.95, gae_lambda=.9) \n",
    "# n_step = batchsize\n",
    "# clip_range clips the gradient\n",
    "# gae_lambda smoothing factor\n",
    "\n",
    "# if ep_len drop and ep_rew shoots up likely because agent is running straight forward\n",
    "# ep_len ideally should keep going down but not too fast, ep_rew should keep going up but not too fast (else unstable training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8fa60-2db2-4183-a544-42117c76eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04503f7-16e4-44e1-a4f2-90033b6a0428",
   "metadata": {},
   "source": [
    "#### Load saved best model and apply Curriculum Learning (S2 - S5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f717c-d6e4-4fa9-9bb9-5bdd4c713ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('./train/train_corridor/best_model_260000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e4298-ca04-43a6-9b18-612c656c0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment for S2\n",
    "DOOM_SKILL = 's2'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1219a1-f657-4513-a019-5c3f0a3972f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment for S3\n",
    "DOOM_SKILL = 's3'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55df27-e36c-43be-9254-ade956c1071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment for S4\n",
    "DOOM_SKILL = 's4'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e246b-a1c1-435e-a0cb-dfa0250f6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment for S5\n",
    "DOOM_SKILL = 's5'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91761cd-2bce-4135-b448-769512a9ea2e",
   "metadata": {},
   "source": [
    "## 5. Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9c2cdf8-4d70-48ae-a041-7a7e92a6d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import eval policy to test agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0eef9c9-95cf-4f40-8f8e-a17e1be7c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model from disc\n",
    "model = PPO.load('./train/train_defend/best_model_70000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d13640f-2645-4a61-a3fc-228f0ad5bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rendered envrironment\n",
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb1c402d-6861-465f-bc9a-3e8d5125ad18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/csqh/anaconda3/envs/pythonlab/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Evaluate mean reward for 10 games\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "052916fc-1094-46f9-b13c-67dfc6bb88a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a8eb004-51ee-4a3c-bd3d-8b2902d06525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 5.0 is 0\n",
      "Total Reward for episode 12.0 is 1\n",
      "Total Reward for episode 6.0 is 2\n",
      "Total Reward for episode 10.0 is 3\n",
      "Total Reward for episode 10.0 is 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in range(5):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs) # Use model to predict what action to take\n",
    "        obs, reward, done, _, info = env.step(action) # take the predicted action\n",
    "        time.sleep(0.25)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(total_reward, episode))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b424463e-7b5f-436f-ba78-d7190f4dd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d821e-e24d-4284-9a59-666ec5589cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
