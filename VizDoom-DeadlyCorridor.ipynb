{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390adbc2",
   "metadata": {},
   "source": [
    "## 1. Getting VizDoom up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41fbd6a2-79b9-4fdb-b138-01ba25aafc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec3301f-9627-44c9-abbe-ff3605e68762",
   "metadata": {},
   "source": [
    "## 2. Converting it to a Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae853175-8aca-4ab6-ac5f-5b0e8de82789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2\n",
    "\n",
    "# Import environment checker\n",
    "from stable_baselines3.common import env_checker\n",
    "# Discrete(3).sample() returns a number from 0, 1, 2 -> used as index to select action\n",
    "# Box(low=0, high=10, shape=(10,10)).sample() -> getting 10x10 array with low=0 and high=10\n",
    "\n",
    "LEVEL = 'deadly_corridor'\n",
    "DOOM_SKILL = 's1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231c158-3c58-4a15-9237-1526e13565b1",
   "metadata": {},
   "source": [
    "### Environment configuration for Reward Shaping \n",
    "#### Additional game variables needed for this level:\n",
    "- DAMAGE_TAKEN (-)\n",
    "- DAMAGECOUNT (+)\n",
    "- SELECTED_WEAPON_AMMO (-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86fd0b97-dfae-4ad4-98ab-7268c8db3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False, config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg'):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(config)\n",
    "        \n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(7)\n",
    "\n",
    "        # Game variables: HEALTH DAMAGE_TAKEN DAMAGECOUNT SELECTED_WEAPON_AMMO \n",
    "        ## We want the change in these variable values, rather than the PiT values\n",
    "        self.damage_taken = 0\n",
    "        self.damagecount = 0\n",
    "        self.ammo = 52\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(7, dtype=np.uint8)\n",
    "        # Movement rewards encapsulates predefined reward in the environment config\n",
    "        movement_reward = self.game.make_action(actions[action], 4) # get action using index -> left, right, shoot\n",
    "\n",
    "        reward = 0\n",
    "        # Get all the other stuff we need to return \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            # ammo = self.game.get_state().game_variables[0] \n",
    "\n",
    "            # Reward shaping\n",
    "            game_variables = self.game.get_state().game_variables # get current PiT game variables\n",
    "            health, damage_taken, damagecount, ammo = game_variables # unpack\n",
    "\n",
    "            # calculate change in damage_taken, damagecount, ammo\n",
    "            damage_taken_delta = -damage_taken + self.damage_taken # disincentivizng us to take damage\n",
    "            self.damage_taken = damage_taken\n",
    "            damagecount_delta = damagecount - self.damagecount # increments by +1: incentivizing more damagecounts (1 damagecount = 1 reward)\n",
    "            self.damagecount = damagecount\n",
    "            ammo_delta = ammo - self.ammo # increments by -1: disincentiving us to take shots that miss\n",
    "                                          # damagecount and ammo will cancel each other out\n",
    "            self.ammo = ammo\n",
    "\n",
    "            # Pack everything into reward function (tuned weights)\n",
    "            reward = movement_reward + damage_taken_delta*10 + damagecount_delta*200 + ammo_delta*5\n",
    "            \n",
    "            info = ammo\n",
    "        # If we dont have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "            \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer  # Apply Grayscale\n",
    "\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18351768-7387-4813-99d0-a2faea4dc26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d7861e-c8cc-4b61-8962-717cbfbb1ff4",
   "metadata": {},
   "source": [
    "Environment checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e6ac7a-9294-40a3-82d5-cc9d2f6f9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a3ba49-76d5-4c4a-817b-7f0dbf3ccc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8503e-9e86-404e-9557-36db8c6cf6f2",
   "metadata": {},
   "source": [
    "## 3. Setup Callback\n",
    "Save model at different state of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd3a115d-fbab-449a-bd50-f19a8f3aac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for file nav\n",
    "import os\n",
    "# Import callback class from sb3\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "667e4204-553a-47eb-89d4-572b15f21eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        return True\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94544ad3-aea1-4559-95ab-8c0a30166e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_corridor'\n",
    "LOG_DIR = './logs/log_corridor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2a9d6be-2e1f-410f-9c40-e0e508cf86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of callback\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR) \n",
    "# after every 10000 steps of training the model, weights are saved for the pytorch agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0684619b-6997-4f27-9246-3eb05be026c9",
   "metadata": {},
   "source": [
    "## 5. Train Model using Curriculum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bedf5cea-9605-4739-97fa-719fe962cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PPO for training\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a93b07b6-5e9a-46d9-916c-7d918a5d4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment\n",
    "DOOM_SKILL = 's1'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfb926b1-88b4-469e-973e-0d4d0f7b313b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# n_steps: How many steps/frames the agent is going to take and store in the buffer \n",
    "# before run through training of actor and critique\n",
    "# ideally not too close to end of game (300) but somewhere close\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.00001, n_steps=8192, clip_range=.1, gamma=.95, gae_lambda=.9) \n",
    "# n_step = batchsize\n",
    "# clip_range clips the gradient\n",
    "# gae_lambda smoothing factor\n",
    "\n",
    "# if ep_len drop and ep_rew shoots up likely because agent is running straight forward\n",
    "# ep_len ideally should keep going down but not too fast, ep_rew should keep going up but not too fast (else unstable training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecf8fa60-2db2-4183-a544-42117c76eff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/log_corridor/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 183      |\n",
      "|    ep_rew_mean     | 82.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 186          |\n",
      "|    ep_rew_mean          | 127          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 25           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 634          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024399245 |\n",
      "|    clip_fraction        | 0.133        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | -2.62e-05    |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.88e+03     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00332     |\n",
      "|    value_loss           | 6.58e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 184          |\n",
      "|    ep_rew_mean          | 191          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 23           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 1039         |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026270107 |\n",
      "|    clip_fraction        | 0.172        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.0122       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.68e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00407     |\n",
      "|    value_loss           | 6.85e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 149         |\n",
      "|    ep_rew_mean          | 230         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 1445        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002667071 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.039       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 3.83e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00336    |\n",
      "|    value_loss           | 7.16e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 114          |\n",
      "|    ep_rew_mean          | 266          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 22           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 1854         |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027083908 |\n",
      "|    clip_fraction        | 0.179        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.91        |\n",
      "|    explained_variance   | 0.0738       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.46e+03     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00382     |\n",
      "|    value_loss           | 9.14e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 107          |\n",
      "|    ep_rew_mean          | 282          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 2262         |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026945458 |\n",
      "|    clip_fraction        | 0.181        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.9         |\n",
      "|    explained_variance   | 0.131        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.1e+03      |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00257     |\n",
      "|    value_loss           | 1.06e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 94.5         |\n",
      "|    ep_rew_mean          | 350          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 2670         |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027202656 |\n",
      "|    clip_fraction        | 0.2          |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.89        |\n",
      "|    explained_variance   | 0.169        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.39e+03     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00319     |\n",
      "|    value_loss           | 1.09e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 95.1         |\n",
      "|    ep_rew_mean          | 326          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 3080         |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030271956 |\n",
      "|    clip_fraction        | 0.199        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.86        |\n",
      "|    explained_variance   | 0.181        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.48e+03     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    value_loss           | 1.16e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 88.2         |\n",
      "|    ep_rew_mean          | 332          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 3489         |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027580322 |\n",
      "|    clip_fraction        | 0.2          |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.86        |\n",
      "|    explained_variance   | 0.224        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 2.61e+03     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00246     |\n",
      "|    value_loss           | 1.13e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 84.5         |\n",
      "|    ep_rew_mean          | 413          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 3897         |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026891802 |\n",
      "|    clip_fraction        | 0.189        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.85        |\n",
      "|    explained_variance   | 0.242        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 9.59e+03     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00261     |\n",
      "|    value_loss           | 1.27e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 78.8         |\n",
      "|    ep_rew_mean          | 435          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 4306         |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031276306 |\n",
      "|    clip_fraction        | 0.212        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.82        |\n",
      "|    explained_variance   | 0.244        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.46e+03     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    value_loss           | 1.3e+04      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 71.6         |\n",
      "|    ep_rew_mean          | 441          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 4716         |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032854106 |\n",
      "|    clip_fraction        | 0.228        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.8         |\n",
      "|    explained_variance   | 0.242        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.3e+03      |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00247     |\n",
      "|    value_loss           | 1.45e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 71          |\n",
      "|    ep_rew_mean          | 530         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 20          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 5124        |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002891326 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.77       |\n",
      "|    explained_variance   | 0.301       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 8.29e+03    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00241    |\n",
      "|    value_loss           | 1.46e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 68.9         |\n",
      "|    ep_rew_mean          | 554          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 5534         |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031935778 |\n",
      "|    clip_fraction        | 0.239        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | 0.279        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 6.8e+03      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00319     |\n",
      "|    value_loss           | 1.59e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 72           |\n",
      "|    ep_rew_mean          | 604          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 5942         |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033049998 |\n",
      "|    clip_fraction        | 0.214        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | 0.286        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 5.53e+03     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    value_loss           | 1.52e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 64.9         |\n",
      "|    ep_rew_mean          | 569          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 6351         |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034876862 |\n",
      "|    clip_fraction        | 0.248        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.67        |\n",
      "|    explained_variance   | 0.277        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 7.19e+03     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    value_loss           | 1.45e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 66.3         |\n",
      "|    ep_rew_mean          | 600          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 6761         |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035947666 |\n",
      "|    clip_fraction        | 0.236        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 0.286        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.44e+04     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    value_loss           | 1.6e+04      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 63.6        |\n",
      "|    ep_rew_mean          | 609         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 20          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 7169        |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003660422 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1e+04       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.000542   |\n",
      "|    value_loss           | 1.51e+04    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:195\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 195\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:97\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 97\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Transpose the terminal observations\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, done \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dones):\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m, in \u001b[0;36mVizDoomGym.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     42\u001b[0m actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;241m7\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Movement rewards encapsulates predefined reward in the environment config\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m movement_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get action using index -> left, right, shoot\u001b[39;00m\n\u001b[1;32m     46\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Get all the other stuff we need to return \u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04503f7-16e4-44e1-a4f2-90033b6a0428",
   "metadata": {},
   "source": [
    "#### Load saved best model and apply Curriculum Learning (S2 - S5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f717c-d6e4-4fa9-9bb9-5bdd4c713ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('./train/train_corridor/best_model_260000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e4298-ca04-43a6-9b18-612c656c0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment for S2\n",
    "DOOM_SKILL = 's2'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1219a1-f657-4513-a019-5c3f0a3972f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment for S3\n",
    "DOOM_SKILL = 's3'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55df27-e36c-43be-9254-ade956c1071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment for S4\n",
    "DOOM_SKILL = 's4'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e246b-a1c1-435e-a0cb-dfa0250f6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment for S5\n",
    "DOOM_SKILL = 's5'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91761cd-2bce-4135-b448-769512a9ea2e",
   "metadata": {},
   "source": [
    "## 5. Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c2cdf8-4d70-48ae-a041-7a7e92a6d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import eval policy to test agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eef9c9-95cf-4f40-8f8e-a17e1be7c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model from disc\n",
    "model = PPO.load('./train/train_defend/best_model_70000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d13640f-2645-4a61-a3fc-228f0ad5bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rendered envrironment\n",
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c402d-6861-465f-bc9a-3e8d5125ad18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/csqh/anaconda3/envs/pythonlab/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Evaluate mean reward for 10 games\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052916fc-1094-46f9-b13c-67dfc6bb88a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8eb004-51ee-4a3c-bd3d-8b2902d06525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 5.0 is 0\n",
      "Total Reward for episode 12.0 is 1\n",
      "Total Reward for episode 6.0 is 2\n",
      "Total Reward for episode 10.0 is 3\n",
      "Total Reward for episode 10.0 is 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in range(5):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs) # Use model to predict what action to take\n",
    "        obs, reward, done, _, info = env.step(action) # take the predicted action\n",
    "        time.sleep(0.25)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(total_reward, episode))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424463e-7b5f-436f-ba78-d7190f4dd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d821e-e24d-4284-9a59-666ec5589cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
