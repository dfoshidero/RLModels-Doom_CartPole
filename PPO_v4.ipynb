{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0547f-535d-49a1-a0cb-2df24c584a95",
   "metadata": {},
   "source": [
    "## Initialize VizDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cddcc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary\n",
    "#!pip install vizdoom\n",
    "#!pip install opencv-python\n",
    "#!pip install pandas\n",
    "#!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34143102-2b66-4ee2-9f76-f6db917d2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f8b51-d30c-4c9f-a8b1-8e24b891a576",
   "metadata": {},
   "source": [
    "## Make it a Gym Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df5aed06-301c-43ff-a0ab-54dca32e78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2\n",
    "\n",
    "LEVEL = 'defend_the_center'\n",
    "DOOM_SKILL = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47d02cc2-14b1-4eb2-a032-add0d7ed26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('VizDoom/scenarios/defend_the_center.cfg')\n",
    "        \n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 4) # get action using index -> left, right, shoot\n",
    "        \n",
    "        # Get all the other stuff we need to return \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            ammo = self.game.get_state().game_variables[0] \n",
    "            info = ammo\n",
    "        # If we dont have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "            \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer  # Apply Grayscale\n",
    "\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48eacd6-a69d-4d14-b890-83f76c4a5e67",
   "metadata": {},
   "source": [
    "## Custom PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7728b4d-a680-4896-b0ea-a68a6683d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39228483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy distribution: tensor([[0.0283, 0.1530, 0.8187]], grad_fn=<SoftmaxBackward0>)\n",
      "Policy distribution: tensor([[0.0283, 0.1530, 0.8187]], grad_fn=<SoftmaxBackward0>)\n",
      "Policy distribution: tensor([[nan, nan, nan]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Favour-Daniel\\AppData\\Local\\Temp\\ipykernel_24792\\2550922741.py:108: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
      "C:\\Users\\Favour-Daniel\\AppData\\Local\\Temp\\ipykernel_24792\\2550922741.py:123: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic_loss = F.mse_loss(V, batch_rtgs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (1, 3)) of distribution Categorical(probs: torch.Size([1, 3])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan]], grad_fn=<DivBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 158\u001b[0m\n\u001b[0;32m    155\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m ppo_agent \u001b[38;5;241m=\u001b[39m PPO(env, in_channels, n_actions, device)\n\u001b[1;32m--> 158\u001b[0m \u001b[43mppo_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train for 500 episodes\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[36], line 57\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m     56\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect_data()\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Data = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[36], line 112\u001b[0m, in \u001b[0;36mPPO.update_policy\u001b[1;34m(self, batch_data)\u001b[0m\n\u001b[0;32m    108\u001b[0m A_k \u001b[38;5;241m=\u001b[39m (A_k \u001b[38;5;241m-\u001b[39m A_k\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;241m/\u001b[39m (A_k\u001b[38;5;241m.\u001b[39mstd() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_updates_per_iteration):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# Recalculate V and log_probs for the current policy\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m     V, curr_log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_acts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# Calculate the ratio (pi_theta / pi_theta_old)\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     ratios \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(curr_log_probs \u001b[38;5;241m-\u001b[39m batch_log_probs)\n",
      "Cell \u001b[1;32mIn[36], line 146\u001b[0m, in \u001b[0;36mPPO.evaluate\u001b[1;34m(self, batch_obs, batch_acts)\u001b[0m\n\u001b[0;32m    144\u001b[0m policy_dist, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(batch_obs)  \u001b[38;5;66;03m# Changed here\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolicy distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m, policy_dist)\n\u001b[1;32m--> 146\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_dist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(batch_acts)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value, log_probs\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom\\lib\\site-packages\\torch\\distributions\\categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     69\u001b[0m )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom\\lib\\site-packages\\torch\\distributions\\distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m             )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (1, 3)) of distribution Categorical(probs: torch.Size([1, 3])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan]], grad_fn=<DivBackward0>)"
     ]
    }
   ],
   "source": [
    "## PPO \n",
    "# Convolutional output size calculator\n",
    "def conv2d_size_out(size, kernel_size = 3, stride = 2, padding = 0):\n",
    "    return (size + 2 * padding - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, n_output):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Temporarily assume some output size after convolution\n",
    "        # This should ideally be calculated based on input size\n",
    "        self.feature_count = 64 * conv2d_size_out(conv2d_size_out(conv2d_size_out(100, 8, 4), 4, 2), 3, 1) * \\\n",
    "                        conv2d_size_out(conv2d_size_out(conv2d_size_out(160, 8, 4), 4, 2), 3, 1)\n",
    "\n",
    "        self.fc = nn.Linear(self.feature_count, 512)\n",
    "        self.actor = nn.Linear(512, n_output)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(-1, self.feature_count)  # Use reshape instead of view\n",
    "        x = F.relu(self.fc(x))\n",
    "        action_probs = F.softmax(self.actor(x), dim=1)\n",
    "        value = self.critic(x)\n",
    "        return action_probs, value\n",
    "    \n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env, in_channels, n_actions, device):\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        self._init_hyperparameters()\n",
    "        self.actor = ActorCriticNetwork(in_channels, n_actions)\n",
    "        self.critic = ActorCriticNetwork(in_channels, 1)\n",
    "\n",
    "        #Initialise optimizer\n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=self.lr)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=self.lr)\n",
    "\n",
    "    def _init_hyperparameters(self):\n",
    "        self.timesteps_per_batch = 1\n",
    "        self.max_timesteps_per_episode = 1600\n",
    "        self.gamma = 0.95\n",
    "        self.n_updates_per_iteration = 5\n",
    "        self.lr = 0.005\n",
    "        self.clip = 0.2  # Clipping parameter for PPO\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            batch_data = self.collect_data()\n",
    "            self.update_policy(batch_data)\n",
    "            print(f\"Episode {episode}: Data = {batch_data}\")\n",
    "\n",
    "    def collect_data(self):\n",
    "        batch_obs = []\n",
    "        batch_acts = []\n",
    "        batch_log_probs = []\n",
    "        batch_rews = []\n",
    "        batch_values = []\n",
    "        t = 0\n",
    "\n",
    "        while t < self.timesteps_per_batch:\n",
    "            state, _ = self.env.reset()\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).permute(0, 3, 1, 2).to(self.device)\n",
    "            done = False\n",
    "            while not done and t < self.timesteps_per_batch:\n",
    "                policy_dist, value = self.actor(state)  # Changed here\n",
    "                action = policy_dist.multinomial(num_samples=1).detach()\n",
    "                log_prob = torch.log(policy_dist.squeeze(0)[action])\n",
    "                next_state, reward, done, _, _ = self.env.step(action.item())\n",
    "\n",
    "                batch_obs.append(state)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                batch_rews.append(reward)\n",
    "                batch_values.append(value)\n",
    "\n",
    "                state = torch.from_numpy(next_state).float().unsqueeze(0).permute(0, 3, 1, 2).to(self.device)\n",
    "                t += 1\n",
    "\n",
    "        rewards_to_go = PPO.calculate_rewards_to_go(torch.tensor(batch_rews, dtype=torch.float), self.gamma)\n",
    "        return {\n",
    "            \"obs\": torch.cat(batch_obs),\n",
    "            \"acts\": torch.stack(batch_acts),\n",
    "            \"log_probs\": torch.stack(batch_log_probs),\n",
    "            \"rewards\": torch.tensor(batch_rews, dtype=torch.float),\n",
    "            \"values\": torch.cat(batch_values),\n",
    "            \"rewards_to_go\": rewards_to_go  # Add this line\n",
    "        }\n",
    "\n",
    "    def update_policy(self, batch_data):\n",
    "        batch_obs = batch_data['obs']\n",
    "        batch_acts = batch_data['acts']\n",
    "        batch_log_probs = batch_data['log_probs']\n",
    "        batch_rtgs = batch_data['rewards_to_go']  # Assuming rewards to go are calculated and passed\n",
    "\n",
    "        # Calculate current V and log probs\n",
    "        current_V, current_log_probs = self.evaluate(batch_obs, batch_acts)\n",
    "\n",
    "        # Compute advantages and normalize\n",
    "        A_k = batch_rtgs - current_V.detach()\n",
    "        A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "\n",
    "        for _ in range(self.n_updates_per_iteration):\n",
    "            # Recalculate V and log_probs for the current policy\n",
    "            V, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
    "\n",
    "            # Calculate the ratio (pi_theta / pi_theta_old)\n",
    "            ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "\n",
    "            # Actor Loss: PPO's clipped objective\n",
    "            surr1 = ratios * A_k\n",
    "            surr2 = torch.clamp(ratios, 1.0 - self.clip, 1.0 + self.clip) * A_k\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            # Critic Loss\n",
    "            critic_loss = F.mse_loss(V, batch_rtgs)\n",
    "\n",
    "            # Perform backward propagation for critic\n",
    "            self.critic_optim.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            self.critic_optim.step()\n",
    "\n",
    "            # Perform backward propagation for actor\n",
    "            self.actor_optim.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            self.actor_optim.step()\n",
    "\n",
    "    def calculate_rewards_to_go(rewards, gamma):\n",
    "        n = len(rewards)\n",
    "        rewards_to_go = torch.zeros_like(rewards)\n",
    "        for i in reversed(range(n)):\n",
    "            rewards_to_go[i] = rewards[i] + (gamma * rewards_to_go[i + 1] if i + 1 < n else 0)\n",
    "        return rewards_to_go\n",
    "\n",
    "    def evaluate(self, batch_obs, batch_acts):\n",
    "        # Get the policy distribution and value estimate for the given observations\n",
    "        policy_dist, value = self.actor(batch_obs)  # Changed here\n",
    "        dist = Categorical(policy_dist)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "\n",
    "        return value, log_probs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = VizDoomGym(render=True)\n",
    "    in_channels = 1  # Assuming grayscale input\n",
    "    n_actions = env.action_space.n\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ppo_agent = PPO(env, in_channels, n_actions, device)\n",
    "    ppo_agent.train(500)  # Train for 500 episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7664881-22e2-42a6-8ab9-693188f1d2c3",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
