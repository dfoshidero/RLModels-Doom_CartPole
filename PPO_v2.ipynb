{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doom Game Using PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize VizDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make it a Gym Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('VizDoom/scenarios/basic.cfg')\n",
    "        self.game.set_window_visible(render)\n",
    "        self.game.init()\n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 4) # get action using index -> left, right, shoot\n",
    "        \n",
    "        # Get all the other stuff we need to return \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            ammo = self.game.get_state().game_variables[0] \n",
    "            info = ammo\n",
    "        # If we dont have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        # Set seed if provided\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "    \n",
    "        self.game.new_episode()  # Start a new episode regardless of the seed\n",
    "\n",
    "        # After starting a new episode, get the initial state\n",
    "        if self.game.get_state() is not None:\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            return self.grayscale(state)\n",
    "        else:\n",
    "            # Return a zero array if no state is available (e.g., at episode start)\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.uint8)\n",
    "\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Pytorch PPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Actor-Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size = 3, stride = 2, padding = 0):\n",
    "    return (size + 2 * padding - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "# Calculate output size after each convolutional layer\n",
    "h_w = [100, 160]  # initial height and width\n",
    "h_w = [conv2d_size_out(x, 8, 4) for x in h_w]  # after first conv layer\n",
    "h_w = [conv2d_size_out(x, 4, 2) for x in h_w]  # after second conv layer\n",
    "h_w = [conv2d_size_out(x, 3, 1) for x in h_w]  # after third conv layer\n",
    "\n",
    "# Calculate the total number of features before the linear layer\n",
    "feature_count = 64 * h_w[0] * h_w[1]\n",
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, n_output):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Temporarily assume some output size after convolution\n",
    "        # This should ideally be calculated based on input size\n",
    "        self.feature_count = 64 * conv2d_size_out(conv2d_size_out(conv2d_size_out(100, 8, 4), 4, 2), 3, 1) * \\\n",
    "                        conv2d_size_out(conv2d_size_out(conv2d_size_out(160, 8, 4), 4, 2), 3, 1)\n",
    "\n",
    "        self.fc = nn.Linear(self.feature_count, 512)\n",
    "        self.actor = nn.Linear(512, n_output)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(-1, self.feature_count)  # Use reshape instead of view\n",
    "        x = F.relu(self.fc(x))\n",
    "        action_probs = F.softmax(self.actor(x), dim=1)\n",
    "        value = self.critic(x)\n",
    "        return action_probs, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Calculate the returns using the rewards and the next state's value estimate.\n",
    "    \"\"\"\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def ppo_update(policy_net, optimizer, states, actions, log_probs_old, returns, advantages, clip_param=0.2):\n",
    "    \"\"\"\n",
    "    Perform one update step of the PPO algorithm.\n",
    "    \"\"\"\n",
    "    for _ in range(10):  # PPO epochs\n",
    "        # Get the log probabilities and state values from the model\n",
    "        log_probs, state_values = policy_net(states)\n",
    "        # Select the log probabilities for the actions taken\n",
    "        log_probs = log_probs.gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        # Calculate entropy to encourage exploration\n",
    "        entropy = -(log_probs * torch.exp(log_probs)).sum(1, keepdim=True).mean()\n",
    "\n",
    "        # Calculate the ratio (pi_theta / pi_theta_old) for the actions taken\n",
    "        ratios = torch.exp(log_probs - log_probs_old)\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1.0 - clip_param, 1.0 + clip_param) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = F.smooth_l1_loss(state_values.squeeze(-1), returns)\n",
    "        loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Completed. Total Reward: -380.0\n",
      "Episode 2: Completed. Total Reward: 5.0\n",
      "Episode 3: Completed. Total Reward: -390.0\n",
      "Episode 4: Completed. Total Reward: 95.0\n",
      "Episode 5: Completed. Total Reward: -385.0\n",
      "Episode 6: Completed. Total Reward: 95.0\n",
      "Episode 7: Completed. Total Reward: -380.0\n",
      "Episode 8: Completed. Total Reward: -380.0\n",
      "Episode 9: Completed. Total Reward: 95.0\n",
      "Episode 10: Completed. Total Reward: -380.0\n",
      "Episode 11: Completed. Total Reward: 95.0\n",
      "Episode 12: Completed. Total Reward: -385.0\n",
      "Episode 13: Completed. Total Reward: -385.0\n",
      "Episode 14: Completed. Total Reward: -390.0\n",
      "Episode 15: Completed. Total Reward: 95.0\n",
      "Episode 16: Completed. Total Reward: -390.0\n",
      "Episode 17: Completed. Total Reward: 95.0\n",
      "Episode 18: Completed. Total Reward: -390.0\n",
      "Episode 19: Completed. Total Reward: -390.0\n",
      "Episode 20: Completed. Total Reward: -10.0\n",
      "Episode 21: Completed. Total Reward: 95.0\n",
      "Episode 22: Completed. Total Reward: -385.0\n",
      "Episode 23: Completed. Total Reward: -385.0\n",
      "Episode 24: Completed. Total Reward: 45.0\n",
      "Episode 25: Completed. Total Reward: 45.0\n",
      "Episode 26: Completed. Total Reward: -390.0\n",
      "Episode 27: Completed. Total Reward: -390.0\n",
      "Episode 28: Completed. Total Reward: -390.0\n",
      "Episode 29: Completed. Total Reward: 47.0\n",
      "Episode 30: Completed. Total Reward: -52.0\n",
      "Episode 31: Completed. Total Reward: -385.0\n",
      "Episode 32: Completed. Total Reward: 95.0\n",
      "Episode 33: Completed. Total Reward: -395.0\n",
      "Episode 34: Completed. Total Reward: -385.0\n",
      "Episode 35: Completed. Total Reward: -390.0\n",
      "Episode 36: Completed. Total Reward: 76.0\n",
      "Episode 37: Completed. Total Reward: 25.0\n",
      "Episode 38: Completed. Total Reward: -385.0\n",
      "Episode 39: Completed. Total Reward: -385.0\n",
      "Episode 40: Completed. Total Reward: -385.0\n",
      "Episode 41: Completed. Total Reward: 95.0\n",
      "Episode 42: Completed. Total Reward: 87.0\n",
      "Episode 43: Completed. Total Reward: -375.0\n",
      "Episode 44: Completed. Total Reward: 91.0\n",
      "Episode 45: Completed. Total Reward: 70.0\n",
      "Episode 46: Completed. Total Reward: -390.0\n",
      "Episode 47: Completed. Total Reward: -380.0\n",
      "Episode 48: Completed. Total Reward: -385.0\n",
      "Episode 49: Completed. Total Reward: 14.0\n",
      "Episode 50: Completed. Total Reward: 95.0\n",
      "Episode 51: Completed. Total Reward: -375.0\n",
      "Episode 52: Completed. Total Reward: 76.0\n",
      "Episode 53: Completed. Total Reward: 95.0\n",
      "Episode 54: Completed. Total Reward: 95.0\n",
      "Episode 55: Completed. Total Reward: 95.0\n",
      "Episode 56: Completed. Total Reward: 95.0\n",
      "Episode 57: Completed. Total Reward: 95.0\n",
      "Episode 58: Completed. Total Reward: -380.0\n",
      "Episode 59: Completed. Total Reward: 95.0\n",
      "Episode 60: Completed. Total Reward: -395.0\n",
      "Episode 61: Completed. Total Reward: -395.0\n",
      "Episode 62: Completed. Total Reward: -385.0\n",
      "Episode 63: Completed. Total Reward: -385.0\n",
      "Episode 64: Completed. Total Reward: -380.0\n",
      "Episode 65: Completed. Total Reward: 72.0\n",
      "Episode 66: Completed. Total Reward: -375.0\n",
      "Episode 67: Completed. Total Reward: -395.0\n",
      "Episode 68: Completed. Total Reward: -390.0\n",
      "Episode 69: Completed. Total Reward: 95.0\n",
      "Episode 70: Completed. Total Reward: 83.0\n",
      "Episode 71: Completed. Total Reward: -390.0\n",
      "Episode 72: Completed. Total Reward: 95.0\n",
      "Episode 73: Completed. Total Reward: -390.0\n",
      "Episode 74: Completed. Total Reward: -390.0\n",
      "Episode 75: Completed. Total Reward: -385.0\n",
      "Episode 76: Completed. Total Reward: -390.0\n",
      "Episode 77: Completed. Total Reward: 95.0\n",
      "Episode 78: Completed. Total Reward: 95.0\n",
      "Episode 79: Completed. Total Reward: -390.0\n",
      "Episode 80: Completed. Total Reward: 91.0\n",
      "Episode 81: Completed. Total Reward: -390.0\n",
      "Episode 82: Completed. Total Reward: 95.0\n",
      "Episode 83: Completed. Total Reward: -395.0\n",
      "Episode 84: Completed. Total Reward: -390.0\n",
      "Episode 85: Completed. Total Reward: 95.0\n",
      "Episode 86: Completed. Total Reward: 95.0\n",
      "Episode 87: Completed. Total Reward: -375.0\n",
      "Episode 88: Completed. Total Reward: 95.0\n",
      "Episode 89: Completed. Total Reward: 95.0\n",
      "Episode 90: Completed. Total Reward: 95.0\n",
      "Episode 91: Completed. Total Reward: -385.0\n",
      "Episode 92: Completed. Total Reward: -39.0\n",
      "Episode 93: Completed. Total Reward: 83.0\n",
      "Episode 94: Completed. Total Reward: 8.0\n",
      "Episode 95: Completed. Total Reward: -390.0\n",
      "Episode 96: Completed. Total Reward: 91.0\n",
      "Episode 97: Completed. Total Reward: -385.0\n",
      "Episode 98: Completed. Total Reward: 53.0\n",
      "Episode 99: Completed. Total Reward: -400.0\n",
      "Episode 100: Completed. Total Reward: 87.0\n",
      "Episode 101: Completed. Total Reward: -390.0\n",
      "Episode 102: Completed. Total Reward: -380.0\n",
      "Episode 103: Completed. Total Reward: 91.0\n",
      "Episode 104: Completed. Total Reward: -385.0\n",
      "Episode 105: Completed. Total Reward: 24.0\n",
      "Episode 106: Completed. Total Reward: -400.0\n",
      "Episode 107: Completed. Total Reward: 95.0\n",
      "Episode 108: Completed. Total Reward: 70.0\n",
      "Episode 109: Completed. Total Reward: 95.0\n",
      "Episode 110: Completed. Total Reward: -385.0\n",
      "Episode 111: Completed. Total Reward: 95.0\n",
      "Episode 112: Completed. Total Reward: -390.0\n",
      "Episode 113: Completed. Total Reward: -390.0\n",
      "Episode 114: Completed. Total Reward: -400.0\n",
      "Episode 115: Completed. Total Reward: -395.0\n",
      "Episode 116: Completed. Total Reward: 95.0\n",
      "Episode 117: Completed. Total Reward: 95.0\n",
      "Episode 118: Completed. Total Reward: -390.0\n",
      "Episode 119: Completed. Total Reward: 95.0\n",
      "Episode 120: Completed. Total Reward: 67.0\n",
      "Episode 121: Completed. Total Reward: 95.0\n",
      "Episode 122: Completed. Total Reward: -390.0\n",
      "Episode 123: Completed. Total Reward: -385.0\n",
      "Episode 124: Completed. Total Reward: -395.0\n",
      "Episode 125: Completed. Total Reward: 71.0\n",
      "Episode 126: Completed. Total Reward: 33.0\n",
      "Episode 127: Completed. Total Reward: 95.0\n",
      "Episode 128: Completed. Total Reward: 95.0\n",
      "Episode 129: Completed. Total Reward: 95.0\n",
      "Episode 130: Completed. Total Reward: -395.0\n",
      "Episode 131: Completed. Total Reward: -17.0\n",
      "Episode 132: Completed. Total Reward: 95.0\n",
      "Episode 133: Completed. Total Reward: 95.0\n",
      "Episode 134: Completed. Total Reward: -390.0\n",
      "Episode 135: Completed. Total Reward: -385.0\n",
      "Episode 136: Completed. Total Reward: 23.0\n",
      "Episode 137: Completed. Total Reward: 37.0\n",
      "Episode 138: Completed. Total Reward: 87.0\n",
      "Episode 139: Completed. Total Reward: -390.0\n",
      "Episode 140: Completed. Total Reward: 95.0\n",
      "Episode 141: Completed. Total Reward: -385.0\n",
      "Episode 142: Completed. Total Reward: -390.0\n",
      "Episode 143: Completed. Total Reward: 95.0\n",
      "Episode 144: Completed. Total Reward: -380.0\n",
      "Episode 145: Completed. Total Reward: 95.0\n",
      "Episode 146: Completed. Total Reward: 45.0\n",
      "Episode 147: Completed. Total Reward: 59.0\n",
      "Episode 148: Completed. Total Reward: -385.0\n",
      "Episode 149: Completed. Total Reward: -390.0\n",
      "Episode 150: Completed. Total Reward: -390.0\n",
      "Episode 151: Completed. Total Reward: 95.0\n",
      "Episode 152: Completed. Total Reward: -400.0\n",
      "Episode 153: Completed. Total Reward: 83.0\n",
      "Episode 154: Completed. Total Reward: -385.0\n",
      "Episode 155: Completed. Total Reward: -390.0\n",
      "Episode 156: Completed. Total Reward: -390.0\n",
      "Episode 157: Completed. Total Reward: 91.0\n",
      "Episode 158: Completed. Total Reward: 49.0\n",
      "Episode 159: Completed. Total Reward: -390.0\n",
      "Episode 160: Completed. Total Reward: 57.0\n",
      "Episode 161: Completed. Total Reward: 95.0\n",
      "Episode 162: Completed. Total Reward: -390.0\n",
      "Episode 163: Completed. Total Reward: 95.0\n",
      "Episode 164: Completed. Total Reward: -385.0\n",
      "Episode 165: Completed. Total Reward: 28.0\n",
      "Episode 166: Completed. Total Reward: -390.0\n",
      "Episode 167: Completed. Total Reward: -395.0\n",
      "Episode 168: Completed. Total Reward: -380.0\n",
      "Episode 169: Completed. Total Reward: -390.0\n",
      "Episode 170: Completed. Total Reward: 48.0\n",
      "Episode 171: Completed. Total Reward: 95.0\n",
      "Episode 172: Completed. Total Reward: -385.0\n",
      "Episode 173: Completed. Total Reward: 95.0\n",
      "Episode 174: Completed. Total Reward: -390.0\n",
      "Episode 175: Completed. Total Reward: -400.0\n",
      "Episode 176: Completed. Total Reward: 62.0\n",
      "Episode 177: Completed. Total Reward: -375.0\n",
      "Episode 178: Completed. Total Reward: -90.0\n",
      "Episode 179: Completed. Total Reward: 95.0\n",
      "Episode 180: Completed. Total Reward: 95.0\n",
      "Episode 181: Completed. Total Reward: -380.0\n",
      "Episode 182: Completed. Total Reward: 45.0\n",
      "Episode 183: Completed. Total Reward: 95.0\n",
      "Episode 184: Completed. Total Reward: -52.0\n",
      "Episode 185: Completed. Total Reward: -380.0\n",
      "Episode 186: Completed. Total Reward: -385.0\n",
      "Episode 187: Completed. Total Reward: -395.0\n",
      "Episode 188: Completed. Total Reward: -43.0\n",
      "Episode 189: Completed. Total Reward: 66.0\n",
      "Episode 190: Completed. Total Reward: 95.0\n",
      "Episode 191: Completed. Total Reward: 87.0\n",
      "Episode 192: Completed. Total Reward: 76.0\n",
      "Episode 193: Completed. Total Reward: -390.0\n",
      "Episode 194: Completed. Total Reward: 95.0\n",
      "Episode 195: Completed. Total Reward: 95.0\n",
      "Episode 196: Completed. Total Reward: 66.0\n",
      "Episode 197: Completed. Total Reward: -395.0\n",
      "Episode 198: Completed. Total Reward: -400.0\n",
      "Episode 199: Completed. Total Reward: -390.0\n",
      "Episode 200: Completed. Total Reward: 46.0\n",
      "Episode 201: Completed. Total Reward: -380.0\n",
      "Episode 202: Completed. Total Reward: -380.0\n",
      "Episode 203: Completed. Total Reward: 95.0\n",
      "Episode 204: Completed. Total Reward: -380.0\n",
      "Episode 205: Completed. Total Reward: -385.0\n",
      "Episode 206: Completed. Total Reward: -400.0\n",
      "Episode 207: Completed. Total Reward: -385.0\n",
      "Episode 208: Completed. Total Reward: -380.0\n",
      "Episode 209: Completed. Total Reward: -385.0\n",
      "Episode 210: Completed. Total Reward: 91.0\n",
      "Episode 211: Completed. Total Reward: -385.0\n",
      "Episode 212: Completed. Total Reward: 95.0\n",
      "Episode 213: Completed. Total Reward: -385.0\n",
      "Episode 214: Completed. Total Reward: 95.0\n",
      "Episode 215: Completed. Total Reward: -385.0\n",
      "Episode 216: Completed. Total Reward: -390.0\n",
      "Episode 217: Completed. Total Reward: -385.0\n",
      "Episode 218: Completed. Total Reward: 9.0\n",
      "Episode 219: Completed. Total Reward: -390.0\n",
      "Episode 220: Completed. Total Reward: 95.0\n",
      "Episode 221: Completed. Total Reward: 72.0\n",
      "Episode 222: Completed. Total Reward: -385.0\n",
      "Episode 223: Completed. Total Reward: 51.0\n",
      "Episode 224: Completed. Total Reward: -390.0\n",
      "Episode 225: Completed. Total Reward: -385.0\n",
      "Episode 226: Completed. Total Reward: -390.0\n",
      "Episode 227: Completed. Total Reward: 67.0\n",
      "Episode 228: Completed. Total Reward: 95.0\n",
      "Episode 229: Completed. Total Reward: -385.0\n",
      "Episode 230: Completed. Total Reward: -390.0\n",
      "Episode 231: Completed. Total Reward: -395.0\n",
      "Episode 232: Completed. Total Reward: 95.0\n",
      "Episode 233: Completed. Total Reward: -385.0\n",
      "Episode 234: Completed. Total Reward: 95.0\n",
      "Episode 235: Completed. Total Reward: 72.0\n",
      "Episode 236: Completed. Total Reward: -390.0\n",
      "Episode 237: Completed. Total Reward: -385.0\n",
      "Episode 238: Completed. Total Reward: 91.0\n",
      "Episode 239: Completed. Total Reward: 52.0\n",
      "Episode 240: Completed. Total Reward: -390.0\n",
      "Episode 241: Completed. Total Reward: -380.0\n",
      "Episode 242: Completed. Total Reward: 95.0\n",
      "Episode 243: Completed. Total Reward: 95.0\n",
      "Episode 244: Completed. Total Reward: -385.0\n",
      "Episode 245: Completed. Total Reward: -385.0\n",
      "Episode 246: Completed. Total Reward: 66.0\n",
      "Episode 247: Completed. Total Reward: -390.0\n",
      "Episode 248: Completed. Total Reward: 95.0\n",
      "Episode 249: Completed. Total Reward: 95.0\n",
      "Episode 250: Completed. Total Reward: 25.0\n",
      "Episode 251: Completed. Total Reward: -395.0\n",
      "Episode 252: Completed. Total Reward: -390.0\n",
      "Episode 253: Completed. Total Reward: -385.0\n",
      "Episode 254: Completed. Total Reward: 95.0\n",
      "Episode 255: Completed. Total Reward: 95.0\n",
      "Episode 256: Completed. Total Reward: -400.0\n",
      "Episode 257: Completed. Total Reward: -390.0\n",
      "Episode 258: Completed. Total Reward: 95.0\n",
      "Episode 259: Completed. Total Reward: -390.0\n",
      "Episode 260: Completed. Total Reward: 95.0\n",
      "Episode 261: Completed. Total Reward: -380.0\n",
      "Episode 262: Completed. Total Reward: -390.0\n",
      "Episode 263: Completed. Total Reward: 95.0\n",
      "Episode 264: Completed. Total Reward: -395.0\n",
      "Episode 265: Completed. Total Reward: 95.0\n",
      "Episode 266: Completed. Total Reward: 12.0\n",
      "Episode 267: Completed. Total Reward: 87.0\n",
      "Episode 268: Completed. Total Reward: 33.0\n",
      "Episode 269: Completed. Total Reward: 95.0\n",
      "Episode 270: Completed. Total Reward: 95.0\n",
      "Episode 271: Completed. Total Reward: 32.0\n",
      "Episode 272: Completed. Total Reward: 95.0\n",
      "Episode 273: Completed. Total Reward: -29.0\n",
      "Episode 274: Completed. Total Reward: -390.0\n",
      "Episode 275: Completed. Total Reward: 95.0\n",
      "Episode 276: Completed. Total Reward: -395.0\n",
      "Episode 277: Completed. Total Reward: -390.0\n",
      "Episode 278: Completed. Total Reward: -380.0\n",
      "Episode 279: Completed. Total Reward: 95.0\n",
      "Episode 280: Completed. Total Reward: -395.0\n",
      "Episode 281: Completed. Total Reward: 29.0\n",
      "Episode 282: Completed. Total Reward: 95.0\n",
      "Episode 283: Completed. Total Reward: -385.0\n",
      "Episode 284: Completed. Total Reward: -400.0\n",
      "Episode 285: Completed. Total Reward: -385.0\n",
      "Episode 286: Completed. Total Reward: -385.0\n",
      "Episode 287: Completed. Total Reward: -385.0\n",
      "Episode 288: Completed. Total Reward: 95.0\n",
      "Episode 289: Completed. Total Reward: -380.0\n",
      "Episode 290: Completed. Total Reward: -385.0\n",
      "Episode 291: Completed. Total Reward: -390.0\n",
      "Episode 292: Completed. Total Reward: 33.0\n",
      "Episode 293: Completed. Total Reward: -385.0\n",
      "Episode 294: Completed. Total Reward: -390.0\n",
      "Episode 295: Completed. Total Reward: -395.0\n",
      "Episode 296: Completed. Total Reward: -390.0\n",
      "Episode 297: Completed. Total Reward: 95.0\n"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m ActorCriticNetwork(in_channels, n_actions)\n\u001b[0;32m     32\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env, model, num_episodes, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m policy_dist, value \u001b[38;5;241m=\u001b[39m model(state)\n\u001b[0;32m     14\u001b[0m action \u001b[38;5;241m=\u001b[39m policy_dist\u001b[38;5;241m.\u001b[39mmultinomial(num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m---> 15\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(next_state)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     18\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m, in \u001b[0;36mVizDoomGym.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Specify action and take step\u001b[39;00m\n\u001b[0;32m     37\u001b[0m actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;241m3\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m---> 38\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get action using index -> left, right, shoot\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Get all the other stuff we need to return \u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_state():  \u001b[38;5;66;03m# if nothing is\u001b[39;00m\n",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "def train(env, model, num_episodes, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "    model.to(device)\n",
    "    gamma = 0.99\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            policy_dist, value = model(state)\n",
    "            action = policy_dist.multinomial(num_samples=1).detach()\n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "            next_state = torch.from_numpy(next_state).float().unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            # Calculate and update loss here\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        print(f\"Episode {episode + 1}: Completed. Total Reward: {total_reward}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = VizDoomGym(render=True)\n",
    "    in_channels = 1  # Assuming grayscale input\n",
    "    n_actions = env.action_space.n\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ActorCriticNetwork(in_channels, n_actions)\n",
    "    num_episodes = 500\n",
    "    train(env, model, num_episodes, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Doom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
