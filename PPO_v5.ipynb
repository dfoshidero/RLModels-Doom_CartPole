{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0547f-535d-49a1-a0cb-2df24c584a95",
   "metadata": {},
   "source": [
    "## Initialize VizDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cddcc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vizdoom in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from vizdoom) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from vizdoom) (0.29.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from vizdoom) (2.5.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==1.10.1+cu113 (from versions: 1.11.0, 1.11.0+cu113, 1.12.0, 1.12.0+cu113, 1.12.1, 1.12.1+cu113, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0)\n",
      "ERROR: No matching distribution found for torch==1.10.1+cu113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: pyglet==1.5.11 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (1.5.11)\n"
     ]
    }
   ],
   "source": [
    "#necessary\n",
    "!pip install vizdoom\n",
    "!pip install opencv-python\n",
    "!pip install pandas\n",
    "!pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip install gym\n",
    "!pip install pyglet==1.5.11\n",
    "\n",
    "# also need to install pytorch-cpu on anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34143102-2b66-4ee2-9f76-f6db917d2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f8b51-d30c-4c9f-a8b1-8e24b891a576",
   "metadata": {},
   "source": [
    "## Make it a Gym Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df5aed06-301c-43ff-a0ab-54dca32e78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2\n",
    "\n",
    "LEVEL = 'defend_the_center'\n",
    "DOOM_SKILL = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47d02cc2-14b1-4eb2-a032-add0d7ed26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('VizDoom/scenarios/defend_the_center.cfg')\n",
    "        \n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 4) # get action using index -> left, right, shoot\n",
    "        \n",
    "        # Get all the other stuff we need to return \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            ammo = self.game.get_state().game_variables[0] \n",
    "            info = ammo\n",
    "        # If we dont have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "            \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer  # Apply Grayscale\n",
    "\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48eacd6-a69d-4d14-b890-83f76c4a5e67",
   "metadata": {},
   "source": [
    "## Custom PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7728b4d-a680-4896-b0ea-a68a6683d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39228483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Algorithm\n",
    "\n",
    "\"\"\"\n",
    "https://www.youtube.com/watch?v=hlv79rcHws0\n",
    "\n",
    "https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/PolicyGradient/PPO/torch/main.py\n",
    "\"\"\"\n",
    "\n",
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, input_dims, alpha, fc1_dims=64, fc2_dims=64, checkpoint_dir='tmp/ppo'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, 'actor_torch_ppo')\n",
    "        os.makedirs(os.path.dirname(self.checkpoint_file), exist_ok=True)\n",
    "        \n",
    "        total_input_size = int(T.prod(T.tensor(input_dims)))  # Flatten the input dimensions\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(total_input_size, fc1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dims, n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        if state.dim() > 1:\n",
    "            state = state.view(state.size(0), -1)  # Flatten the state\n",
    "        \n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "\n",
    "        return dist \n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, alpha, fc1_dims=64, fc2_dims=64, checkpoint_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, 'critic_torch_ppo')\n",
    "        os.makedirs(os.path.dirname(self.checkpoint_file), exist_ok=True)\n",
    "        \n",
    "        total_input_size = int(T.prod(T.tensor(input_dims)))  # Flatten the input dimensions\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(total_input_size, fc1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dims, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        if state.dim() > 1:\n",
    "            state = state.view(state.size(0), -1)  # Flatten the state\n",
    "        \n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, n_actions, input_dims, gamma, alpha, gae_lambda,\n",
    "                 policy_clip, batch_size, N, n_epochs, entropy_coefficient):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_coefficient = entropy_coefficient\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
    "        self.critic = CriticNetwork(input_dims, alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []\n",
    "\n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('...saving models...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        print('...loading models...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "\n",
    "        dist = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def learn(self):\n",
    "        episode_actor_losses = []\n",
    "        episode_critic_losses = []\n",
    "        episode_values = []\n",
    "        \n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_probs_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "            \n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount * (reward_arr[k] + self.gamma * values[k+1] * (1 - int(dones_arr[k])) - values[k])\n",
    "                advantage[t] = a_t\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(old_probs_arr[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip, 1+self.policy_clip) * advantage[batch]\n",
    "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                # Calculate entropy bonus\n",
    "                entropy = dist.entropy().mean()\n",
    "                actor_loss -= self.entropy_coefficient * entropy  # Adding entropy bonus\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns - critic_value) ** 2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5 * critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "            # Collect losses for each batch\n",
    "            episode_actor_losses.append(actor_loss.item())\n",
    "            episode_critic_losses.append(critic_loss.item())\n",
    "            episode_values.append(critic_value.mean().item())\n",
    "\n",
    "        # Store average loss and value for the episode\n",
    "        self.actor_losses.append(np.mean(episode_actor_losses))\n",
    "        self.critic_losses.append(np.mean(episode_critic_losses))\n",
    "        self.values.append(np.mean(episode_values))\n",
    "\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "    # Reset stored data after each episode or training session\n",
    "    def reset_learning_debug_data(self):\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []\n",
    "\n",
    "    def reset_learning_debug_data(self):\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e7b4f",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d518ad7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 68\u001b[0m     \u001b[43mhyperparameter_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 52\u001b[0m, in \u001b[0;36mhyperparameter_tuning\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Use ProcessPoolExecutor to run tasks in parallel\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mProcessPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Submit tasks\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     future_to_params \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(evaluate_hyperparameters, VizDoomGym(render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), params, \u001b[38;5;241m3\u001b[39m): params \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m tasks}\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(future_to_params):\n\u001b[0;32m     55\u001b[0m         params \u001b[38;5;241m=\u001b[39m future_to_params[future]\n",
      "Cell \u001b[1;32mIn[23], line 52\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Use ProcessPoolExecutor to run tasks in parallel\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mProcessPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Submit tasks\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     future_to_params \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(evaluate_hyperparameters, \u001b[43mVizDoomGym\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m, params, \u001b[38;5;241m3\u001b[39m): params \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m tasks}\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(future_to_params):\n\u001b[0;32m     55\u001b[0m         params \u001b[38;5;241m=\u001b[39m future_to_params[future]\n",
      "Cell \u001b[1;32mIn[20], line 23\u001b[0m, in \u001b[0;36mVizDoomGym.__init__\u001b[1;34m(self, render)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mset_window_visible(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Start the game\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Create action space and observation space\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;241m=\u001b[39m Box(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m, shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m160\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_hyperparameters(env, agent_params, n_games=3):\n",
    "    agent = PPOAgent(**agent_params)\n",
    "    total_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _, _ = agent.choose_action(observation)\n",
    "            observation, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    return avg_reward\n",
    "\n",
    "def hyperparameter_tuning():\n",
    "    #env = gym.make('CartPole-v1')\n",
    "    env = VizDoomGym(render=False)\n",
    "    learning_rates = [0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "    gammas = [0.99, 0.95, 0.90]\n",
    "    policy_clips = [0.2, 0.4, 0.6, 0.8]\n",
    "    N_values = [5, 10, 15, 20]\n",
    "    entropy_coefficients = [0.01, 0.05, 0.001, 0.005]\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for entropy_coeff in entropy_coefficients:\n",
    "        for alpha in learning_rates:\n",
    "            for gamma in gammas:\n",
    "                for policy_clip in policy_clips:\n",
    "                    for N in N_values:\n",
    "                        agent_params = {\n",
    "                            'n_actions': env.action_space.n,\n",
    "                            'input_dims': env.observation_space.shape,\n",
    "                            'alpha': alpha,\n",
    "                            'gamma': gamma,\n",
    "                            'gae_lambda': 0.95,\n",
    "                            'policy_clip': policy_clip,\n",
    "                            'batch_size': 5,\n",
    "                            'N': N,\n",
    "                            'n_epochs': 4,\n",
    "                            'entropy_coefficient': entropy_coeff\n",
    "                        }\n",
    "                        avg_reward = evaluate_hyperparameters(env, agent_params)\n",
    "                        results.append((avg_reward, agent_params))\n",
    "                        print(f'Tested {agent_params} -> Avg Reward: {avg_reward}')\n",
    "    \n",
    "    # Normalize rewards to sum to 1 to use as weights\n",
    "    total_reward = sum([result[0] for result in results])\n",
    "    weights = [result[0] / total_reward for result in results]\n",
    "\n",
    "    # Weighted average of parameters\n",
    "    avg_params = {}\n",
    "    for key in results[0][1].keys():\n",
    "        param_values = [params[key] for _, params in results]\n",
    "        if isinstance(param_values[0], float):  # Check if the parameter is float\n",
    "            avg_params[key] = sum(weight * params[key] for weight, (_, params) in zip(weights, results))\n",
    "        elif isinstance(param_values[0], int):  # Check if the parameter is integer\n",
    "            # Use weighted average and round it to get an integer\n",
    "            weighted_sum = sum(weight * params[key] for weight, (_, params) in zip(weights, results))\n",
    "            avg_params[key] = round(weighted_sum)\n",
    "        else:\n",
    "            # For non-numeric parameters, take the value from the best-performing configuration\n",
    "            avg_params[key] = results[0][1][key]  # Assumes results is sorted by performance, best first\n",
    "\n",
    "    print(f\"Weighted Average of Best Hyperparameters: {avg_params}\")\n",
    "    return avg_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7664881-22e2-42a6-8ab9-693188f1d2c3",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566fc91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"c:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\multiprocess\\pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"c:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\pathos\\helpers\\mp_helper.py\", line 15, in <lambda>\n    func = lambda args: f(*args)\n  File \"C:\\Users\\Favour-Daniel\\AppData\\Local\\Temp\\ipykernel_35320\\1098747005.py\", line 6, in evaluate_hyperparameters\nNameError: name 'Env' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m#env = gym.make('CartPole-v1')\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     env \u001b[38;5;241m=\u001b[39m VizDoomGym(render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 74\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparameter_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# Create the agent with the best hyperparameters\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     agent \u001b[38;5;241m=\u001b[39m PPOAgent(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params)\n",
      "Cell \u001b[1;32mIn[15], line 152\u001b[0m, in \u001b[0;36mhyperparameter_tuning\u001b[1;34m()\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Use pathos multiprocessing pool\u001b[39;00m\n\u001b[0;32m    151\u001b[0m pool \u001b[38;5;241m=\u001b[39m Pool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# Set the number of processes you want to use\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_hyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    154\u001b[0m pool\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\pathos\\multiprocessing.py:154\u001b[0m, in \u001b[0;36mProcessPool.map\u001b[1;34m(self, f, *args, **kwds)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OLD312a7:\n\u001b[0;32m    153\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _pool\u001b[38;5;241m.\u001b[39mmap(star(f), \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39margs), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\multiprocess\\pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom_\\lib\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Env' is not defined"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def plot_learning_curves(timesteps, scores, figure_file):\n",
    "    os.makedirs(os.path.dirname(figure_file), exist_ok=True)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    cumulative_avg = np.cumsum(scores) / (np.arange(len(scores)) + 1)  # Compute the cumulative average\n",
    "\n",
    "    ax.plot(timesteps, scores, label='Reward per Episode', alpha=0.3)  # Plot raw scores\n",
    "    ax.plot(timesteps, cumulative_avg, label='Cumulative Average', color='red')  # Plot cumulative average\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.legend()\n",
    "    plt.title('Training Curve')\n",
    "    plt.savefig(figure_file)\n",
    "    plt.show()\n",
    "\n",
    "def plot_curve_smooth(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)\n",
    "\n",
    "def smooth_curve(data, window=100):\n",
    "    \"\"\"Calculate the running average over a fixed window.\"\"\"\n",
    "    running_avg = np.zeros(len(data))\n",
    "    for i in range(len(data)):\n",
    "        running_avg[i] = np.mean(data[max(0, i-window):(i+1)])\n",
    "    return running_avg\n",
    "\n",
    "def plot_additional_metrics(episodes, actor_losses, critic_losses, values, figure_file_prefix):\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(7, 14))\n",
    "\n",
    "    # Ensure 'episodes' is a list or array of the right size\n",
    "    episodes = list(episodes) if len(episodes) == len(actor_losses) else list(range(len(actor_losses)))\n",
    "\n",
    "    # Smoothed data\n",
    "    smoothed_actor_losses = smooth_curve(actor_losses)\n",
    "    smoothed_critic_losses = smooth_curve(critic_losses)\n",
    "    smoothed_values = smooth_curve(values)\n",
    "\n",
    "    # Actor Loss\n",
    "    axs[0].plot(episodes, smoothed_actor_losses, label='Actor Loss', color='blue')\n",
    "    axs[0].set_title('Actor Loss Over Time (Smoothed)')\n",
    "    axs[0].set_xlabel('Episode')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Critic Loss\n",
    "    axs[1].plot(episodes, smoothed_critic_losses, label='Critic Loss', color='green')\n",
    "    axs[1].set_title('Critic Loss Over Time (Smoothed)')\n",
    "    axs[1].set_xlabel('Episode')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Value Estimates\n",
    "    axs[2].plot(episodes, smoothed_values, label='Value Estimates', color='red')\n",
    "    axs[2].set_title('Value Estimates Over Time (Smoothed)')\n",
    "    axs[2].set_xlabel('Episode')\n",
    "    axs[2].set_ylabel('Value')\n",
    "    axs[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{figure_file_prefix}_additional_metrics.png\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #env = gym.make('CartPole-v1')\n",
    "    env = VizDoomGym(render=True)\n",
    "    best_params = hyperparameter_tuning()\n",
    "\n",
    "    # Create the agent with the best hyperparameters\n",
    "    agent = PPOAgent(**best_params)\n",
    "    n_games = 300\n",
    "\n",
    "    # Directory for saving plots and model checkpoints\n",
    "    plot_dir = './logs/ppo/plots'\n",
    "    checkpoint_dir = './logs/ppo/checkpoints'\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    figure_file = os.path.join(plot_dir, 'test.png')\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    N = best_params.get('N', 20)\n",
    "\n",
    "    for i in range(n_games):\n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            observation_, reward, done, info, _ = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "            if n_steps % N == 0:\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_models()\n",
    "\n",
    "        print(\"Episode \", i, \": score %.1f\" % score, \" avg score %.1f\" % avg_score,\\\n",
    "                \" timesteps\", n_steps, \" learning steps\", learn_iters)\n",
    "    x = [i+1 for i in range(len(score_history))]\n",
    "    plot_curve_smooth(x, score_history, figure_file)\n",
    "    episodes = range(1, n_games + 1)\n",
    "    plot_additional_metrics(episodes, agent.actor_losses, agent.critic_losses, agent.values, plot_dir)\n",
    "\n",
    "    #report average values per however many runs\n",
    "\n",
    "    # changed to LeakyReLU for cases of negative input\n",
    "    # added entropy bonus to avoid agent converging too early, rewards more exploration - saw much better initial results\n",
    "    # added hp tuning to ensure best params - as model seemed quite sensitive\n",
    "    # updated hp tuning to use weighted average of model vals\n",
    "    # implemented parallelization to run hyperparams in parallel\n",
    "    # add some information about CATASTROPHE FORGETTING in report. PPO is very susceptible and converges on bad policies often\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
