{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doom Game Using PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize VizDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make it a Gym Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('VizDoom/scenarios/basic.cfg')\n",
    "        self.game.set_window_visible(render)\n",
    "        self.game.init()\n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 4) # get action using index -> left, right, shoot\n",
    "        \n",
    "        # Get all the other stuff we need to return \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            ammo = self.game.get_state().game_variables[0] \n",
    "            info = ammo\n",
    "        # If we dont have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        # Set seed if provided\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "    \n",
    "        self.game.new_episode()  # Start a new episode regardless of the seed\n",
    "\n",
    "        # After starting a new episode, get the initial state\n",
    "        if self.game.get_state() is not None:\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            return self.grayscale(state)\n",
    "        else:\n",
    "            # Return a zero array if no state is available (e.g., at episode start)\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.uint8)\n",
    "\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Pytorch PPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Actor-Critic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size = 3, stride = 2, padding = 0):\n",
    "    return (size + 2 * padding - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "# Calculate output size after each convolutional layer\n",
    "h_w = [100, 160]  # initial height and width\n",
    "h_w = [conv2d_size_out(x, 8, 4) for x in h_w]  # after first conv layer\n",
    "h_w = [conv2d_size_out(x, 4, 2) for x in h_w]  # after second conv layer\n",
    "h_w = [conv2d_size_out(x, 3, 1) for x in h_w]  # after third conv layer\n",
    "\n",
    "# Calculate the total number of features before the linear layer\n",
    "feature_count = 64 * h_w[0] * h_w[1]\n",
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, n_output):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Temporarily assume some output size after convolution\n",
    "        # This should ideally be calculated based on input size\n",
    "        self.feature_count = 64 * conv2d_size_out(conv2d_size_out(conv2d_size_out(100, 8, 4), 4, 2), 3, 1) * \\\n",
    "                        conv2d_size_out(conv2d_size_out(conv2d_size_out(160, 8, 4), 4, 2), 3, 1)\n",
    "\n",
    "        self.fc = nn.Linear(self.feature_count, 512)\n",
    "        self.actor = nn.Linear(512, n_output)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(-1, self.feature_count)  # Use reshape instead of view\n",
    "        x = F.relu(self.fc(x))\n",
    "        action_probs = F.softmax(self.actor(x), dim=1)\n",
    "        value = self.critic(x)\n",
    "        return action_probs, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Calculate the returns using the rewards and the next state's value estimate.\n",
    "    \"\"\"\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def ppo_update(policy_net, optimizer, states, actions, log_probs_old, returns, advantages, clip_param=0.2):\n",
    "    \"\"\"\n",
    "    Perform one update step of the PPO algorithm.\n",
    "    \"\"\"\n",
    "    for _ in range(10):  # PPO epochs\n",
    "        # Get the log probabilities and state values from the model\n",
    "        log_probs, state_values = policy_net(states)\n",
    "        # Select the log probabilities for the actions taken\n",
    "        log_probs = log_probs.gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        # Calculate entropy to encourage exploration\n",
    "        entropy = -(log_probs * torch.exp(log_probs)).sum(1, keepdim=True).mean()\n",
    "\n",
    "        # Calculate the ratio (pi_theta / pi_theta_old) for the actions taken\n",
    "        ratios = torch.exp(log_probs - log_probs_old)\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1.0 - clip_param, 1.0 + clip_param) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = F.smooth_l1_loss(state_values.squeeze(-1), returns)\n",
    "        loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv2d_size_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m n_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[0;32m     30\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mActorCriticNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m     33\u001b[0m train(env, model, num_episodes, device)\n",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m, in \u001b[0;36mActorCriticNetwork.__init__\u001b[1;34m(self, in_channels, n_output)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Temporarily assume some output size after convolution\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# This should ideally be calculated based on input size\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mconv2d_size_out\u001b[49m(conv2d_size_out(conv2d_size_out(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m4\u001b[39m), \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \\\n\u001b[0;32m     11\u001b[0m                 conv2d_size_out(conv2d_size_out(conv2d_size_out(\u001b[38;5;241m160\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m4\u001b[39m), \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count, \u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m512\u001b[39m, n_output)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conv2d_size_out' is not defined"
     ]
    }
   ],
   "source": [
    "def train(env, model, num_episodes, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "    model.to(device)\n",
    "    gamma = 0.99\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            policy_dist, value = model(state)\n",
    "            action = policy_dist.multinomial(num_samples=1).detach()\n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "            next_state = torch.from_numpy(next_state).float().unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            # Calculate and update loss here\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        print(f\"Episode {episode + 1}: Completed. Total Reward: {total_reward}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = VizDoomGym(render=True)\n",
    "    in_channels = 1  # Assuming grayscale input\n",
    "    n_actions = env.action_space.n\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ActorCriticNetwork(in_channels, n_actions)\n",
    "    num_episodes = 500\n",
    "    train(env, model, num_episodes, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Doom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
