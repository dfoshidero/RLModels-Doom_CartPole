{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0547f-535d-49a1-a0cb-2df24c584a95",
   "metadata": {},
   "source": [
    "## Initialize VizDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6cddcc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vizdoom in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from vizdoom) (1.26.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from vizdoom) (0.29.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from vizdoom) (2.5.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==1.10.1+cu113 (from versions: 1.11.0, 1.11.0+cu113, 1.12.0, 1.12.0+cu113, 1.12.1, 1.12.1+cu113, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0)\n",
      "ERROR: No matching distribution found for torch==1.10.1+cu113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: pyglet==1.5.11 in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (1.5.11)\n",
      "Requirement already satisfied: joblib in c:\\users\\favour-daniel\\anaconda3\\envs\\doom_\\lib\\site-packages (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "#necessary\n",
    "!pip install vizdoom\n",
    "!pip install opencv-python\n",
    "!pip install pandas\n",
    "!pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip install gym\n",
    "!pip install pyglet==1.5.11\n",
    "!pip install joblib\n",
    "\n",
    "# also need to install pytorch-cpu on anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "34143102-2b66-4ee2-9f76-f6db917d2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2\n",
    "\n",
    "# Extra imports\n",
    "import os\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f8b51-d30c-4c9f-a8b1-8e24b891a576",
   "metadata": {},
   "source": [
    "## VizDoom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47d02cc2-14b1-4eb2-a032-add0d7ed26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('VizDoom/scenarios/defend_the_center.cfg')\n",
    "        \n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "    \n",
    "    def step(self, action, frame_skip=4):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        total_reward = 0\n",
    "        for _ in range(frame_skip):\n",
    "            reward = self.game.make_action(actions[action], 2)  # Increase frame skip value here\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Break the loop if the game ends during frame skipping\n",
    "            if self.game.is_episode_finished():\n",
    "                break\n",
    "        \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            ammo = self.game.get_state().game_variables[0] \n",
    "            info = ammo\n",
    "        # If we don't have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\": info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        print(\"Step state shape:\", state.shape)\n",
    "        return state, total_reward, done, truncated, info\n",
    "\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "            \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer  # Apply Grayscale\n",
    "\n",
    "        print(\"Reset state shape:\", state.shape)\n",
    "        return self.grayscale(state)\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48eacd6-a69d-4d14-b890-83f76c4a5e67",
   "metadata": {},
   "source": [
    "## Custom PPO model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7dbd3",
   "metadata": {},
   "source": [
    "### PPO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "39228483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Algorithm\n",
    "\n",
    "\"\"\"\n",
    "https://www.youtube.com/watch?v=hlv79rcHws0\n",
    "\n",
    "https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/PolicyGradient/PPO/torch/main.py\n",
    "\"\"\"\n",
    "\n",
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, input_dims, alpha, fc1_dims=64, fc2_dims=64, checkpoint_dir='tmp/ppo', noise_std=0.5):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, 'actor_torch_ppo')\n",
    "        os.makedirs(os.path.dirname(self.checkpoint_file), exist_ok=True)\n",
    "        \n",
    "        self.noise_std = noise_std  # Standard deviation of the Gaussian noise\n",
    "        total_input_size = int(T.prod(T.tensor(input_dims)))  # Flatten the input dimensions\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(total_input_size, fc1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dims, n_actions),\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        if state.dim() > 1:\n",
    "            state = state.view(state.size(0), -1)  # Flatten the state\n",
    "        \n",
    "        logits = self.actor(state)\n",
    "        if self.training:  # Only add noise during training\n",
    "            noise = T.randn_like(logits) * self.noise_std\n",
    "            logits = logits + noise\n",
    "        dist = Categorical(logits=logits.softmax(dim=-1))\n",
    "\n",
    "        return dist \n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, alpha, fc1_dims=64, fc2_dims=64, checkpoint_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, 'critic_torch_ppo')\n",
    "        os.makedirs(os.path.dirname(self.checkpoint_file), exist_ok=True)\n",
    "        \n",
    "        total_input_size = int(T.prod(T.tensor(input_dims)))  # Flatten the input dimensions\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(total_input_size, fc1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dims, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        if state.dim() > 1:\n",
    "            state = state.view(state.size(0), -1)  # Flatten the state\n",
    "        \n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "    \n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, n_actions, input_dims, gamma, alpha, gae_lambda,\n",
    "                 policy_clip, batch_size, N, n_epochs, entropy_coefficient, save_dir, actor, critic):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.N = N\n",
    "        self.n_epochs - n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_coefficient = entropy_coefficient\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
    "        self.critic = CriticNetwork(input_dims, alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []\n",
    "\n",
    "    def print_params(self):\n",
    "        # Print statement accessing the attributes with descriptive names\n",
    "        print(f\"Agent Parameters:\\n\"\n",
    "              f\"  Discount Factor (Gamma): {self.gamma}\\n\"\n",
    "              f\"  Learning Rate (Alpha): {self.alpha}\\n\"\n",
    "              f\"  GAE Lambda: {self.gae_lambda}\\n\"\n",
    "              f\"  Policy Clipping Range: {self.policy_clip}\\n\"\n",
    "              f\"  Batch Size: {self.batch_size}\\n\"\n",
    "              f\"  Steps per Batch (N): {self.N}\\n\"\n",
    "              f\"  Number of Epochs per Update: {self.n_epochs}\\n\"\n",
    "              f\"  Entropy Coefficient: {self.entropy_coefficient}\")\n",
    "\n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('...saving models...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        T.save(self.actor.state_dict(), os.path.join(self.save_dir, 'actor.pt'))\n",
    "        T.save(self.critic.state_dict(), os.path.join(self.save_dir, 'critic.pt'))\n",
    "        print(\"Models saved!\")\n",
    "\n",
    "    def load_models(self, actor_path, critic_path):\n",
    "        self.actor.load_state_dict(T.load(actor_path))\n",
    "        self.critic.load_state_dict(T.load(critic_path))\n",
    "        self.actor.eval()\n",
    "        self.critic.eval()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        print(\"Observation shape in choose_action:\", observation.shape)\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        \n",
    "        dist = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        episode_actor_losses = []\n",
    "        episode_critic_losses = []\n",
    "        episode_values = []\n",
    "        \n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_probs_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "            \n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount * (reward_arr[k] + self.gamma * values[k+1] * (1 - int(dones_arr[k])) - values[k])\n",
    "                advantage[t] = a_t\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "\n",
    "            # sources explaining why we keep track of raw action probabilities:\n",
    "            # https://cs.stackexchange.com/questions/70518/why-do-we-use-the-log-in-gradient-based-reinforcement-algorithms\n",
    "            # https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient\n",
    "            # essentially, makes gradient ascient easier\n",
    "       \n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(old_probs_arr[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip, 1+self.policy_clip) * advantage[batch]\n",
    "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                # Calculate entropy bonus\n",
    "                entropy = dist.entropy().mean()\n",
    "                actor_loss -= self.entropy_coefficient * entropy  # Adding entropy bonus\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns - critic_value) ** 2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5 * critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "            # Collect losses for each batch\n",
    "            episode_actor_losses.append(actor_loss.item())\n",
    "            episode_critic_losses.append(critic_loss.item())\n",
    "            episode_values.append(critic_value.mean().item())\n",
    "\n",
    "        # Store average loss and value for the episode\n",
    "        self.actor_losses.append(np.mean(episode_actor_losses))\n",
    "        self.critic_losses.append(np.mean(episode_critic_losses))\n",
    "        self.values.append(np.mean(episode_values))\n",
    "\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "    # Reset stored data after each episode or training session\n",
    "    def reset_learning_debug_data(self):\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []\n",
    "\n",
    "    def reset_learning_debug_data(self):\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7664881-22e2-42a6-8ab9-693188f1d2c3",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1566fc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset state shape: (3, 240, 320)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Total reward: 1.0\n",
      "Reset state shape: (3, 240, 320)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Total reward: -1.0\n",
      "Reset state shape: (3, 240, 320)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Total reward: 0.0\n",
      "Reset state shape: (3, 240, 320)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Total reward: 0.0\n",
      "Reset state shape: (3, 240, 320)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Total reward: -1.0\n",
      "Reset state shape: (3, 240, 320)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Total reward: 2.0\n",
      "Reset state shape: (3, 240, 320)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Total reward: -1.0\n",
      "Reset state shape: (3, 240, 320)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Total reward: 2.0\n",
      "Reset state shape: (3, 240, 320)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Total reward: 0.0\n",
      "Reset state shape: (3, 240, 320)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Observation shape in choose_action: (100, 160, 1)\n",
      "Step state shape: (100, 160, 1)\n",
      "Total reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create environment\n",
    "    env = VizDoomGym(render=True)\n",
    "\n",
    "    best_params = {\n",
    "                        'n_actions': env.action_space.n,\n",
    "                        'input_dims': env.observation_space.shape,\n",
    "                        'alpha': 0.0003,\n",
    "                        'gamma': 0.99,\n",
    "                        'gae_lambda': 0.92,\n",
    "                        'policy_clip': 0.2,\n",
    "                        'batch_size': 32,\n",
    "                        'N': 16,\n",
    "                        'n_epochs': 15,\n",
    "                        'entropy_coefficient': 0.01,\n",
    "                        'save_dir': \"./output\",\n",
    "                        'actor': actor,\n",
    "                        'critic': critic,\n",
    "                    }\n",
    "    \n",
    "    # Load the models\n",
    "    actor = ActorNetwork(n_actions=3, input_dims=[100, 160, 1], alpha=0.0003)\n",
    "    critic = CriticNetwork(input_dims=[100, 160, 1], alpha=0.0003)\n",
    "    agent = PPOAgent(**best_params)\n",
    "    agent.load_models('./models/actor.pt', './models/critic.pt')\n",
    "\n",
    "    episodes = 10\n",
    "    for _ in range(episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation, reward, done, info, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Total reward: {total_reward}\")\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
