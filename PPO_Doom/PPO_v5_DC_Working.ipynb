{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0547f-535d-49a1-a0cb-2df24c584a95",
   "metadata": {},
   "source": [
    "## Initialize VizDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cddcc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary\n",
    "!pip install vizdoom\n",
    "!pip install opencv-python\n",
    "!pip install pandas\n",
    "!pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip install gym\n",
    "!pip install pyglet==1.5.11\n",
    "!pip install joblib\n",
    "\n",
    "# also need to install pytorch-cpu on anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34143102-2b66-4ee2-9f76-f6db917d2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f8b51-d30c-4c9f-a8b1-8e24b891a576",
   "metadata": {},
   "source": [
    "## Make it a Gym Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5aed06-301c-43ff-a0ab-54dca32e78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2\n",
    "\n",
    "LEVEL = 'defend_the_center'\n",
    "DOOM_SKILL = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d02cc2-14b1-4eb2-a032-add0d7ed26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('VizDoom/scenarios/defend_the_center.cfg')\n",
    "        \n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "    \n",
    "    def step(self, action, frame_skip=4):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        total_reward = 0\n",
    "        for _ in range(frame_skip):\n",
    "            reward = self.game.make_action(actions[action], 2)  # Increase frame skip value here\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Break the loop if the game ends during frame skipping\n",
    "            if self.game.is_episode_finished():\n",
    "                break\n",
    "        \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            ammo = self.game.get_state().game_variables[0] \n",
    "            info = ammo\n",
    "        # If we don't have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\": info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, total_reward, done, truncated, info\n",
    "\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "            \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer  # Apply Grayscale\n",
    "\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48eacd6-a69d-4d14-b890-83f76c4a5e67",
   "metadata": {},
   "source": [
    "## Custom PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7728b4d-a680-4896-b0ea-a68a6683d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39228483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Algorithm\n",
    "\n",
    "\"\"\"\n",
    "https://www.youtube.com/watch?v=hlv79rcHws0\n",
    "\n",
    "https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/PolicyGradient/PPO/torch/main.py\n",
    "\"\"\"\n",
    "\n",
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, input_dims, alpha, fc1_dims=64, fc2_dims=64, checkpoint_dir='tmp/ppo', noise_std=0.5):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, 'actor_torch_ppo')\n",
    "        os.makedirs(os.path.dirname(self.checkpoint_file), exist_ok=True)\n",
    "        \n",
    "        self.noise_std = noise_std  # Standard deviation of the Gaussian noise\n",
    "        total_input_size = int(T.prod(T.tensor(input_dims)))  # Flatten the input dimensions\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(total_input_size, fc1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dims, n_actions),\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        if state.dim() > 1:\n",
    "            state = state.view(state.size(0), -1)  # Flatten the state\n",
    "        \n",
    "        logits = self.actor(state)\n",
    "        if self.training:  # Only add noise during training\n",
    "            noise = T.randn_like(logits) * self.noise_std\n",
    "            logits = logits + noise\n",
    "        dist = Categorical(logits=logits.softmax(dim=-1))\n",
    "\n",
    "        return dist \n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, alpha, fc1_dims=64, fc2_dims=64, checkpoint_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, 'critic_torch_ppo')\n",
    "        os.makedirs(os.path.dirname(self.checkpoint_file), exist_ok=True)\n",
    "        \n",
    "        total_input_size = int(T.prod(T.tensor(input_dims)))  # Flatten the input dimensions\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(total_input_size, fc1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dims, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        if state.dim() > 1:\n",
    "            state = state.view(state.size(0), -1)  # Flatten the state\n",
    "        \n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, n_actions, input_dims, gamma, alpha, gae_lambda,\n",
    "                 policy_clip, batch_size, N, n_epochs, entropy_coefficient, save_dir):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.N = N\n",
    "        self.n_epochs - n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_coefficient = entropy_coefficient\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
    "        self.critic = CriticNetwork(input_dims, alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []\n",
    "\n",
    "    def print_params(self):\n",
    "        # Print statement accessing the attributes with descriptive names\n",
    "        print(f\"Agent Parameters:\\n\"\n",
    "              f\"  Discount Factor (Gamma): {self.gamma}\\n\"\n",
    "              f\"  Learning Rate (Alpha): {self.alpha}\\n\"\n",
    "              f\"  GAE Lambda: {self.gae_lambda}\\n\"\n",
    "              f\"  Policy Clipping Range: {self.policy_clip}\\n\"\n",
    "              f\"  Batch Size: {self.batch_size}\\n\"\n",
    "              f\"  Steps per Batch (N): {self.N}\\n\"\n",
    "              f\"  Number of Epochs per Update: {self.n_epochs}\\n\"\n",
    "              f\"  Entropy Coefficient: {self.entropy_coefficient}\")\n",
    "\n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('...saving models...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        T.save(self.actor.state_dict(), os.path.join(self.save_dir, 'actor.pt'))\n",
    "        T.save(self.critic.state_dict(), os.path.join(self.save_dir, 'critic.pt'))\n",
    "        print(\"Models saved!\")\n",
    "\n",
    "    def load_models(self):\n",
    "        print('...loading models...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "\n",
    "        dist = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def learn(self):\n",
    "        episode_actor_losses = []\n",
    "        episode_critic_losses = []\n",
    "        episode_values = []\n",
    "        \n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_probs_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "            \n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount * (reward_arr[k] + self.gamma * values[k+1] * (1 - int(dones_arr[k])) - values[k])\n",
    "                advantage[t] = a_t\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "\n",
    "            # sources explaining why we keep track of raw action probabilities:\n",
    "            # https://cs.stackexchange.com/questions/70518/why-do-we-use-the-log-in-gradient-based-reinforcement-algorithms\n",
    "            # https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient\n",
    "            # essentially, makes gradient ascient easier\n",
    "       \n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(old_probs_arr[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip, 1+self.policy_clip) * advantage[batch]\n",
    "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                # Calculate entropy bonus\n",
    "                entropy = dist.entropy().mean()\n",
    "                actor_loss -= self.entropy_coefficient * entropy  # Adding entropy bonus\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns - critic_value) ** 2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5 * critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "            # Collect losses for each batch\n",
    "            episode_actor_losses.append(actor_loss.item())\n",
    "            episode_critic_losses.append(critic_loss.item())\n",
    "            episode_values.append(critic_value.mean().item())\n",
    "\n",
    "        # Store average loss and value for the episode\n",
    "        self.actor_losses.append(np.mean(episode_actor_losses))\n",
    "        self.critic_losses.append(np.mean(episode_critic_losses))\n",
    "        self.values.append(np.mean(episode_values))\n",
    "\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "    # Reset stored data after each episode or training session\n",
    "    def reset_learning_debug_data(self):\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []\n",
    "\n",
    "    def reset_learning_debug_data(self):\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e7b4f",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d518ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hyperparameters(env, agent_params, n_games=10):\n",
    "    agent = PPOAgent(**agent_params)\n",
    "    total_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _, _ = agent.choose_action(observation)\n",
    "            observation, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    return avg_reward\n",
    "\n",
    "def hyperparameter_tuning():\n",
    "    #env = gym.make('CartPole-v1')\n",
    "    env = VizDoomGym(render=False)\n",
    "    learning_rates = [0.00001]\n",
    "    gammas = [0.90]\n",
    "    policy_clips = [0.2]\n",
    "    entropy_coefficients = [0.3]\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for entropy_coeff in entropy_coefficients:\n",
    "        for alpha in learning_rates:\n",
    "            for gamma in gammas:\n",
    "                for policy_clip in policy_clips:\n",
    "                    agent_params = {\n",
    "                        'n_actions': env.action_space.n,\n",
    "                        'input_dims': env.observation_space.shape,\n",
    "                        'alpha': alpha,\n",
    "                        'gamma': gamma,\n",
    "                        'gae_lambda': 0.92,\n",
    "                        'policy_clip': policy_clip,\n",
    "                        'batch_size': 32,\n",
    "                        'N': 16,\n",
    "                        'n_epochs': 15,\n",
    "                        'entropy_coefficient': entropy_coeff,\n",
    "                        'save_dir': \"E:\\RLModelTraining\\Doom_PPO_Final_2\"\n",
    "                    }\n",
    "                    avg_reward = evaluate_hyperparameters(env, agent_params)\n",
    "                    results.append((avg_reward, agent_params))\n",
    "                    print(f'Tested {agent_params} -> Avg Reward: {avg_reward}')\n",
    "    \n",
    "    # Normalize rewards to sum to 1 to use as weights\n",
    "    total_reward = sum([result[0] for result in results])\n",
    "    weights = [result[0] / total_reward for result in results]\n",
    "\n",
    "    # Weighted average of parameters\n",
    "    avg_params = {}\n",
    "    for key in results[0][1].keys():\n",
    "        param_values = [params[key] for _, params in results]\n",
    "        if isinstance(param_values[0], float):  # Check if the parameter is float\n",
    "            avg_params[key] = sum(weight * params[key] for weight, (_, params) in zip(weights, results))\n",
    "        elif isinstance(param_values[0], int):  # Check if the parameter is integer\n",
    "            # Use weighted average and round it to get an integer\n",
    "            weighted_sum = sum(weight * params[key] for weight, (_, params) in zip(weights, results))\n",
    "            avg_params[key] = round(weighted_sum)\n",
    "        else:\n",
    "            # For non-numeric parameters, take the value from the best-performing configuration\n",
    "            avg_params[key] = results[0][1][key]  # Assumes results is sorted by performance, best first\n",
    "\n",
    "    print(f\"Weighted Average of Best Hyperparameters: {avg_params}\")\n",
    "    return avg_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7664881-22e2-42a6-8ab9-693188f1d2c3",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "########################################################################################################################################################\n",
    "# PLOTTING FUNCS\n",
    "def plot_curve_smooth(x, scores, figure_file):\n",
    "    os.makedirs(os.path.dirname(figure_file), exist_ok=True)\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "\n",
    "    fig, ax = plt.subplots()  # Using subplots for consistency\n",
    "    ax.plot(x, running_avg, label='Running Average')\n",
    "    ax.set_xlabel('Episode')  # Align the x label\n",
    "    ax.set_ylabel('Reward')  # Align the y label\n",
    "    ax.legend()\n",
    "    plt.title('Running Average of Previous 100 Scores')\n",
    "    plt.savefig(figure_file)\n",
    "    plt.show()\n",
    "\n",
    "def smooth_curve(data, window=100):\n",
    "    \"\"\"Calculate the running average over a fixed window.\"\"\"\n",
    "    running_avg = np.zeros(len(data))\n",
    "    for i in range(len(data)):\n",
    "        running_avg[i] = np.mean(data[max(0, i-window):(i+1)])\n",
    "    return running_avg\n",
    "\n",
    "def plot_metrics(episodes, actor_losses, critic_losses, values, checkpoint_avg_lengths, checkpoint_avg_rewards, total_batches):\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "    # Ensure 'episodes' is a list or array of the right size\n",
    "    episodes = list(episodes) if len(episodes) == len(actor_losses) else list(range(len(actor_losses)))\n",
    "\n",
    "    # Plot Episode Length and Reward\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_ylabel('Average Episode Length', color=color)\n",
    "    ax1.set_xlabel('Batch')\n",
    "    ax1.set_title('PPO Agent Reward by Batch (=16 Episodes)')\n",
    "    ax1.plot(range(1, total_batches + 1), checkpoint_avg_lengths, color=color, label='Episode Length')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Average Reward per Episode', color=color)\n",
    "    ax2.plot(range(1, total_batches + 1), checkpoint_avg_rewards, color=color, label='Episode Reward')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Save the plot with batch name\n",
    "    plot_save_path = os.path.join(save_dir, f\"plot_batch {total_batches}_metrics.png\")\n",
    "    plt.savefig(plot_save_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "########################################################################################################################################################\n",
    "# MAIN\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Initialize environment\n",
    "    #env = gym.make('CartPole-v1')\n",
    "    env = VizDoomGym(render=True)\n",
    "    best_params = hyperparameter_tuning()\n",
    "\n",
    "    # Directory for saving plots and model checkpoints\n",
    "    save_dir=\"E:\\RLModelTraining\\Doom_PPO_Final_2\"\n",
    "\n",
    "    # Create the agent with the best hyperparameters\n",
    "    agent = PPOAgent(**best_params)\n",
    "    num_timesteps = 500000\n",
    "\n",
    "    # Agent loop params\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    N = best_params.get('N', 16)\n",
    "\n",
    "    total_batches = 0\n",
    "    episode_lengths = []\n",
    "    episode_rewards = []\n",
    "    \n",
    "    checkpoint_timesteps = []\n",
    "    checkpoint_avg_rewards = []\n",
    "    checkpoint_avg_lengths = []\n",
    "\n",
    "    # MAIN LOOP until total timesteps\n",
    "    while n_steps < num_timesteps:\n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        total_reward = 0\n",
    "        episode_length = 0\n",
    "        \n",
    "        # INNER LOOP until episode complete\n",
    "        while not done:\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            observation_, reward, done, info, _ = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "\n",
    "            total_reward += reward\n",
    "            episode_length += 1\n",
    "            \n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "\n",
    "            if n_steps % N == 0:\n",
    "\n",
    "                total_batches += 1\n",
    "\n",
    "                # Calculate running averages of episode length and reward\n",
    "                avg_episode_lengths = [np.mean(episode_lengths)]\n",
    "                avg_episode_rewards = [np.mean(episode_rewards)]\n",
    "\n",
    "                checkpoint_timesteps.append(n_steps)\n",
    "                checkpoint_avg_rewards.append(avg_episode_rewards[-1])\n",
    "                checkpoint_avg_lengths.append(avg_episode_lengths[-1])\n",
    "                \n",
    "                n_games = len(episode_rewards)\n",
    "                episodes = range(1, n_games + 1)\n",
    "\n",
    "                # Plotting\n",
    "                \n",
    "                print(f\"Plot saved at batch {total_batches} to {save_dir}\")\n",
    "                print(f\"Batch {total_batches}: Length = {episode_length}, Reward = {total_reward}\")\n",
    "                agent.print_params()\n",
    "                \n",
    "                clear_output(wait=True)\n",
    "                plot_metrics(episodes, agent.actor_losses, agent.critic_losses, agent.values, checkpoint_avg_lengths, checkpoint_avg_rewards, total_batches)\n",
    "                \n",
    "                # Agent Learns\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            \n",
    "            observation = observation_\n",
    "\n",
    "        episode_lengths.append(episode_length)\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        # SAVE MODEL (to checkpoint file each time agent scores better)    \n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        agent.save_models()\n",
    "            \n",
    "\n",
    "    #report average values per however many runs\n",
    "\n",
    "    # changed to LeakyReLU for cases of negative input\n",
    "    # added entropy bonus to avoid agent converging too early, rewards more exploration - saw much better initial results\n",
    "    # added hp tuning to ensure best params - as model seemed quite sensitive\n",
    "    # updated hp tuning to use weighted average of model vals\n",
    "    # implemented action space noise to force agent to make more random actions\n",
    "    # add some information about CATASTROPHE FORGETTING in report. PPO is very susceptible and converges on bad policies often\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
