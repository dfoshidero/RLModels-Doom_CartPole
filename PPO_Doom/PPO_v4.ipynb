{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0547f-535d-49a1-a0cb-2df24c584a95",
   "metadata": {},
   "source": [
    "## Initialize VizDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cddcc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary\n",
    "#!pip install vizdoom\n",
    "#!pip install opencv-python\n",
    "#!pip install pandas\n",
    "#!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34143102-2b66-4ee2-9f76-f6db917d2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f8b51-d30c-4c9f-a8b1-8e24b891a576",
   "metadata": {},
   "source": [
    "## Make it a Gym Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df5aed06-301c-43ff-a0ab-54dca32e78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2\n",
    "\n",
    "LEVEL = 'defend_the_center'\n",
    "DOOM_SKILL = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47d02cc2-14b1-4eb2-a032-add0d7ed26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('VizDoom/scenarios/defend_the_center.cfg')\n",
    "        \n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 4) # get action using index -> left, right, shoot\n",
    "        \n",
    "        # Get all the other stuff we need to return \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            ammo = self.game.get_state().game_variables[0] \n",
    "            info = ammo\n",
    "        # If we dont have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "            \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer  # Apply Grayscale\n",
    "\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48eacd6-a69d-4d14-b890-83f76c4a5e67",
   "metadata": {},
   "source": [
    "## Custom PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7728b4d-a680-4896-b0ea-a68a6683d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c010c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional output size calculator\n",
    "def conv2d_size_out(size, kernel_size = 3, stride = 2, padding = 0):\n",
    "    return (size + 2 * padding - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, n_output):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Temporarily assume some output size after convolution\n",
    "        # This should ideally be calculated based on input size\n",
    "        self.feature_count = 64 * conv2d_size_out(conv2d_size_out(conv2d_size_out(100, 8, 4), 4, 2), 3, 1) * \\\n",
    "                        conv2d_size_out(conv2d_size_out(conv2d_size_out(160, 8, 4), 4, 2), 3, 1)\n",
    "\n",
    "        self.fc = nn.Linear(self.feature_count, 512)\n",
    "        self.actor = nn.Linear(512, n_output)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(-1, self.feature_count)  # Use reshape instead of view\n",
    "        x = F.relu(self.fc(x))\n",
    "        action_probs = F.softmax(self.actor(x), dim=1)\n",
    "        value = self.critic(x)\n",
    "        return action_probs, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39228483",
   "metadata": {},
   "outputs": [
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 126\u001b[0m\n\u001b[0;32m    123\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m ppo_agent \u001b[38;5;241m=\u001b[39m PPO(env, in_channels, n_actions, device)\n\u001b[1;32m--> 126\u001b[0m \u001b[43mppo_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train for 500 episodes\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 25\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_episodes):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[1;32m---> 25\u001b[0m         batch_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_policy(batch_data)\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Data = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 38\u001b[0m, in \u001b[0;36mPPO.collect_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     35\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m t \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps_per_batch:\n\u001b[1;32m---> 38\u001b[0m     state, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     40\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 52\u001b[0m, in \u001b[0;36mVizDoomGym.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_state()\u001b[38;5;241m.\u001b[39mscreen_buffer \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_state() \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m160\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrayscale(state)\n",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "## PPO \n",
    "    \n",
    "class PPO:\n",
    "    def __init__(self, env, in_channels, n_actions, device):\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        self._init_hyperparameters()\n",
    "        self.actor = ActorCriticNetwork(in_channels, n_actions)\n",
    "        self.critic = ActorCriticNetwork(in_channels, 1)\n",
    "\n",
    "        #Initialise optimizer\n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=self.lr)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=self.lr)\n",
    "\n",
    "    def _init_hyperparameters(self):\n",
    "        self.timesteps_per_batch = 1\n",
    "        self.max_timesteps_per_episode = 1600\n",
    "        self.gamma = 0.95\n",
    "        self.n_updates_per_iteration = 5\n",
    "        self.lr = 0.005\n",
    "        self.clip = 0.2  # Clipping parameter for PPO\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            batch_data = self.collect_data()\n",
    "            self.update_policy(batch_data)\n",
    "            print(f\"Episode {episode}: Data = {batch_data}\")\n",
    "\n",
    "    def collect_data(self):\n",
    "        batch_obs = []\n",
    "        batch_acts = []\n",
    "        batch_log_probs = []\n",
    "        batch_rews = []\n",
    "        batch_values = []\n",
    "        t = 0\n",
    "\n",
    "        while t < self.timesteps_per_batch:\n",
    "            state, _ = self.env.reset()\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).permute(0, 3, 1, 2).to(self.device)\n",
    "            done = False\n",
    "            while not done and t < self.timesteps_per_batch:\n",
    "                policy_dist, value = self.actor(state)  # Changed here\n",
    "                action = policy_dist.multinomial(num_samples=1).detach()\n",
    "                log_prob = torch.log(policy_dist.squeeze(0)[action])\n",
    "                next_state, reward, done, _, _ = self.env.step(action.item())\n",
    "\n",
    "                batch_obs.append(state)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                batch_rews.append(reward)\n",
    "                batch_values.append(value)\n",
    "\n",
    "                state = torch.from_numpy(next_state).float().unsqueeze(0).permute(0, 3, 1, 2).to(self.device)\n",
    "                t += 1\n",
    "\n",
    "        rewards_to_go = PPO.calculate_rewards_to_go(torch.tensor(batch_rews, dtype=torch.float), self.gamma)\n",
    "        return {\n",
    "            \"obs\": torch.cat(batch_obs),\n",
    "            \"acts\": torch.stack(batch_acts),\n",
    "            \"log_probs\": torch.stack(batch_log_probs),\n",
    "            \"rewards\": torch.tensor(batch_rews, dtype=torch.float),\n",
    "            \"values\": torch.cat(batch_values),\n",
    "            \"rewards_to_go\": rewards_to_go  # Add this line\n",
    "        }\n",
    "\n",
    "    def update_policy(self, batch_data):\n",
    "        batch_obs = batch_data['obs']\n",
    "        batch_acts = batch_data['acts']\n",
    "        batch_log_probs = batch_data['log_probs']\n",
    "        batch_rtgs = batch_data['rewards_to_go']  # Assuming rewards to go are calculated and passed\n",
    "\n",
    "        # Calculate current V and log probs\n",
    "        current_V, current_log_probs = self.evaluate(batch_obs, batch_acts)\n",
    "\n",
    "        # Compute advantages and normalize\n",
    "        A_k = batch_rtgs - current_V.detach()\n",
    "        A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "\n",
    "        for _ in range(self.n_updates_per_iteration):\n",
    "            # Recalculate V and log_probs for the current policy\n",
    "            V, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
    "\n",
    "            # Calculate the ratio (pi_theta / pi_theta_old)\n",
    "            ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "\n",
    "            # Actor Loss: PPO's clipped objective\n",
    "            surr1 = ratios * A_k\n",
    "            surr2 = torch.clamp(ratios, 1.0 - self.clip, 1.0 + self.clip) * A_k\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            # Critic Loss\n",
    "            critic_loss = F.mse_loss(V, batch_rtgs)\n",
    "\n",
    "            # Perform backward propagation for critic\n",
    "            self.critic_optim.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            self.critic_optim.step()\n",
    "\n",
    "            # Perform backward propagation for actor\n",
    "            self.actor_optim.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            self.actor_optim.step()\n",
    "\n",
    "    def calculate_rewards_to_go(rewards, gamma):\n",
    "        n = len(rewards)\n",
    "        rewards_to_go = torch.zeros_like(rewards)\n",
    "        for i in reversed(range(n)):\n",
    "            rewards_to_go[i] = rewards[i] + (gamma * rewards_to_go[i + 1] if i + 1 < n else 0)\n",
    "        return rewards_to_go\n",
    "\n",
    "    def evaluate(self, batch_obs, batch_acts):\n",
    "        # Get the policy distribution and value estimate for the given observations\n",
    "        policy_dist, value = self.actor(batch_obs)  # Changed here\n",
    "        dist = Categorical(policy_dist)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "\n",
    "        return value, log_probs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = VizDoomGym(render=True)\n",
    "    in_channels = 1  # Assuming grayscale input\n",
    "    n_actions = env.action_space.n\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ppo_agent = PPO(env, in_channels, n_actions, device)\n",
    "    ppo_agent.train(500)  # Train for 500 episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64290712",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b37c6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed observation shape (should be [1, 1, 96, 96]): torch.Size([1, 1, 96, 96])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 9216]' is invalid for input of size 4096",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed observation shape (should be [1, 1, 96, 96]):\u001b[39m\u001b[38;5;124m\"\u001b[39m, obs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 45\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     46\u001b[0m obs, reward, done, _, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Favour-Daniel\\anaconda3\\envs\\Doom\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 26\u001b[0m, in \u001b[0;36mActorCriticNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[0;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))\n\u001b[1;32m---> 26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_count\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use reshape instead of view\u001b[39;00m\n\u001b[0;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x))\n\u001b[0;32m     28\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 9216]' is invalid for input of size 4096"
     ]
    }
   ],
   "source": [
    "def load_model(model_path):\n",
    "    model = ActorCriticNetwork(in_channels=1, n_output=3)  # Assuming grayscale images and 3 action outputs\n",
    "    state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Ensure to load on CPU if GPU is not available\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load the PyTorch model\n",
    "model = load_model('E:\\RLModelTraining\\Doom_PPO\\model_batch_25713_timestep_batch_25713.pt')  # Replace with your actual model path\n",
    "\n",
    "# Helper function to preprocess observations if needed\n",
    "def preprocess(observation):\n",
    "    # Extract the array part of the observation if it's in a tuple\n",
    "    if isinstance(observation, tuple):\n",
    "        observation = observation[0]\n",
    "\n",
    "    # Check if the observation is a numpy array and has the expected number of dimensions\n",
    "    if not isinstance(observation, np.ndarray) or observation.ndim != 3:\n",
    "        raise ValueError(\"Observation is not in the expected format or shape\")\n",
    "\n",
    "    # Convert to grayscale (assuming the model expects single-channel inputs)\n",
    "    if observation.shape[2] == 3:\n",
    "        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Resize the image to 96x96, which should match the model's input layer requirement\n",
    "    observation = cv2.resize(observation, (96, 96))\n",
    "\n",
    "    # Normalize and add necessary dimensions for PyTorch\n",
    "    observation = np.expand_dims(observation, axis=0)  # Add batch dimension\n",
    "    observation = np.expand_dims(observation, axis=0)  # Add channel dimension\n",
    "    observation = torch.tensor(observation, dtype=torch.float32) / 255.0  # Normalize\n",
    "\n",
    "    return observation\n",
    "\n",
    "\n",
    "\n",
    "env = VizDoomGym(render=True)\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    obs = preprocess(obs)\n",
    "    print(\"Processed observation shape (should be [1, 1, 96, 96]):\", obs.shape)\n",
    "    with torch.no_grad():\n",
    "        action = model(obs).max(1)[1].item()\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7664881-22e2-42a6-8ab9-693188f1d2c3",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
