{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0547f-535d-49a1-a0cb-2df24c584a95",
   "metadata": {},
   "source": [
    "## Initialize VizDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "6cddcc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary\n",
    "#!pip install vizdoom\n",
    "#!pip install opencv-python\n",
    "#!pip install pandas\n",
    "#!pip install torch\n",
    "#!pip install gym\n",
    "#!pip install pyglet==1.5.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "34143102-2b66-4ee2-9f76-f6db917d2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f8b51-d30c-4c9f-a8b1-8e24b891a576",
   "metadata": {},
   "source": [
    "## Make it a Gym Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "df5aed06-301c-43ff-a0ab-54dca32e78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2\n",
    "\n",
    "LEVEL = 'defend_the_center'\n",
    "DOOM_SKILL = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "47d02cc2-14b1-4eb2-a032-add0d7ed26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('VizDoom/scenarios/defend_the_center.cfg')\n",
    "        \n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 4) # get action using index -> left, right, shoot\n",
    "        \n",
    "        # Get all the other stuff we need to return \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            ammo = self.game.get_state().game_variables[0] \n",
    "            info = ammo\n",
    "        # If we dont have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "            \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer  # Apply Grayscale\n",
    "\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48eacd6-a69d-4d14-b890-83f76c4a5e67",
   "metadata": {},
   "source": [
    "## Custom PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b7728b4d-a680-4896-b0ea-a68a6683d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "39228483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Algorithm\n",
    "\n",
    "\"\"\"\n",
    "https://www.youtube.com/watch?v=hlv79rcHws0\n",
    "\"\"\"\n",
    "\n",
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, input_dims, alpha,\n",
    "                 fc1_dims=256, fc2_dims=256, checkpoint_dir='tmp/ppo'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, 'actor_torch_ppo')\n",
    "        os.makedirs(os.path.dirname(self.checkpoint_file), exist_ok=True)\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(*input_dims, fc1_dims),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(fc2_dims, n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "\n",
    "        return dist\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256, \n",
    "                 checkpoint_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(checkpoint_dir, 'critic_torch_ppo')\n",
    "        os.makedirs(os.path.dirname(self.checkpoint_file), exist_ok=True)\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(*input_dims, fc1_dims),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(fc2_dims, 1),\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "\n",
    "        return value\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, n_actions, input_dims, gamma, alpha, gae_lambda,\n",
    "                 policy_clip, batch_size, N, n_epochs):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
    "        self.critic = CriticNetwork(input_dims, alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []\n",
    "\n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('...saving models...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        print('...loading models...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "\n",
    "        dist = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def learn(self):\n",
    "        entropy_coefficient = 0.01  # Set the entropy coefficient to a suitable value\n",
    "        episode_actor_losses = []\n",
    "        episode_critic_losses = []\n",
    "        episode_values = []\n",
    "        \n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_probs_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "            \n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount * (reward_arr[k] + self.gamma * values[k+1] * (1 - int(dones_arr[k])) - values[k])\n",
    "                advantage[t] = a_t\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(old_probs_arr[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip, 1+self.policy_clip) * advantage[batch]\n",
    "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                # Calculate entropy bonus\n",
    "                entropy = dist.entropy().mean()\n",
    "                actor_loss -= entropy_coefficient * entropy  # Adding entropy bonus\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns - critic_value) ** 2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5 * critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "            # Collect losses for each batch\n",
    "            episode_actor_losses.append(actor_loss.item())\n",
    "            episode_critic_losses.append(critic_loss.item())\n",
    "            episode_values.append(critic_value.mean().item())\n",
    "\n",
    "        # Store average loss and value for the episode\n",
    "        self.actor_losses.append(np.mean(episode_actor_losses))\n",
    "        self.critic_losses.append(np.mean(episode_critic_losses))\n",
    "        self.values.append(np.mean(episode_values))\n",
    "\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "    # Reset stored data after each episode or training session\n",
    "    def reset_learning_debug_data(self):\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []\n",
    "\n",
    "    def reset_learning_debug_data(self):\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.values = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e7b4f",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "6d518ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hyperparameters(env, agent_params, n_games=50):\n",
    "    agent = PPOAgent(**agent_params)\n",
    "    total_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _, _ = agent.choose_action(observation)\n",
    "            observation, reward, done, info, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    return avg_reward\n",
    "\n",
    "def hyperparameter_tuning():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    gammas = [0.99, 0.95, 0.90]\n",
    "    gae_lambdas = [0.95, 0.97, 0.99]\n",
    "\n",
    "    best_avg_reward = -np.inf\n",
    "    best_params = {}\n",
    "\n",
    "    for alpha in learning_rates:\n",
    "        for gamma in gammas:\n",
    "            for gae_lambda in gae_lambdas:\n",
    "                agent_params = {\n",
    "                    'n_actions': env.action_space.n,\n",
    "                    'input_dims': env.observation_space.shape,\n",
    "                    'alpha': alpha,\n",
    "                    'gamma': gamma,\n",
    "                    'gae_lambda': gae_lambda,\n",
    "                    'policy_clip': 0.5,\n",
    "                    'batch_size': 5,\n",
    "                    'N': 2048,\n",
    "                    'n_epochs': 4\n",
    "                }\n",
    "                avg_reward = evaluate_hyperparameters(env, agent_params)\n",
    "                print(f'Tested {agent_params} -> Avg Reward: {avg_reward}')\n",
    "                \n",
    "                if avg_reward > best_avg_reward:\n",
    "                    best_avg_reward = avg_reward\n",
    "                    best_params = agent_params\n",
    "\n",
    "    print(f\"Best Hyperparameters: {best_params} with Average Reward: {best_avg_reward}\")\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7664881-22e2-42a6-8ab9-693188f1d2c3",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "1566fc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 22.83\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.97, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 22.51\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.99, 'gae_lambda': 0.99, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 23.11\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 22.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.97, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 20.41\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.95, 'gae_lambda': 0.99, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 20.38\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.9, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 22.45\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.9, 'gae_lambda': 0.97, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 22.62\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.001, 'gamma': 0.9, 'gae_lambda': 0.99, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 21.22\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 21.11\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.97, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 23.76\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.99, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 22.79\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 21.03\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.97, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 21.04\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.95, 'gae_lambda': 0.99, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 21.19\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.9, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 21.32\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.9, 'gae_lambda': 0.97, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 21.67\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.9, 'gae_lambda': 0.99, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 21.3\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 21.88\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.97, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 22.35\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.99, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 22.5\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 19.31\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.97, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 22.34\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.95, 'gae_lambda': 0.99, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 22.0\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.9, 'gae_lambda': 0.95, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 19.28\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.9, 'gae_lambda': 0.97, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 22.42\n",
      "Tested {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0001, 'gamma': 0.9, 'gae_lambda': 0.99, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} -> Avg Reward: 22.2\n",
      "Best Hyperparameters: {'n_actions': 2, 'input_dims': (4,), 'alpha': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.97, 'policy_clip': 0.2, 'batch_size': 5, 'N': 2048, 'n_epochs': 4} with Average Reward: 23.76\n",
      "...saving models...\n",
      "Episode  0 : score 22.0  avg score 22.0  timesteps 22  learning steps 1\n",
      "...saving models...\n",
      "Episode  1 : score 33.0  avg score 27.5  timesteps 55  learning steps 2\n",
      "Episode  2 : score 10.0  avg score 21.7  timesteps 65  learning steps 3\n",
      "Episode  3 : score 12.0  avg score 19.2  timesteps 77  learning steps 3\n",
      "Episode  4 : score 17.0  avg score 18.8  timesteps 94  learning steps 4\n",
      "Episode  5 : score 13.0  avg score 17.8  timesteps 107  learning steps 5\n",
      "Episode  6 : score 16.0  avg score 17.6  timesteps 123  learning steps 6\n",
      "Episode  7 : score 18.0  avg score 17.6  timesteps 141  learning steps 7\n",
      "Episode  8 : score 10.0  avg score 16.8  timesteps 151  learning steps 7\n",
      "Episode  9 : score 10.0  avg score 16.1  timesteps 161  learning steps 8\n",
      "Episode  10 : score 11.0  avg score 15.6  timesteps 172  learning steps 8\n",
      "Episode  11 : score 9.0  avg score 15.1  timesteps 181  learning steps 9\n",
      "Episode  12 : score 9.0  avg score 14.6  timesteps 190  learning steps 9\n",
      "Episode  13 : score 13.0  avg score 14.5  timesteps 203  learning steps 10\n",
      "Episode  14 : score 10.0  avg score 14.2  timesteps 213  learning steps 10\n",
      "Episode  15 : score 12.0  avg score 14.1  timesteps 225  learning steps 11\n",
      "Episode  16 : score 18.0  avg score 14.3  timesteps 243  learning steps 12\n",
      "Episode  17 : score 29.0  avg score 15.1  timesteps 272  learning steps 13\n",
      "Episode  18 : score 32.0  avg score 16.0  timesteps 304  learning steps 15\n",
      "Episode  19 : score 18.0  avg score 16.1  timesteps 322  learning steps 16\n",
      "Episode  20 : score 27.0  avg score 16.6  timesteps 349  learning steps 17\n",
      "Episode  21 : score 13.0  avg score 16.5  timesteps 362  learning steps 18\n",
      "Episode  22 : score 19.0  avg score 16.6  timesteps 381  learning steps 19\n",
      "Episode  23 : score 21.0  avg score 16.8  timesteps 402  learning steps 20\n",
      "Episode  24 : score 17.0  avg score 16.8  timesteps 419  learning steps 20\n",
      "Episode  25 : score 32.0  avg score 17.3  timesteps 451  learning steps 22\n",
      "Episode  26 : score 24.0  avg score 17.6  timesteps 475  learning steps 23\n",
      "Episode  27 : score 64.0  avg score 19.2  timesteps 539  learning steps 26\n",
      "Episode  28 : score 75.0  avg score 21.2  timesteps 614  learning steps 30\n",
      "Episode  29 : score 135.0  avg score 25.0  timesteps 749  learning steps 37\n",
      "Episode  30 : score 48.0  avg score 25.7  timesteps 797  learning steps 39\n",
      "Episode  31 : score 50.0  avg score 26.5  timesteps 847  learning steps 42\n",
      "...saving models...\n",
      "Episode  32 : score 93.0  avg score 28.5  timesteps 940  learning steps 47\n",
      "Episode  33 : score 25.0  avg score 28.4  timesteps 965  learning steps 48\n",
      "...saving models...\n",
      "Episode  34 : score 125.0  avg score 31.1  timesteps 1090  learning steps 54\n",
      "...saving models...\n",
      "Episode  35 : score 78.0  avg score 32.4  timesteps 1168  learning steps 58\n",
      "...saving models...\n",
      "Episode  36 : score 124.0  avg score 34.9  timesteps 1292  learning steps 64\n",
      "...saving models...\n",
      "Episode  37 : score 116.0  avg score 37.1  timesteps 1408  learning steps 70\n",
      "Episode  38 : score 33.0  avg score 36.9  timesteps 1441  learning steps 72\n",
      "Episode  39 : score 26.0  avg score 36.7  timesteps 1467  learning steps 73\n",
      "Episode  40 : score 34.0  avg score 36.6  timesteps 1501  learning steps 75\n",
      "Episode  41 : score 27.0  avg score 36.4  timesteps 1528  learning steps 76\n",
      "Episode  42 : score 36.0  avg score 36.4  timesteps 1564  learning steps 78\n",
      "Episode  43 : score 25.0  avg score 36.1  timesteps 1589  learning steps 79\n",
      "...saving models...\n",
      "Episode  44 : score 131.0  avg score 38.2  timesteps 1720  learning steps 86\n",
      "...saving models...\n",
      "Episode  45 : score 83.0  avg score 39.2  timesteps 1803  learning steps 90\n",
      "...saving models...\n",
      "Episode  46 : score 213.0  avg score 42.9  timesteps 2016  learning steps 100\n",
      "...saving models...\n",
      "Episode  47 : score 81.0  avg score 43.7  timesteps 2097  learning steps 104\n",
      "...saving models...\n",
      "Episode  48 : score 222.0  avg score 47.3  timesteps 2319  learning steps 115\n",
      "...saving models...\n",
      "Episode  49 : score 92.0  avg score 48.2  timesteps 2411  learning steps 120\n",
      "...saving models...\n",
      "Episode  50 : score 78.0  avg score 48.8  timesteps 2489  learning steps 124\n",
      "...saving models...\n",
      "Episode  51 : score 97.0  avg score 49.7  timesteps 2586  learning steps 129\n",
      "...saving models...\n",
      "Episode  52 : score 178.0  avg score 52.2  timesteps 2764  learning steps 138\n",
      "...saving models...\n",
      "Episode  53 : score 178.0  avg score 54.5  timesteps 2942  learning steps 147\n",
      "...saving models...\n",
      "Episode  54 : score 378.0  avg score 60.4  timesteps 3320  learning steps 166\n",
      "...saving models...\n",
      "Episode  55 : score 161.0  avg score 62.2  timesteps 3481  learning steps 174\n",
      "...saving models...\n",
      "Episode  56 : score 69.0  avg score 62.3  timesteps 3550  learning steps 177\n",
      "...saving models...\n",
      "Episode  57 : score 336.0  avg score 67.0  timesteps 3886  learning steps 194\n",
      "...saving models...\n",
      "Episode  58 : score 108.0  avg score 67.7  timesteps 3994  learning steps 199\n",
      "...saving models...\n",
      "Episode  59 : score 222.0  avg score 70.3  timesteps 4216  learning steps 210\n",
      "...saving models...\n",
      "Episode  60 : score 135.0  avg score 71.3  timesteps 4351  learning steps 217\n",
      "...saving models...\n",
      "Episode  61 : score 130.0  avg score 72.3  timesteps 4481  learning steps 224\n",
      "...saving models...\n",
      "Episode  62 : score 103.0  avg score 72.8  timesteps 4584  learning steps 229\n",
      "...saving models...\n",
      "Episode  63 : score 194.0  avg score 74.7  timesteps 4778  learning steps 238\n",
      "...saving models...\n",
      "Episode  64 : score 151.0  avg score 75.8  timesteps 4929  learning steps 246\n",
      "...saving models...\n",
      "Episode  65 : score 204.0  avg score 77.8  timesteps 5133  learning steps 256\n",
      "...saving models...\n",
      "Episode  66 : score 711.0  avg score 87.2  timesteps 5844  learning steps 292\n",
      "...saving models...\n",
      "Episode  67 : score 313.0  avg score 90.5  timesteps 6157  learning steps 307\n",
      "...saving models...\n",
      "Episode  68 : score 175.0  avg score 91.8  timesteps 6332  learning steps 316\n",
      "...saving models...\n",
      "Episode  69 : score 379.0  avg score 95.9  timesteps 6711  learning steps 335\n",
      "Episode  70 : score 95.0  avg score 95.9  timesteps 6806  learning steps 340\n",
      "...saving models...\n",
      "Episode  71 : score 323.0  avg score 99.0  timesteps 7129  learning steps 356\n",
      "...saving models...\n",
      "Episode  72 : score 1881.0  avg score 123.4  timesteps 9010  learning steps 450\n",
      "...saving models...\n",
      "Episode  73 : score 265.0  avg score 125.3  timesteps 9275  learning steps 463\n",
      "...saving models...\n",
      "Episode  74 : score 213.0  avg score 126.5  timesteps 9488  learning steps 474\n",
      "...saving models...\n",
      "Episode  75 : score 600.0  avg score 132.7  timesteps 10088  learning steps 504\n",
      "...saving models...\n",
      "Episode  76 : score 310.0  avg score 135.0  timesteps 10398  learning steps 519\n",
      "...saving models...\n",
      "Episode  77 : score 461.0  avg score 139.2  timesteps 10859  learning steps 542\n",
      "...saving models...\n",
      "Episode  78 : score 941.0  avg score 149.4  timesteps 11800  learning steps 590\n",
      "...saving models...\n",
      "Episode  79 : score 1080.0  avg score 161.0  timesteps 12880  learning steps 644\n",
      "...saving models...\n",
      "Episode  80 : score 855.0  avg score 169.6  timesteps 13735  learning steps 686\n",
      "...saving models...\n",
      "Episode  81 : score 8716.0  avg score 273.8  timesteps 22451  learning steps 1122\n",
      "Episode  82 : score 9.0  avg score 270.6  timesteps 22460  learning steps 1123\n",
      "...saving models...\n",
      "Episode  83 : score 1164.0  avg score 281.2  timesteps 23624  learning steps 1181\n",
      "Episode  84 : score 46.0  avg score 278.5  timesteps 23670  learning steps 1183\n",
      "Episode  85 : score 124.0  avg score 276.7  timesteps 23794  learning steps 1189\n",
      "Episode  86 : score 221.0  avg score 276.0  timesteps 24015  learning steps 1200\n",
      "Episode  87 : score 104.0  avg score 274.1  timesteps 24119  learning steps 1205\n",
      "Episode  88 : score 209.0  avg score 273.3  timesteps 24328  learning steps 1216\n",
      "Episode  89 : score 202.0  avg score 272.6  timesteps 24530  learning steps 1226\n",
      "Episode  90 : score 192.0  avg score 271.7  timesteps 24722  learning steps 1236\n",
      "Episode  91 : score 103.0  avg score 269.8  timesteps 24825  learning steps 1241\n",
      "Episode  92 : score 118.0  avg score 268.2  timesteps 24943  learning steps 1247\n",
      "Episode  93 : score 105.0  avg score 266.5  timesteps 25048  learning steps 1252\n",
      "Episode  94 : score 164.0  avg score 265.4  timesteps 25212  learning steps 1260\n",
      "Episode  95 : score 84.0  avg score 263.5  timesteps 25296  learning steps 1264\n",
      "Episode  96 : score 173.0  avg score 262.6  timesteps 25469  learning steps 1273\n",
      "Episode  97 : score 155.0  avg score 261.5  timesteps 25624  learning steps 1281\n",
      "Episode  98 : score 73.0  avg score 259.6  timesteps 25697  learning steps 1284\n",
      "Episode  99 : score 123.0  avg score 258.2  timesteps 25820  learning steps 1291\n",
      "Episode  100 : score 199.0  avg score 260.0  timesteps 26019  learning steps 1300\n",
      "Episode  101 : score 114.0  avg score 260.8  timesteps 26133  learning steps 1306\n",
      "Episode  102 : score 83.0  avg score 261.5  timesteps 26216  learning steps 1310\n",
      "Episode  103 : score 132.0  avg score 262.7  timesteps 26348  learning steps 1317\n",
      "Episode  104 : score 91.0  avg score 263.4  timesteps 26439  learning steps 1321\n",
      "Episode  105 : score 161.0  avg score 264.9  timesteps 26600  learning steps 1330\n",
      "Episode  106 : score 147.0  avg score 266.2  timesteps 26747  learning steps 1337\n",
      "Episode  107 : score 181.0  avg score 267.9  timesteps 26928  learning steps 1346\n",
      "Episode  108 : score 70.0  avg score 268.5  timesteps 26998  learning steps 1349\n",
      "Episode  109 : score 115.0  avg score 269.5  timesteps 27113  learning steps 1355\n",
      "Episode  110 : score 131.0  avg score 270.7  timesteps 27244  learning steps 1362\n",
      "Episode  111 : score 113.0  avg score 271.8  timesteps 27357  learning steps 1367\n",
      "Episode  112 : score 165.0  avg score 273.3  timesteps 27522  learning steps 1376\n",
      "Episode  113 : score 131.0  avg score 274.5  timesteps 27653  learning steps 1382\n",
      "Episode  114 : score 82.0  avg score 275.2  timesteps 27735  learning steps 1386\n",
      "Episode  115 : score 69.0  avg score 275.8  timesteps 27804  learning steps 1390\n",
      "Episode  116 : score 90.0  avg score 276.5  timesteps 27894  learning steps 1394\n",
      "Episode  117 : score 107.0  avg score 277.3  timesteps 28001  learning steps 1400\n",
      "Episode  118 : score 100.0  avg score 278.0  timesteps 28101  learning steps 1405\n",
      "Episode  119 : score 174.0  avg score 279.5  timesteps 28275  learning steps 1413\n",
      "Episode  120 : score 104.0  avg score 280.3  timesteps 28379  learning steps 1418\n",
      "Episode  121 : score 99.0  avg score 281.2  timesteps 28478  learning steps 1423\n",
      "...saving models...\n",
      "Episode  122 : score 148.0  avg score 282.4  timesteps 28626  learning steps 1431\n",
      "...saving models...\n",
      "Episode  123 : score 130.0  avg score 283.5  timesteps 28756  learning steps 1437\n",
      "...saving models...\n",
      "Episode  124 : score 91.0  avg score 284.3  timesteps 28847  learning steps 1442\n",
      "...saving models...\n",
      "Episode  125 : score 101.0  avg score 285.0  timesteps 28948  learning steps 1447\n",
      "...saving models...\n",
      "Episode  126 : score 182.0  avg score 286.6  timesteps 29130  learning steps 1456\n",
      "...saving models...\n",
      "Episode  127 : score 136.0  avg score 287.3  timesteps 29266  learning steps 1463\n",
      "...saving models...\n",
      "Episode  128 : score 170.0  avg score 288.2  timesteps 29436  learning steps 1471\n",
      "...saving models...\n",
      "Episode  129 : score 217.0  avg score 289.0  timesteps 29653  learning steps 1482\n",
      "...saving models...\n",
      "Episode  130 : score 150.0  avg score 290.1  timesteps 29803  learning steps 1490\n",
      "...saving models...\n",
      "Episode  131 : score 129.0  avg score 290.9  timesteps 29932  learning steps 1496\n",
      "...saving models...\n",
      "Episode  132 : score 202.0  avg score 291.9  timesteps 30134  learning steps 1506\n",
      "...saving models...\n",
      "Episode  133 : score 184.0  avg score 293.5  timesteps 30318  learning steps 1515\n",
      "...saving models...\n",
      "Episode  134 : score 167.0  avg score 293.9  timesteps 30485  learning steps 1524\n",
      "...saving models...\n",
      "Episode  135 : score 225.0  avg score 295.4  timesteps 30710  learning steps 1535\n",
      "Episode  136 : score 119.0  avg score 295.4  timesteps 30829  learning steps 1541\n",
      "...saving models...\n",
      "Episode  137 : score 141.0  avg score 295.6  timesteps 30970  learning steps 1548\n",
      "...saving models...\n",
      "Episode  138 : score 111.0  avg score 296.4  timesteps 31081  learning steps 1554\n",
      "...saving models...\n",
      "Episode  139 : score 122.0  avg score 297.4  timesteps 31203  learning steps 1560\n",
      "...saving models...\n",
      "Episode  140 : score 131.0  avg score 298.3  timesteps 31334  learning steps 1566\n",
      "...saving models...\n",
      "Episode  141 : score 126.0  avg score 299.3  timesteps 31460  learning steps 1573\n",
      "...saving models...\n",
      "Episode  142 : score 109.0  avg score 300.1  timesteps 31569  learning steps 1578\n",
      "...saving models...\n",
      "Episode  143 : score 105.0  avg score 300.9  timesteps 31674  learning steps 1583\n",
      "...saving models...\n",
      "Episode  144 : score 195.0  avg score 301.5  timesteps 31869  learning steps 1593\n",
      "...saving models...\n",
      "Episode  145 : score 148.0  avg score 302.1  timesteps 32017  learning steps 1600\n",
      "Episode  146 : score 203.0  avg score 302.0  timesteps 32220  learning steps 1611\n",
      "...saving models...\n",
      "Episode  147 : score 182.0  avg score 303.1  timesteps 32402  learning steps 1620\n",
      "Episode  148 : score 214.0  avg score 303.0  timesteps 32616  learning steps 1630\n",
      "...saving models...\n",
      "Episode  149 : score 246.0  avg score 304.5  timesteps 32862  learning steps 1643\n",
      "...saving models...\n",
      "Episode  150 : score 506.0  avg score 308.8  timesteps 33368  learning steps 1668\n",
      "...saving models...\n",
      "Episode  151 : score 153.0  avg score 309.4  timesteps 33521  learning steps 1676\n",
      "...saving models...\n",
      "Episode  152 : score 188.0  avg score 309.4  timesteps 33709  learning steps 1685\n",
      "...saving models...\n",
      "Episode  153 : score 285.0  avg score 310.5  timesteps 33994  learning steps 1699\n",
      "Episode  154 : score 244.0  avg score 309.2  timesteps 34238  learning steps 1711\n",
      "Episode  155 : score 263.0  avg score 310.2  timesteps 34501  learning steps 1725\n",
      "...saving models...\n",
      "Episode  156 : score 236.0  avg score 311.9  timesteps 34737  learning steps 1736\n",
      "Episode  157 : score 206.0  avg score 310.6  timesteps 34943  learning steps 1747\n",
      "Episode  158 : score 215.0  avg score 311.6  timesteps 35158  learning steps 1757\n",
      "...saving models...\n",
      "Episode  159 : score 512.0  avg score 314.5  timesteps 35670  learning steps 1783\n",
      "...saving models...\n",
      "Episode  160 : score 149.0  avg score 314.7  timesteps 35819  learning steps 1790\n",
      "...saving models...\n",
      "Episode  161 : score 483.0  avg score 318.2  timesteps 36302  learning steps 1815\n",
      "...saving models...\n",
      "Episode  162 : score 221.0  avg score 319.4  timesteps 36523  learning steps 1826\n",
      "...saving models...\n",
      "Episode  163 : score 317.0  avg score 320.6  timesteps 36840  learning steps 1842\n",
      "...saving models...\n",
      "Episode  164 : score 374.0  avg score 322.9  timesteps 37214  learning steps 1860\n",
      "...saving models...\n",
      "Episode  165 : score 351.0  avg score 324.3  timesteps 37565  learning steps 1878\n",
      "Episode  166 : score 207.0  avg score 319.3  timesteps 37772  learning steps 1888\n",
      "Episode  167 : score 437.0  avg score 320.5  timesteps 38209  learning steps 1910\n",
      "Episode  168 : score 238.0  avg score 321.1  timesteps 38447  learning steps 1922\n",
      "Episode  169 : score 240.0  avg score 319.8  timesteps 38687  learning steps 1934\n",
      "Episode  170 : score 212.0  avg score 320.9  timesteps 38899  learning steps 1944\n",
      "Episode  171 : score 236.0  avg score 320.1  timesteps 39135  learning steps 1956\n",
      "Episode  172 : score 335.0  avg score 304.6  timesteps 39470  learning steps 1973\n",
      "Episode  173 : score 149.0  avg score 303.4  timesteps 39619  learning steps 1980\n",
      "Episode  174 : score 145.0  avg score 302.8  timesteps 39764  learning steps 1988\n",
      "Episode  175 : score 130.0  avg score 298.1  timesteps 39894  learning steps 1994\n",
      "Episode  176 : score 118.0  avg score 296.1  timesteps 40012  learning steps 2000\n",
      "Episode  177 : score 59.0  avg score 292.1  timesteps 40071  learning steps 2003\n",
      "Episode  178 : score 112.0  avg score 283.8  timesteps 40183  learning steps 2009\n",
      "Episode  179 : score 120.0  avg score 274.2  timesteps 40303  learning steps 2015\n",
      "Episode  180 : score 121.0  avg score 266.9  timesteps 40424  learning steps 2021\n",
      "Episode  181 : score 126.0  avg score 181.0  timesteps 40550  learning steps 2027\n",
      "Episode  182 : score 146.0  avg score 182.4  timesteps 40696  learning steps 2034\n",
      "Episode  183 : score 150.0  avg score 172.2  timesteps 40846  learning steps 2042\n",
      "Episode  184 : score 148.0  avg score 173.2  timesteps 40994  learning steps 2049\n",
      "Episode  185 : score 144.0  avg score 173.4  timesteps 41138  learning steps 2056\n",
      "Episode  186 : score 164.0  avg score 172.9  timesteps 41302  learning steps 2065\n",
      "Episode  187 : score 276.0  avg score 174.6  timesteps 41578  learning steps 2078\n",
      "Episode  188 : score 267.0  avg score 175.2  timesteps 41845  learning steps 2092\n",
      "Episode  189 : score 256.0  avg score 175.7  timesteps 42101  learning steps 2105\n",
      "Episode  190 : score 1904.0  avg score 192.8  timesteps 44005  learning steps 2200\n",
      "Episode  191 : score 675.0  avg score 198.6  timesteps 44680  learning steps 2234\n",
      "Episode  192 : score 2367.0  avg score 221.0  timesteps 47047  learning steps 2352\n",
      "Episode  193 : score 466.0  avg score 224.7  timesteps 47513  learning steps 2375\n",
      "Episode  194 : score 439.0  avg score 227.4  timesteps 47952  learning steps 2397\n",
      "Episode  195 : score 747.0  avg score 234.0  timesteps 48699  learning steps 2434\n",
      "Episode  196 : score 201.0  avg score 234.3  timesteps 48900  learning steps 2445\n",
      "Episode  197 : score 153.0  avg score 234.3  timesteps 49053  learning steps 2452\n",
      "Episode  198 : score 219.0  avg score 235.8  timesteps 49272  learning steps 2463\n",
      "Episode  199 : score 579.0  avg score 240.3  timesteps 49851  learning steps 2492\n",
      "Episode  200 : score 665.0  avg score 245.0  timesteps 50516  learning steps 2525\n",
      "Episode  201 : score 618.0  avg score 250.0  timesteps 51134  learning steps 2556\n",
      "Episode  202 : score 766.0  avg score 256.8  timesteps 51900  learning steps 2595\n",
      "Episode  203 : score 1206.0  avg score 267.6  timesteps 53106  learning steps 2655\n",
      "Episode  204 : score 142.0  avg score 268.1  timesteps 53248  learning steps 2662\n",
      "Episode  205 : score 362.0  avg score 270.1  timesteps 53610  learning steps 2680\n",
      "Episode  206 : score 295.0  avg score 271.6  timesteps 53905  learning steps 2695\n",
      "Episode  207 : score 217.0  avg score 271.9  timesteps 54122  learning steps 2706\n",
      "Episode  208 : score 859.0  avg score 279.8  timesteps 54981  learning steps 2749\n",
      "Episode  209 : score 425.0  avg score 282.9  timesteps 55406  learning steps 2770\n",
      "Episode  210 : score 654.0  avg score 288.2  timesteps 56060  learning steps 2803\n",
      "Episode  211 : score 357.0  avg score 290.6  timesteps 56417  learning steps 2820\n",
      "Episode  212 : score 1162.0  avg score 300.6  timesteps 57579  learning steps 2878\n",
      "Episode  213 : score 1033.0  avg score 309.6  timesteps 58612  learning steps 2930\n",
      "Episode  214 : score 771.0  avg score 316.5  timesteps 59383  learning steps 2969\n",
      "Episode  215 : score 800.0  avg score 323.8  timesteps 60183  learning steps 3009\n",
      "...saving models...\n",
      "Episode  216 : score 4079.0  avg score 363.7  timesteps 64262  learning steps 3213\n",
      "...saving models...\n",
      "Episode  217 : score 366.0  avg score 366.3  timesteps 64628  learning steps 3231\n",
      "...saving models...\n",
      "Episode  218 : score 234.0  avg score 367.6  timesteps 64862  learning steps 3243\n",
      "...saving models...\n",
      "Episode  219 : score 338.0  avg score 369.2  timesteps 65200  learning steps 3260\n",
      "...saving models...\n",
      "Episode  220 : score 418.0  avg score 372.4  timesteps 65618  learning steps 3280\n",
      "...saving models...\n",
      "Episode  221 : score 2774.0  avg score 399.1  timesteps 68392  learning steps 3419\n",
      "Episode  222 : score 147.0  avg score 399.1  timesteps 68539  learning steps 3426\n",
      "...saving models...\n",
      "Episode  223 : score 139.0  avg score 399.2  timesteps 68678  learning steps 3433\n",
      "...saving models...\n",
      "Episode  224 : score 158.0  avg score 399.9  timesteps 68836  learning steps 3441\n",
      "...saving models...\n",
      "Episode  225 : score 186.0  avg score 400.7  timesteps 69022  learning steps 3451\n",
      "...saving models...\n",
      "Episode  226 : score 189.0  avg score 400.8  timesteps 69211  learning steps 3460\n",
      "...saving models...\n",
      "Episode  227 : score 200.0  avg score 401.4  timesteps 69411  learning steps 3470\n",
      "...saving models...\n",
      "Episode  228 : score 223.0  avg score 402.0  timesteps 69634  learning steps 3481\n",
      "Episode  229 : score 171.0  avg score 401.5  timesteps 69805  learning steps 3490\n",
      "...saving models...\n",
      "Episode  230 : score 474.0  avg score 404.8  timesteps 70279  learning steps 3513\n",
      "...saving models...\n",
      "Episode  231 : score 179.0  avg score 405.3  timesteps 70458  learning steps 3522\n",
      "Episode  232 : score 144.0  avg score 404.7  timesteps 70602  learning steps 3530\n",
      "Episode  233 : score 124.0  avg score 404.1  timesteps 70726  learning steps 3536\n",
      "Episode  234 : score 158.0  avg score 404.0  timesteps 70884  learning steps 3544\n",
      "Episode  235 : score 122.0  avg score 403.0  timesteps 71006  learning steps 3550\n",
      "Episode  236 : score 117.0  avg score 402.9  timesteps 71123  learning steps 3556\n",
      "Episode  237 : score 148.0  avg score 403.0  timesteps 71271  learning steps 3563\n",
      "Episode  238 : score 153.0  avg score 403.4  timesteps 71424  learning steps 3571\n",
      "Episode  239 : score 213.0  avg score 404.3  timesteps 71637  learning steps 3581\n",
      "...saving models...\n",
      "Episode  240 : score 238.0  avg score 405.4  timesteps 71875  learning steps 3593\n",
      "...saving models...\n",
      "Episode  241 : score 2614.0  avg score 430.3  timesteps 74489  learning steps 3724\n",
      "...saving models...\n",
      "Episode  242 : score 259.0  avg score 431.8  timesteps 74748  learning steps 3737\n",
      "...saving models...\n",
      "Episode  243 : score 141.0  avg score 432.1  timesteps 74889  learning steps 3744\n",
      "Episode  244 : score 132.0  avg score 431.5  timesteps 75021  learning steps 3751\n",
      "Episode  245 : score 166.0  avg score 431.7  timesteps 75187  learning steps 3759\n",
      "Episode  246 : score 202.0  avg score 431.7  timesteps 75389  learning steps 3769\n",
      "Episode  247 : score 198.0  avg score 431.9  timesteps 75587  learning steps 3779\n",
      "Episode  248 : score 244.0  avg score 432.1  timesteps 75831  learning steps 3791\n",
      "Episode  249 : score 243.0  avg score 432.1  timesteps 76074  learning steps 3803\n",
      "...saving models...\n",
      "Episode  250 : score 695.0  avg score 434.0  timesteps 76769  learning steps 3838\n",
      "...saving models...\n",
      "Episode  251 : score 4486.0  avg score 477.3  timesteps 81255  learning steps 4062\n",
      "...saving models...\n",
      "Episode  252 : score 490.0  avg score 480.4  timesteps 81745  learning steps 4087\n",
      "...saving models...\n",
      "Episode  253 : score 2826.0  avg score 505.8  timesteps 84571  learning steps 4228\n",
      "...saving models...\n",
      "Episode  254 : score 377.0  avg score 507.1  timesteps 84948  learning steps 4247\n",
      "...saving models...\n",
      "Episode  255 : score 789.0  avg score 512.4  timesteps 85737  learning steps 4286\n",
      "...saving models...\n",
      "Episode  256 : score 515.0  avg score 515.1  timesteps 86252  learning steps 4312\n",
      "...saving models...\n",
      "Episode  257 : score 385.0  avg score 516.9  timesteps 86637  learning steps 4331\n",
      "...saving models...\n",
      "Episode  258 : score 306.0  avg score 517.9  timesteps 86943  learning steps 4347\n",
      "Episode  259 : score 373.0  avg score 516.5  timesteps 87316  learning steps 4365\n",
      "...saving models...\n",
      "Episode  260 : score 770.0  avg score 522.7  timesteps 88086  learning steps 4404\n",
      "Episode  261 : score 460.0  avg score 522.4  timesteps 88546  learning steps 4427\n",
      "...saving models...\n",
      "Episode  262 : score 519.0  avg score 525.4  timesteps 89065  learning steps 4453\n",
      "...saving models...\n",
      "Episode  263 : score 568.0  avg score 527.9  timesteps 89633  learning steps 4481\n",
      "...saving models...\n",
      "Episode  264 : score 1663.0  avg score 540.8  timesteps 91296  learning steps 4564\n",
      "...saving models...\n",
      "Episode  265 : score 1898.0  avg score 556.3  timesteps 93194  learning steps 4659\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def plot_learning_curves(timesteps, scores, figure_file):\n",
    "    os.makedirs(os.path.dirname(figure_file), exist_ok=True)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    cumulative_avg = np.cumsum(scores) / (np.arange(len(scores)) + 1)  # Compute the cumulative average\n",
    "\n",
    "    ax.plot(timesteps, scores, label='Reward per Episode', alpha=0.3)  # Plot raw scores\n",
    "    ax.plot(timesteps, cumulative_avg, label='Cumulative Average', color='red')  # Plot cumulative average\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.legend()\n",
    "    plt.title('Training Curve')\n",
    "    plt.savefig(figure_file)\n",
    "    plt.show()\n",
    "\n",
    "def plot_curve_smooth(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)\n",
    "\n",
    "def plot_additional_metrics(episodes, actor_losses, critic_losses, values, figure_file_prefix):\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(5, 10))\n",
    "\n",
    "    # Ensure 'episodes' is a list or array of the right size\n",
    "    episodes = list(episodes) if len(episodes) == len(actor_losses) else list(range(len(actor_losses)))\n",
    "\n",
    "    # Actor Loss\n",
    "    axs[0].plot(episodes, actor_losses, label='Actor Loss', color='blue')\n",
    "    axs[0].set_title('Actor Loss Over Time')\n",
    "    axs[0].set_xlabel('Episode')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Critic Loss\n",
    "    axs[1].plot(episodes, critic_losses, label='Critic Loss', color='green')\n",
    "    axs[1].set_title('Critic Loss Over Time')\n",
    "    axs[1].set_xlabel('Episode')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Value Estimates\n",
    "    axs[2].plot(episodes, values, label='Value Estimates', color='red')\n",
    "    axs[2].set_title('Value Estimates Over Time')\n",
    "    axs[2].set_xlabel('Episode')\n",
    "    axs[2].set_ylabel('Value')\n",
    "    axs[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{figure_file_prefix}_additional_metrics.png\")\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_hyperparameters(env, agent_params, n_games=100):\n",
    "    agent = PPOAgent(**agent_params)\n",
    "    total_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _, _ = agent.choose_action(observation)\n",
    "            observation, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    return avg_reward\n",
    "\n",
    "def hyperparameter_tuning():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    learning_rates = [0.001, 0.0005, 0.0001, 0.00001, 0.000001]\n",
    "    gammas = [0.99, 0.95, 0.90]\n",
    "    gae_lambdas = [0.95, 0.97, 0.99]\n",
    "\n",
    "    best_avg_reward = -np.inf\n",
    "    # Initialize best_params with a default configuration\n",
    "    best_params = {\n",
    "        'n_actions': env.action_space.n,\n",
    "        'input_dims': env.observation_space.shape,\n",
    "        'alpha': 0.0003,  # Default learning rate\n",
    "        'gamma': 0.99,    # Default discount factor\n",
    "        'gae_lambda': 0.95,  # Default GAE lambda\n",
    "        'policy_clip': 0.2,\n",
    "        'batch_size': 5,\n",
    "        'N': 2048,\n",
    "        'n_epochs': 4\n",
    "    }\n",
    "\n",
    "    for alpha in learning_rates:\n",
    "        for gamma in gammas:\n",
    "            for gae_lambda in gae_lambdas:\n",
    "                agent_params = {\n",
    "                    'n_actions': env.action_space.n,\n",
    "                    'input_dims': env.observation_space.shape,\n",
    "                    'alpha': alpha,\n",
    "                    'gamma': gamma,\n",
    "                    'gae_lambda': gae_lambda,\n",
    "                    'policy_clip': 0.2,\n",
    "                    'batch_size': 5,\n",
    "                    'N': 2048,\n",
    "                    'n_epochs': 4\n",
    "                }\n",
    "                avg_reward = evaluate_hyperparameters(env, agent_params)\n",
    "                print(f'Tested {agent_params} -> Avg Reward: {avg_reward}')\n",
    "                \n",
    "                if avg_reward > best_avg_reward:\n",
    "                    best_avg_reward = avg_reward\n",
    "                    best_params = agent_params\n",
    "\n",
    "    print(f\"Best Hyperparameters: {best_params} with Average Reward: {best_avg_reward}\")\n",
    "    return best_params\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('CartPole-v1')\n",
    "    best_params = hyperparameter_tuning()\n",
    "\n",
    "    # Create the agent with the best hyperparameters\n",
    "    agent = PPOAgent(**best_params)\n",
    "    n_games = 1000\n",
    "\n",
    "    # Directory for saving plots and model checkpoints\n",
    "    plot_dir = './logs/ppo/plots'\n",
    "    checkpoint_dir = './logs/ppo/checkpoints'\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    figure_file = os.path.join(plot_dir, 'cartpole.png')\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    for i in range(n_games):\n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            observation_, reward, done, info, _ = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "            if n_steps % N == 0:\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_models()\n",
    "\n",
    "        print(\"Episode \", i, \": score %.1f\" % score, \" avg score %.1f\" % avg_score,\\\n",
    "                \" timesteps\", n_steps, \" learning steps\", learn_iters)\n",
    "    x = [i+1 for i in range(len(score_history))]\n",
    "    plot_curve_smooth(x, score_history, figure_file)\n",
    "    episodes = range(1, n_games + 1)\n",
    "    plot_additional_metrics(episodes, agent.actor_losses, agent.critic_losses, agent.values, plot_dir)\n",
    "\n",
    "    #report average values per however many runs\n",
    "\n",
    "    # changed to LeakyReLU for cases of negative input\n",
    "    # added entropy bonus to avoid agent converging too early, rewards more exploration - saw much better initial results\n",
    "    # added hp tuning to ensure best params - as model seemed quite sensitive\n",
    "    # add some information about CATASTROPHE FORGETTING in report. PPO is very susceptible and converges on bad policies often"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
