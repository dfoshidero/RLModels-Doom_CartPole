{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8103f60",
   "metadata": {},
   "source": [
    "# PPO Model for CartPole Environment\n",
    "\n",
    "**Please expand the cells to view the code!**\n",
    "\n",
    "### Description\n",
    "This notebook implements a Proximal Policy Optimization (PPO) model for the CartPole environment within OpenAI gym using PyTorch, with a rendered interface.\n",
    "\n",
    "Modify parameters in the hyperparameter tuning section to explore different configurations. Adding more parameters will exponentially increase runtime as it tests each combination to find the best weighted average.\n",
    "\n",
    "### Key Components:\n",
    "- **Dual Actor-Critic Network**: The actor proposes actions, and the critic evaluates them, facilitating a robust decision-making process.\n",
    "- **PPO Agent**: Implements efficient policy learning through clipped objective functions, which help prevent destabilizing large updates.\n",
    "- **Hyperparameter Tuning**: Employs a systematic search over a grid of potential values to optimize the model's performance by maximizing average rewards.\n",
    "\n",
    "Naturally, the PPO model is very customisable - with small changes making major difference in the model output.\n",
    "\n",
    "### How to Run:\n",
    "1. **Setup**: Ensure all required libraries are installed and the CartPole gym environment is properly configured. This was developed in **Python 3.10.14**.\n",
    "2. **Parameter Tuning**: Adjust the parameters in the hyperparameter tuning section as needed.\n",
    "3. **Execution**: Run the cells sequentially to train the model. Monitor the output for performance metrics and visualizations.\n",
    "\n",
    "### References:\n",
    "\n",
    "bentrevett, 2022. pytorch-rl/5 - Proximal Policy Optimization (PPO) [CartPole].ipynb at master · bentrevett/pytorch-rl [Online]. GitHub. Available from: https://github.com/bentrevett/pytorch-rl/blob/master/5%20-%20Proximal%20Policy%20Optimization%20(PPO)%20%5BCartPole%5D.ipynb [Accessed 7 May 2024].\n",
    "\n",
    "Chrysovergis, I., 2021. Keras documentation: Proximal Policy Optimization [Online]. keras.io. Available from: https://keras.io/examples/rl/ppo_cartpole/ [Accessed 7 May 2024].\n",
    "\n",
    "Computer Science Stack Exchange, 2017. Why do we use the log in gradient-based reinforcement algorithms? [Online]. Available from: https://cs.stackexchange.com/questions/70518/why-do-we-use-the-log-in-gradient-based-reinforcement-algorithms [Accessed 7 May 2024].\n",
    "\n",
    "Machine Learning with Phil, 2020. Proximal Policy Optimization (PPO) is Easy With PyTorch | Full PPO Tutorial [Online]. www.youtube.com. Available from: https://www.youtube.com/watch?v=hlv79rcHws0 [Accessed 7 May 2024].\n",
    "\n",
    "Mansar, Y., 2020. Learning to Play CartPole and LunarLander with Proximal Policy Optimization [Online]. Medium. Available from: https://towardsdatascience.com/learning-to-play-cartpole-and-lunarlander-with-proximal-policy-optimization-dacbd6045417 [Accessed 7 May 2024].\n",
    "\n",
    "Schöpf, P., Auddy, S., Hollenstein, J. and Rodriguez-sanchez, A., 2022. Hypernetwork-PPO for Continual Reinforcement Learning [Online]. openreview.net. Available from: https://openreview.net/forum?id=s9wY71poI25 [Accessed 7 May 2024].\n",
    "\n",
    "Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O., 2017. Proximal Policy Optimization Algorithms [Online]. arXiv.org. Available from: https://doi.org/10.48550/arXiv.1707.06347.\n",
    "\n",
    "Stable Baselines3, n.d. PPO — Stable Baselines3 1.4.1a3 documentation [Online]. stable-baselines3.readthedocs.io. Available from: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html.\n",
    "\n",
    "Tabor, P., 2020. Youtube-Code-Repository/ReinforcementLearning/PolicyGradient/PPO/torch/main.py at master · philtabor/Youtube-Code-Repository [Online]. GitHub. Available from: https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/PolicyGradient/PPO/torch/main.py [Accessed 7 May 2024].\n",
    "\n",
    "trtd56, 2022. trtd56/ppo-CartPole · Hugging Face [Online]. huggingface.co. Available from: https://huggingface.co/trtd56/ppo-CartPole [Accessed 7 May 2024]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0547f-535d-49a1-a0cb-2df24c584a95",
   "metadata": {},
   "source": [
    "## Initialize VizDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cddcc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary\n",
    "!pip install vizdoom\n",
    "!pip install opencv-python\n",
    "!pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip install gym\n",
    "\n",
    "# also need to install pytorch-cpu on anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34143102-2b66-4ee2-9f76-f6db917d2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import gym\n",
    "import json\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.distributions.categorical import Categorical\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48eacd6-a69d-4d14-b890-83f76c4a5e67",
   "metadata": {},
   "source": [
    "## Custom PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39228483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, input_dims, alpha,\n",
    "            fc1_dims=256, fc2_dims=256, chkpt_dir='tmp/ppo'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        self.actor = nn.Sequential(\n",
    "                nn.Linear(*input_dims, fc1_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc1_dims, fc2_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc2_dims, n_actions),\n",
    "                nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        \n",
    "        return dist\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\n",
    "            chkpt_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dims, fc1_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc1_dims, fc2_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc2_dims, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, n_actions, input_dims, gamma, alpha, gae_lambda,\n",
    "                 policy_clip, batch_size, N, n_epochs, entropy_coefficient):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
    "        self.critic = CriticNetwork(input_dims, alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "       \n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('... saving models ...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "\n",
    "        dist = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(dones_arr[k])) - values[k])\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                #prob_ratio = (new_probs - old_probs).exp()\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e7b4f",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d518ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hyperparameters(env, agent_params, n_games=100):\n",
    "    agent = PPOAgent(**agent_params)\n",
    "    total_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _, _ = agent.choose_action(observation)\n",
    "            observation, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    return avg_reward\n",
    "\n",
    "def hyperparameter_tuning():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    #env = VizDoomGym(render=False)\n",
    "    learning_rates = [0.0005, 0.0003, 0.0001]\n",
    "    gammas = [0.99]\n",
    "    policy_clips = [0.1, 0.2]\n",
    "    entropy_coefficients = [0.00001]\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for entropy_coeff in entropy_coefficients:\n",
    "        for alpha in learning_rates:\n",
    "            for gamma in gammas:\n",
    "                for policy_clip in policy_clips:\n",
    "                    agent_params = {\n",
    "                        'n_actions': env.action_space.n,\n",
    "                        'input_dims': env.observation_space.shape,\n",
    "                        'alpha': alpha,\n",
    "                        'gamma': gamma,\n",
    "                        'gae_lambda': 0.95,\n",
    "                        'policy_clip': policy_clip,\n",
    "                        'batch_size': 64,\n",
    "                        'N': 20,\n",
    "                        'n_epochs': 4,\n",
    "                        'entropy_coefficient': entropy_coeff\n",
    "                    }\n",
    "                    avg_reward = evaluate_hyperparameters(env, agent_params)\n",
    "                    results.append((avg_reward, agent_params))\n",
    "                    print(f'Tested {agent_params} -> Avg Reward: {avg_reward}')\n",
    "    \n",
    "    # Normalize rewards to sum to 1 to use as weights\n",
    "    total_reward = sum([result[0] for result in results])\n",
    "    weights = [result[0] / total_reward for result in results]\n",
    "\n",
    "    # Weighted average of parameters\n",
    "    avg_params = {}\n",
    "    for key in results[0][1].keys():\n",
    "        param_values = [params[key] for _, params in results]\n",
    "        if isinstance(param_values[0], float):  # Check if the parameter is float\n",
    "            avg_params[key] = sum(weight * params[key] for weight, (_, params) in zip(weights, results))\n",
    "        elif isinstance(param_values[0], int):  # Check if the parameter is integer\n",
    "            # Use weighted average and round it to get an integer\n",
    "            weighted_sum = sum(weight * params[key] for weight, (_, params) in zip(weights, results))\n",
    "            avg_params[key] = round(weighted_sum)\n",
    "        else:\n",
    "            # For non-numeric parameters, take the value from the best-performing configuration\n",
    "            avg_params[key] = results[0][1][key]  # Assumes results is sorted by performance, best first\n",
    "\n",
    "    print(f\"Weighted Average of Best Hyperparameters: {avg_params}\")\n",
    "    return avg_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7664881-22e2-42a6-8ab9-693188f1d2c3",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main\n",
    "\n",
    "########################################################################################################################################################\n",
    "# PLOTTING FUNCS\n",
    "def plot_curve_smooth(x, scores, figure_file):\n",
    "    os.makedirs(os.path.dirname(figure_file), exist_ok=True)\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "\n",
    "    fig, ax = plt.subplots()  # Using subplots for consistency\n",
    "    ax.plot(x, running_avg, label='Running Average')\n",
    "    ax.set_xlabel('Episode')  # Align the x label\n",
    "    ax.set_ylabel('Reward')  # Align the y label\n",
    "    ax.legend()\n",
    "    plt.title('Running Average of Previous 100 Scores')\n",
    "    plt.savefig(figure_file)\n",
    "    plt.show()\n",
    "\n",
    "def smooth_curve(data, window=100):\n",
    "    \"\"\"Calculate the running average over a fixed window.\"\"\"\n",
    "    running_avg = np.zeros(len(data))\n",
    "    for i in range(len(data)):\n",
    "        running_avg[i] = np.mean(data[max(0, i-window):(i+1)])\n",
    "    return running_avg\n",
    "\n",
    "def plot_metrics(checkpoint_timesteps, checkpoint_avg_lengths, checkpoint_avg_rewards):\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(10, 12))\n",
    "\n",
    "    # Ensure 'episodes' is a list or array of the right size\n",
    "    episodemain = list(range(1, len(checkpoint_avg_lengths) + 1))\n",
    "\n",
    "    # Plot Episode Length and Reward\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_ylabel('Mean Episode Length', color=color)\n",
    "    ax1.set_xlabel('Timesteps')\n",
    "    ax1.set_title('Mean Reward per Timestep')\n",
    "    ax1.plot(checkpoint_timesteps, checkpoint_avg_lengths, color=color, label='Episode Length')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Mean Reward per Episode', color=color)\n",
    "    ax2.plot(checkpoint_timesteps, checkpoint_avg_rewards, color=color, label='Episode Reward')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Save the plot with batch name\n",
    "    plot_save_path = os.path.join(save_dir, f\"plot_batch {total_batches}_metrics.png\")\n",
    "    #plt.savefig(plot_save_path)\n",
    "    plt.show()\n",
    "\n",
    "########################################################################################################################################################\n",
    "# MAIN\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Initialize environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    #env = VizDoomGym(render=True)\n",
    "    best_params = hyperparameter_tuning()\n",
    "\n",
    "    # Create the agent with the best hyperparameters\n",
    "    agent = PPOAgent(**best_params)\n",
    "    num_timesteps = 200000\n",
    "\n",
    "    # Directory for saving plots and model checkpoints\n",
    "    save_dir=\"E:\\RLModelTraining\\CartPole_PPO_Logs\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    log_path = os.path.join(save_dir, \"batch_64.txt\")\n",
    "\n",
    "    # Open the log file at the beginning of the script\n",
    "    out_f = open(log_path, 'w')\n",
    "\n",
    "    # Agent loop params\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    N = best_params.get('N', 16)\n",
    "\n",
    "    total_timesteps = 0\n",
    "    total_batches = 0\n",
    "    episode_lengths = []\n",
    "    episode_rewards = []\n",
    "    \n",
    "    checkpoint_timesteps = []\n",
    "    checkpoint_avg_rewards = []\n",
    "    checkpoint_avg_lengths = []\n",
    "\n",
    "    # MAIN LOOP until total timesteps\n",
    "    while total_timesteps < num_timesteps:\n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        total_reward = 0\n",
    "        episode_length = 0\n",
    "        \n",
    "        # INNER LOOP until episode complete\n",
    "        while not done and total_timesteps < num_timesteps:\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            observation_, reward, done, info, _ = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "\n",
    "            total_reward += reward\n",
    "            episode_length += 1\n",
    "            total_timesteps += 1\n",
    "            \n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "\n",
    "            if n_steps % N == 0:\n",
    "\n",
    "                total_batches += 1\n",
    "                # Calculate running averages of episode length and reward\n",
    "                avg_episode_lengths = [np.mean(episode_lengths)]\n",
    "                avg_episode_rewards = [np.mean(episode_rewards)]\n",
    "\n",
    "                checkpoint_timesteps.append(total_timesteps)\n",
    "                checkpoint_avg_rewards.append(avg_episode_rewards[-1])\n",
    "                checkpoint_avg_lengths.append(avg_episode_lengths[-1])\n",
    "                \n",
    "                n_games = len(episode_rewards)\n",
    "                episodes = range(1, n_games + 1)\n",
    "\n",
    "                # Plotting\n",
    "                \n",
    "                print(f\"Plot saved at batch {total_batches} to {save_dir}\")\n",
    "                print(f\"Plot saved at batch {total_batches} to {save_dir}\")\n",
    "                print(f\"Batch {total_batches}...\")\n",
    "                if episode_lengths and episode_rewards:  # Checks if lists are not empty\n",
    "                    print(f\"Episode {len(episode_lengths)}: Length = {episode_lengths[-1]}, Reward = {episode_rewards[-1]}\")\n",
    "                else:\n",
    "                    print(\"No episodes have completed yet.\")\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                plot_metrics(checkpoint_timesteps, checkpoint_avg_lengths, checkpoint_avg_rewards)\n",
    "                \n",
    "\n",
    "                \n",
    "                # Agent Learns\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation_\n",
    "\n",
    "        episode_lengths.append(episode_length)\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        n_games = len(episode_rewards)\n",
    "\n",
    "        # Write to log\n",
    "        log_data = {\n",
    "            'episode': n_games,\n",
    "            'reward': score,\n",
    "            'total_steps': total_timesteps,\n",
    "            'length': episode_length,\n",
    "            'avg_score_last_100': avg_score,\n",
    "            # Include other metrics if necessary\n",
    "        }\n",
    "        out_f.write(json.dumps(log_data) + '\\n')\n",
    "        out_f.flush()  # Ensure writing to file after each episode\n",
    "        print(f\"Episode {n_games} logged.\")\n",
    "\n",
    "        # SAVE MODEL (to checkpoint file each time agent scores better)    \n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_models()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
