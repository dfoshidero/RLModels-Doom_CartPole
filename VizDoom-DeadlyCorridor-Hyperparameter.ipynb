{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390adbc2",
   "metadata": {},
   "source": [
    "## 1. Getting VizDoom up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fbd6a2-79b9-4fdb-b138-01ba25aafc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "# Import os to deal with filepaths\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec3301f-9627-44c9-abbe-ff3605e68762",
   "metadata": {},
   "source": [
    "## 2. Converting it to a Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae853175-8aca-4ab6-ac5f-5b0e8de82789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2\n",
    "\n",
    "# Import environment checker\n",
    "# Discrete(3).sample() returns a number from 0, 1, 2 -> used as index to select action\n",
    "# Box(low=0, high=10, shape=(10,10)).sample() -> getting 10x10 array with low=0 and high=10\n",
    "import optuna\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "\n",
    "LEVEL = 'deadly_corridor'\n",
    "DOOM_SKILL = 's1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231c158-3c58-4a15-9237-1526e13565b1",
   "metadata": {},
   "source": [
    "### Environment configuration for Reward Shaping \n",
    "#### Additional game variables needed for this level:\n",
    "- DAMAGE_TAKEN (-)\n",
    "- DAMAGECOUNT (+)\n",
    "- SELECTED_WEAPON_AMMO (-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86fd0b97-dfae-4ad4-98ab-7268c8db3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False, config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg'):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(config)\n",
    "        \n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(7)\n",
    "\n",
    "        # Game variables: HEALTH DAMAGE_TAKEN DAMAGECOUNT SELECTED_WEAPON_AMMO \n",
    "        ## We want the change in these variable values, rather than the PiT values\n",
    "        self.damage_taken = 0\n",
    "        self.hitcount = 0\n",
    "        self.ammo = 52\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(7, dtype=np.uint8)\n",
    "        # Movement rewards encapsulates predefined reward in the environment config\n",
    "        movement_reward = self.game.make_action(actions[action], 4) # get action using index -> left, right, shoot\n",
    "\n",
    "        reward = 0\n",
    "        # Get all the other stuff we need to return \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            # ammo = self.game.get_state().game_variables[0] \n",
    "\n",
    "            # Reward shaping\n",
    "            game_variables = self.game.get_state().game_variables # get current PiT game variables\n",
    "            health, damage_taken, hitcount, ammo = game_variables # unpack\n",
    "\n",
    "            # calculate change in damage_taken, hitcount, ammo\n",
    "            damage_taken_delta = -damage_taken + self.damage_taken # disincentivizng us to take damage\n",
    "            self.damage_taken = damage_taken\n",
    "            hitcount_delta = hitcount - self.hitcount # increments by +1: incentivizing more hitcounts (1 hitcount = 1 reward)\n",
    "            self.hitcount = hitcount\n",
    "            ammo_delta = ammo - self.ammo # increments by -1: disincentiving us to take shots that miss\n",
    "                                          # hitcount and ammo will cancel each other out\n",
    "            self.ammo = ammo\n",
    "\n",
    "            # Pack everything into reward function (tuned weights)\n",
    "            reward = movement_reward + damage_taken_delta*10 + hitcount_delta*200 + ammo_delta*5\n",
    "            \n",
    "            info = ammo\n",
    "        # If we dont have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "            \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer  # Apply Grayscale\n",
    "\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18351768-7387-4813-99d0-a2faea4dc26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d7861e-c8cc-4b61-8962-717cbfbb1ff4",
   "metadata": {},
   "source": [
    "Environment checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6ac7a-9294-40a3-82d5-cc9d2f6f9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a3ba49-76d5-4c4a-817b-7f0dbf3ccc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcdbc87-f1b5-49fa-b9ae-590e596ed672",
   "metadata": {},
   "source": [
    "## 4. Optuna optimisation framework for HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a50c38-fc94-469a-b364-dc181a74a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/log_corridor_hpo'\n",
    "OPT_DIR = './opt/opt_corridor_hpo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1e13e31-7ad5-4df4-93d6-35d742fd51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return test hyperparameters - define the objective function\n",
    "def optimise_ppo(trial):\n",
    "    return {\n",
    "        'n_steps': trial.suggest_int('n_steps', 2048, 8192),\n",
    "        'gamma': trial.suggest_float('gamma', 0.8, 0.9999, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-4, log=True),\n",
    "        'clip_range': trial.suggest_float('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda': trial.suggest_float('gae_lambda', 0.8, 0.99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac2fa02d-400f-4c0b-91b6-a642681b083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a training loop and return mean reward\n",
    "def optimise_agent(trial):\n",
    "    try:\n",
    "        model_params = optimise_ppo(trial)\n",
    "        model_params['n_steps'] = round(model_params['n_steps']/64) * 64\n",
    "        \n",
    "        # Create environment \n",
    "        env = VizDoomGym()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "        # Create algo \n",
    "        model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        # model.learn(total_timesteps=300)\n",
    "        model.learn(total_timesteps=100000)\n",
    "\n",
    "        # Evaluate model \n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=20)\n",
    "        env.close()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "        \n",
    "    except Exception as e:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6751cc01-6b31-4e60-86e1-02dc3a3ac6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-02 15:28:26,994] A new study created in memory with name: no-name-96cebd0c-2c3a-4697-9b04-fd779123c4e4\n",
      "/Users/csqh/anaconda3/envs/pythonlab/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:283: UserWarning: Path 'opt/opt_corridor_hpo' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n",
      "[I 2024-04-04 03:56:56,190] Trial 4 finished with value: -15.986538599999994 and parameters: {'n_steps': 2823, 'gamma': 0.9631059078391256, 'learning_rate': 3.2252073369692905e-05, 'clip_range': 0.23934589909770057, 'gae_lambda': 0.8321502792041914}. Best is trial 4 with value: -15.986538599999994.\n",
      "[I 2024-04-04 04:31:40,298] Trial 6 finished with value: -40.23874135 and parameters: {'n_steps': 3236, 'gamma': 0.8086597453736779, 'learning_rate': 1.3308147900414879e-05, 'clip_range': 0.16327174237081, 'gae_lambda': 0.909335380740592}. Best is trial 4 with value: -15.986538599999994.\n",
      "[I 2024-04-04 05:07:42,333] Trial 18 finished with value: 378.8636208500001 and parameters: {'n_steps': 5049, 'gamma': 0.8206512736846115, 'learning_rate': 7.74205514669805e-05, 'clip_range': 0.20701537636628392, 'gae_lambda': 0.8206267863297668}. Best is trial 18 with value: 378.8636208500001.\n",
      "[I 2024-04-04 05:24:52,620] Trial 12 finished with value: -24.2354965 and parameters: {'n_steps': 5056, 'gamma': 0.8479093820754078, 'learning_rate': 8.060617533743699e-05, 'clip_range': 0.3250577802224607, 'gae_lambda': 0.9234717854891336}. Best is trial 18 with value: 378.8636208500001.\n",
      "[I 2024-04-04 05:26:27,529] Trial 7 finished with value: 470.7769744500001 and parameters: {'n_steps': 7810, 'gamma': 0.803830775548804, 'learning_rate': 1.2751656541707368e-05, 'clip_range': 0.1457834541079157, 'gae_lambda': 0.8968463695763083}. Best is trial 7 with value: 470.7769744500001.\n",
      "[I 2024-04-04 05:42:33,993] Trial 10 finished with value: 1475.0768409500001 and parameters: {'n_steps': 4587, 'gamma': 0.8576640427386206, 'learning_rate': 1.2810433220295355e-05, 'clip_range': 0.1779915656827156, 'gae_lambda': 0.9643582681985213}. Best is trial 10 with value: 1475.0768409500001.\n",
      "[I 2024-04-04 05:43:21,870] Trial 5 finished with value: -19.29241415 and parameters: {'n_steps': 4615, 'gamma': 0.8037664263009878, 'learning_rate': 6.758215205575612e-05, 'clip_range': 0.29147570334087014, 'gae_lambda': 0.8718284659253033}. Best is trial 10 with value: 1475.0768409500001.\n",
      "[I 2024-04-04 06:01:57,219] Trial 3 finished with value: 1407.71584175 and parameters: {'n_steps': 3360, 'gamma': 0.8366848598612198, 'learning_rate': 8.217197278097866e-05, 'clip_range': 0.28380492195686335, 'gae_lambda': 0.8728313945277979}. Best is trial 10 with value: 1475.0768409500001.\n",
      "[I 2024-04-04 06:02:35,008] Trial 17 finished with value: 1490.8202545499998 and parameters: {'n_steps': 6407, 'gamma': 0.9186560131909186, 'learning_rate': 8.816547473660592e-05, 'clip_range': 0.18401620536852387, 'gae_lambda': 0.8014353787139078}. Best is trial 17 with value: 1490.8202545499998.\n",
      "[I 2024-04-04 06:18:46,289] Trial 9 finished with value: -36.98772345000001 and parameters: {'n_steps': 7902, 'gamma': 0.9856611659427501, 'learning_rate': 2.1441740220738776e-05, 'clip_range': 0.3809661794074567, 'gae_lambda': 0.9029050479196744}. Best is trial 17 with value: 1490.8202545499998.\n",
      "[I 2024-04-04 06:36:28,401] Trial 13 finished with value: -22.996925349999998 and parameters: {'n_steps': 4316, 'gamma': 0.8994106381786618, 'learning_rate': 1.537702138960823e-05, 'clip_range': 0.36648416857902355, 'gae_lambda': 0.8470414533657769}. Best is trial 17 with value: 1490.8202545499998.\n",
      "[I 2024-04-04 06:36:33,470] Trial 0 finished with value: -52.99743585 and parameters: {'n_steps': 5104, 'gamma': 0.9511579964307424, 'learning_rate': 3.311654528457458e-05, 'clip_range': 0.30392384983289755, 'gae_lambda': 0.8009882729450571}. Best is trial 17 with value: 1490.8202545499998.\n"
     ]
    }
   ],
   "source": [
    "# Creating the experiment\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimise_agent, n_trials=100, n_jobs=20)\n",
    "# study.optimize(optimize_agent, n_trials=100, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d4022-5938-4b94-abc3-36e49e9d1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8503e-9e86-404e-9557-36db8c6cf6f2",
   "metadata": {},
   "source": [
    "## 5. Setup Callback\n",
    "Save model at different state of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a115d-fbab-449a-bd50-f19a8f3aac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for file nav\n",
    "import os\n",
    "# Import callback class from sb3\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e4204-553a-47eb-89d4-572b15f21eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        return True\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94544ad3-aea1-4559-95ab-8c0a30166e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_corridor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9d6be-2e1f-410f-9c40-e0e508cf86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of callback\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR) \n",
    "# after every 10000 steps of training the model, weights are saved for the pytorch agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0684619b-6997-4f27-9246-3eb05be026c9",
   "metadata": {},
   "source": [
    "## 5. Train Model using Curriculum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf5cea-9605-4739-97fa-719fe962cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PPO for training\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b07b6-5e9a-46d9-916c-7d918a5d4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment\n",
    "DOOM_SKILL = 's1'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d156e0-142b-41f2-a749-3b88a722d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = study.best_params\n",
    "model_params['n_steps'] = 7488\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb926b1-88b4-469e-973e-0d4d0f7b313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_steps: How many steps/frames the agent is going to take and store in the buffer \n",
    "# before run through training of actor and critique\n",
    "# ideally not too close to end of game (300) but somewhere close\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8fa60-2db2-4183-a544-42117c76eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04503f7-16e4-44e1-a4f2-90033b6a0428",
   "metadata": {},
   "source": [
    "#### Load saved best model and apply Curriculum Learning (S2 - S5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f717c-d6e4-4fa9-9bb9-5bdd4c713ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('./train/train_corridor/best_model_400000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e4298-ca04-43a6-9b18-612c656c0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment for S2\n",
    "DOOM_SKILL = 's2'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1219a1-f657-4513-a019-5c3f0a3972f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment for S3\n",
    "DOOM_SKILL = 's3'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55df27-e36c-43be-9254-ade956c1071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment for S4\n",
    "DOOM_SKILL = 's4'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e246b-a1c1-435e-a0cb-dfa0250f6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment for S5\n",
    "DOOM_SKILL = 's5'\n",
    "env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=400000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91761cd-2bce-4135-b448-769512a9ea2e",
   "metadata": {},
   "source": [
    "## 5. Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c2cdf8-4d70-48ae-a041-7a7e92a6d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import eval policy to test agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eef9c9-95cf-4f40-8f8e-a17e1be7c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model from disc\n",
    "model = PPO.load('./train/train_corridor/best_model_250000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d13640f-2645-4a61-a3fc-228f0ad5bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rendered envrironment\n",
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c402d-6861-465f-bc9a-3e8d5125ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate mean reward for 10 games\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052916fc-1094-46f9-b13c-67dfc6bb88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8eb004-51ee-4a3c-bd3d-8b2902d06525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for episode in range(5):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs) # Use model to predict what action to take\n",
    "        obs, reward, done, _, info = env.step(action) # take the predicted action\n",
    "        time.sleep(0.1)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(episode,total_reward))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424463e-7b5f-436f-ba78-d7190f4dd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d821e-e24d-4284-9a59-666ec5589cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
