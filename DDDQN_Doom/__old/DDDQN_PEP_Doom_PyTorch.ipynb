{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.compat.v1 as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "# from tensorflow.python.framework import ops\n",
    "import vizdoom as vzd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "import cv2\n",
    "\n",
    "import random                 # Handling random number generation\n",
    "import time                   # Handling time calculation\n",
    "from skimage import transform # Help us to preprocess the frames\n",
    "\n",
    "from collections import deque # Ordered collection with ends\n",
    "import matplotlib.pyplot as plt  # Display graphs\n",
    "\n",
    "import warnings                  # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [240,320,4]      # Our input is a stack of 4 frames hence 100x120x4 (Width, height, channels)\n",
    "action_size = 3          # 3 possible actions\n",
    "learning_rate =  0.00025      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 1000000         # Total episodes for training\n",
    "# max_steps = 100000            # Max possible steps in an episode\n",
    "batch_size = 128\n",
    "save_interval = 50\n",
    "# FIXED Q TARGETS HYPERPARAMETERS\n",
    "max_tau = 1000 #Tau is the C step where we update our target network\n",
    "clip_norm = 0.001\n",
    "\n",
    "# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.001            # minimum exploration probability\n",
    "decay_rate = 0.000005          # exponential decay rate for exploration prob\n",
    "\n",
    "# Q LEARNING hyperparameters\n",
    "gamma = 0.99               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "## If you have GPU change to 1million\n",
    "pretrain_length = 5000             # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 100000 ##100000                 # Number of experiences the Memory can keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_environment():\n",
    "    game = vzd.DoomGame()\n",
    "\n",
    "    # Load the correct configuration\n",
    "    # game.load_config('./VizDoom/scenarios/defend_the_center.cfg')\n",
    "\n",
    "    # Load the correct scenario (in our case defend_the_center)\n",
    "    # game.set_screen_format(vzd.ScreenFormat.RGB24)\n",
    "    # game.set_screen_resolution(vzd.ScreenResolution.RES_160X120)\n",
    "    game.set_depth_buffer_enabled(True) # depth buffer\n",
    "\n",
    "    game.set_labels_buffer_enabled(True) # labeling of game objects in labeling\n",
    "\n",
    "    game.set_automap_buffer_enabled(True) # Enables buffer with top down map of the current episode/level.\n",
    "\n",
    "    game.set_objects_info_enabled(True) # Information of all obects present in the current episode level \n",
    "\n",
    "    game.set_sectors_info_enabled(True) # Enables information about all sectors (map layout).\n",
    "\n",
    "    # Disable rendering options\n",
    "    game.set_render_hud(False)\n",
    "    game.set_render_minimal_hud(False)  \n",
    "    game.set_render_crosshair(False)\n",
    "    game.set_render_weapon(True)\n",
    "    game.set_render_decals(False)  \n",
    "    game.set_render_particles(False)\n",
    "    game.set_render_effects_sprites(False) \n",
    "    game.set_render_messages(False)  \n",
    "    game.set_render_corpses(False)\n",
    "    game.set_render_screen_flashes(True) \n",
    "\n",
    "    game.set_episode_start_time(1)\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "    game.set_doom_scenario_path('./VizDoom/scenarios/defend_the_center.wad')\n",
    "    game.set_doom_map(\"map01\")\n",
    "    game.set_available_game_variables([vzd.GameVariable.HEALTH, vzd.GameVariable.AMMO2])\n",
    "    game.set_available_buttons([vzd.Button.TURN_LEFT, vzd.Button.TURN_RIGHT, vzd.Button.ATTACK])\n",
    "\n",
    "    game.set_window_visible(False) #no pop out window\n",
    "    # game.init()\n",
    "\n",
    "    # Here we create an hot encoded version of our actions (3 possible actions)\n",
    "    # possible_actions = [[1, 0, 0, 0, 0], [0, 1, 0, 0, 0]...]\n",
    "    possible_actions = np.identity(3,dtype=int).tolist()\n",
    "\n",
    "    return game, possible_actions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    # Crop the screen (remove part that contains no information)\n",
    "    # [Up: Down, Left: right]\n",
    "    # # Check if the cropped frame has non-zero dimensions\n",
    "    # if cropped_frame.size == 0:\n",
    "    #     print(\"A\")\n",
    "    #     # If the cropped frame has zero dimensions, return a default frame with zeros\n",
    "    #     return np.zeros((100, 120), dtype=np.float32)\n",
    "    \n",
    "    # print(\"b\")\n",
    "    # Normalize Pixel Values\n",
    "    if frame.shape == (100, 120):\n",
    "        return frame\n",
    "    # print(\"frame to preprocess: \", frame.shape)\n",
    "    gray = cv2.cvtColor(np.moveaxis(frame, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "    resize = cv2.resize(gray, (120,100), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    normalized_frame = resize/255.0\n",
    "    #print(\"processed shape: \", normalized_frame.shape)\n",
    "\n",
    "    # plt.imshow(resize, cmap='gray')  # cmap='gray' for grayscale\n",
    "    # plt.axis('off')  # Turn off axis\n",
    "    # plt.show()\n",
    "    \n",
    "    return normalized_frame # 100x120x1 frame\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((100,120), dtype=np.float32) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "game, possible_actions = create_environment()\n",
    "\n",
    "stack_size = 4 \n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((100,120), dtype=np.float32) for i in range(stack_size)], maxlen=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQNNet(nn.Module):\n",
    "    def __init__(self, image_height, image_width, num_actions):\n",
    "        super().__init__()\n",
    "        h = image_height\n",
    "        w = image_width\n",
    "        self.c1 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=4)\n",
    "        h = h // 4\n",
    "        w = w // 4\n",
    "        self.c2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=4)\n",
    "        h = h // 4\n",
    "        w = w // 4\n",
    "        \n",
    "        self.fc_value = nn.Linear(h * w * 16, 512)\n",
    "        self.value = nn.Linear(512, 1)\n",
    "        \n",
    "        self.fc_advantage = nn.Linear(h * w * 16, 512)\n",
    "        self.advantage = nn.Linear(512, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.c2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        value = F.relu(self.fc_value(x))\n",
    "        value = self.value(value)\n",
    "        \n",
    "        advantage = F.relu(self.fc_advantage(x))\n",
    "        advantage = self.advantage(advantage)\n",
    "        \n",
    "        # Combining value and advantage streams to get output (Q values)\n",
    "        output = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        #print(\"network output shape: \", output.shape)  # tensor[1,3]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou: \n",
    "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "tree_index  0 0  0  We fill the leaves from left to right\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "        \n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "        \n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "        \n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propagate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            \n",
    "            \"\"\"\n",
    "            Here we want to access the line above\n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "            \n",
    "                0\n",
    "               / \\\n",
    "              1   2\n",
    "             / \\ / \\\n",
    "            3  4 5  [6] \n",
    "            \n",
    "            If we are in leaf at index 6, we updated the priority score\n",
    "            We need then to update index 2 node\n",
    "            So tree_index = (tree_index - 1) // 2\n",
    "            tree_index = (6-1)//2\n",
    "            tree_index = 2 (because // round the result)\n",
    "            \"\"\"\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for experiences\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True: # the while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            \n",
    "            else: # downward search, always search for a higher priority node\n",
    "                \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                    \n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "            \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree \n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        \n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        \n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "        \n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        \n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "    \n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "                                   \n",
    "            b_idx[i]= index\n",
    "            \n",
    "            experience = [data]\n",
    "            \n",
    "            memory_b.append(experience)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(memory_size)\n",
    "import numpy as np \n",
    "from skimage import transform\n",
    "\n",
    "# Render the environment\n",
    "game.init()\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        #print(\"raw state shape, \", state.shape) # [3, 240, 320]\n",
    "        #print(\"raw state shape to torch, \", torch.from_numpy(state).shape) # torch[[3, 240, 320]]\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True) ### All stacked frames are preprocessed\n",
    "    \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "\n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        #experience = np.hstack((state, [action, reward], next_state, done))\n",
    "        \n",
    "        experience = state, action, reward, next_state, done\n",
    "        #print(\"shape of a stack frame of dead: \", state.shape)\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        experience = state, action, reward, next_state, done\n",
    "        #print(\"shape of a stack frame still alive: \", state.shape)\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state\n",
    "        test_state = state\n",
    "\n",
    "game.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DQNetwork\n",
    "PolicyNetwork = DDDQNNet(100, 120, 3)\n",
    "\n",
    "# Instantiate the target network\n",
    "TargetNetwork = DDDQNNet(100, 120, 3)\n",
    "\n",
    "optimizer = optim.RMSprop(PolicyNetwork.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initilise stack size\n",
    "stack_size = 4\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((100,120), dtype=float) for i in range(stack_size)], maxlen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stacked_frame_to_torch_tensor(stacked_frame):\n",
    "    '''\n",
    "    input: [100, 120, 4]\n",
    "    output: torch[[4, 100, 120]]\n",
    "    '''\n",
    "    torch_tensor = torch.from_numpy(stacked_frame).float()\n",
    "    torch_tensor = torch_tensor.transpose(1, 2)\n",
    "    torch_tensor = torch_tensor.transpose(0, 1)\n",
    "    return torch_tensor.unsqueeze(0)\n",
    "\n",
    "\n",
    "def select_action(explore_start, explore_stop, decay_rate, decay_step, state):\n",
    "    \"\"\"\n",
    "    This function will do the part\n",
    "    With Ïµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\n",
    "    Input state is 4 stacked states. \n",
    "    \"\"\"\n",
    "\n",
    "    num = np.random.rand()\n",
    "\n",
    "    # Get explore probability\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "\n",
    "    if (num < explore_probability):\n",
    "        action = random.choice(possible_actions)\n",
    "        return action, explore_probability\n",
    "\n",
    "    # Greedy action\n",
    "    else:\n",
    "        max_index = PolicyNetwork(stacked_frame_to_torch_tensor(state)).argmax(dim=1).item()\n",
    "        action = possible_actions[max_index]\n",
    "        return action, explore_probability\n",
    "\n",
    "\n",
    "def update_target_params(PolicyNetwork, TargetNetwork):\n",
    "    \"\"\"\n",
    "    Copy parameters of the Policy network to Target network.\n",
    "    \"\"\"\n",
    "    TargetNetwork_state_dict = TargetNetwork.state_dict()\n",
    "    PolicyNetwork_state_dict = PolicyNetwork.state_dict()\n",
    "    for key in PolicyNetwork_state_dict:\n",
    "        TargetNetwork_state_dict[key] = PolicyNetwork_state_dict[key]\n",
    "        TargetNetwork.load_state_dict(TargetNetwork_state_dict)\n",
    "\n",
    "    return PolicyNetwork, TargetNetwork    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0     |      Reward: 0.0     |     length: 285      |      Loss: 0.07130824029445648\n",
      "======Model saved.======\n",
      "Episode 1     |      Reward: 0.0     |     length: 273      |      Loss: 0.04678485915064812\n",
      "Episode 2     |      Reward: 0.0     |     length: 249      |      Loss: 0.06714797765016556\n",
      "Episode 3     |      Reward: 1.0     |     length: 359      |      Loss: 0.040310099720954895\n",
      "Episode 4     |      Reward: 1.0     |     length: 311      |      Loss: 0.06556517630815506\n",
      "Episode 5     |      Reward: 2.0     |     length: 427      |      Loss: 0.07729307562112808\n",
      "Episode 6     |      Reward: 3.0     |     length: 439      |      Loss: 0.07204821705818176\n",
      "Episode 7     |      Reward: 0.0     |     length: 265      |      Loss: 0.07819285988807678\n",
      "Episode 8     |      Reward: 1.0     |     length: 293      |      Loss: 0.07179142534732819\n"
     ]
    },
    {
     "ename": "SignalException",
     "evalue": "Signal SIGINT received. ViZDoom instance has been closed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSignalException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m decay_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     31\u001b[0m action, explore_probability \u001b[38;5;241m=\u001b[39m select_action(explore_start, explore_stop, decay_rate, decay_step, state) \u001b[38;5;66;03m# the state input here is a stacked state already\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m done \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mis_episode_finished()\n\u001b[1;32m     35\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[0;31mSignalException\u001b[0m: Signal SIGINT received. ViZDoom instance has been closed."
     ]
    }
   ],
   "source": [
    "import vizdoom as vzd\n",
    "# Set decay step\n",
    "decay_step = 0\n",
    "# Set tau \n",
    "tau = 0\n",
    "# Set Target network params\n",
    "PolicyNetwork, TargetNetwork = update_target_params(PolicyNetwork, TargetNetwork)\n",
    "\n",
    "out_f = open(\"./log_PyTorch_DDDQN.txt\", 'w')\n",
    "\n",
    "# Initialise new game instance\n",
    "game, possible_actions = create_environment()\n",
    "game.init()\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "\n",
    "    episode_step = 0\n",
    "    episode_reward = 0\n",
    "    game.new_episode()\n",
    "\n",
    "    state = game.get_state().screen_buffer\n",
    "    # print(type(state))\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    # why stack_frames turned dimensions to 100X120\n",
    "    \n",
    "    while not game.is_episode_finished():\n",
    "\n",
    "        episode_step += 1\n",
    "        tau += 1\n",
    "        decay_step +=1\n",
    "        action, explore_probability = select_action(explore_start, explore_stop, decay_rate, decay_step, state) # the state input here is a stacked state already\n",
    "        \n",
    "        reward = game.make_action(action)\n",
    "        done = game.is_episode_finished()\n",
    "        episode_reward += reward\n",
    "\n",
    "        # if episode finished\n",
    "        if done: \n",
    "            next_state = np.zeros((100,120), dtype=float)\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "            # Add experience to memory\n",
    "            experience = state, action, reward, next_state, done\n",
    "            memory.store(experience)\n",
    "\n",
    "        else:\n",
    "            # Get the next state\n",
    "            next_state = game.get_state().screen_buffer\n",
    "            \n",
    "            # Stack the frame of the next_state (Note the output next_state is stacked)\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "    \n",
    "\n",
    "            # Add experience to memory\n",
    "            experience = state, action, reward, next_state, done\n",
    "            memory.store(experience)\n",
    "            \n",
    "            # st+1 is now our current state\n",
    "            state = next_state \n",
    "\n",
    "\n",
    "        ### LEARNING      \n",
    "        # Obtain random mini-batch from memory\n",
    "        tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "        \n",
    "        batch_states = torch.cat([stacked_frame_to_torch_tensor(each[0][0]) for each in batch])\n",
    "\n",
    "        batch_actions = torch.FloatTensor([each[0][1] for each in batch])\n",
    "        batch_rewards = torch.FloatTensor([each[0][2] for each in batch]).unsqueeze(1)\n",
    "        batch_next_states = torch.cat([stacked_frame_to_torch_tensor(each[0][3]) for each in batch] ) # stacked frames of np arrays \n",
    "        batch_dones = torch.FloatTensor([each[0][4] for each in batch]).unsqueeze(1)\n",
    "\n",
    "        actions_index = batch_actions.detach().numpy().flatten()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            policy_q_next = PolicyNetwork(batch_next_states)\n",
    "            target_q_next = TargetNetwork(batch_next_states)\n",
    "            online_max_action = torch.argmax(policy_q_next, dim=1, keepdim=True)\n",
    "            y = batch_rewards + (1 - batch_dones) * gamma * target_q_next.gather(1, online_max_action.long())\n",
    "\n",
    "\n",
    "        loss = F.mse_loss(PolicyNetwork(batch_states).gather(1, batch_actions.long()), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_value_(PolicyNetwork.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(PolicyNetwork.parameters(), clip_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        terminal_np = batch_dones.detach().numpy().flatten()\n",
    "        policy_q_next_np = policy_q_next.detach().numpy()\n",
    "        target_q_next_np = target_q_next.detach().numpy()\n",
    "        rewards_np = batch_rewards.detach().numpy().flatten()\n",
    "\n",
    "        target_qs_batch = []\n",
    "\n",
    "        for i in range(0, len(batch)):\n",
    "\n",
    "            terminal = terminal_np[i]\n",
    "            action = np.argmax(policy_q_next_np[i])\n",
    "\n",
    "            if terminal:\n",
    "                target_qs_batch.append(torch.tensor(rewards_np[i], dtype=torch.float32)) # rewards_mb[i] is a TENSOR\n",
    "\n",
    "            else: \n",
    "                target = rewards_np[i] + gamma * target_q_next_np[i][action]\n",
    "\n",
    "                target_qs_batch.append(torch.tensor(target, dtype=torch.float32))\n",
    "\n",
    "        ACTIONS = [torch.tensor(np.array([1,0,0]), dtype=torch.float32), torch.tensor(np.array([0,1,0]), dtype=torch.float32), torch.tensor(np.array([0,0, 1]), dtype=torch.float32)]\n",
    "\n",
    "        predicted_qs = [torch.sum(torch.mul(policy_q_next[i], ACTIONS[int(actions_index[i])])) for i in range(batch_size)]\n",
    "\n",
    "        absolute_errors = [torch.abs(torch.subtract(predicted_qs[i], target_qs_batch[i])) for i in range(batch_size)]\n",
    "        \n",
    "        absolute_errors_np = np.array([error.item() for error in absolute_errors])\n",
    "        # print(absolute_errors_np)\n",
    "        ISWeights_list = ISWeights_mb.tolist()\n",
    "        memory.batch_update(tree_idx, absolute_errors_np)\n",
    "\n",
    "        \n",
    "        if tau > max_tau:\n",
    "            PolicyNetwork, TargetNetwork = update_target_params(PolicyNetwork, TargetNetwork)\n",
    "            tau = 0\n",
    "            # print(\"Model updated\")\n",
    "\n",
    "    # Write batch stats to log files \n",
    "    loss_at_end_of_episode = torch.mean(torch.stack(absolute_errors), dim=0).item()\n",
    "    print(f\"Episode {episode}     |      Reward: {episode_reward}     |     length: {episode_step}      |      Loss: {loss_at_end_of_episode}\")      \n",
    "    \n",
    "    # Write file to log\n",
    "    out_f.write(json.dumps({\n",
    "        'episode': episode,\n",
    "        'reward': episode_reward,\n",
    "        'length': episode_step,\n",
    "        'loss': loss_at_end_of_episode\n",
    "    }) + '\\n')\n",
    "\n",
    "    out_f.flush()\n",
    "    \n",
    "    if episode % save_interval == 0:\n",
    "        torch.save(PolicyNetwork, f'./models/episode_{episode}.pt')\n",
    "        print(f\"======Model saved.======\")\n",
    "    # print(f\"Episode: {episode}   |    Reward: {episode_reward}   |    Length: {episode_step}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
