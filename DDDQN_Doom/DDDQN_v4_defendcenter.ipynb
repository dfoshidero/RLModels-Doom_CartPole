{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDDQN Model for Vizdoom Environment\n",
    "\n",
    "\n",
    "**Please expand the cells to view the code!**\n",
    "\n",
    "### Description\n",
    "This notebook implements a Double Deep Dueling Q-Learning (DDDQN) model for the Vizdoom environment using tensorflow.\n",
    "\n",
    "\n",
    "\n",
    "### Key Components:\n",
    "- **Prioritized Experience Replay**: Implementation of a sum tree structure for efficient sampling, promoting beneficial learning episodes.\n",
    "- **DDDQN Architecture**: Integrates dual streams for value and advantage functions (Dueling), along with convolutional layers to effectively process spatial information. Uses two streams that calculates V(s) and A(s,a) to update the q-values.\n",
    "- **Hyperparameter Tuning**: Modify the subsequent hyperparameters to enhance the performance:\n",
    "\n",
    "    - `learning_rate`: Determines how quickly the model learns from the training data.\n",
    "    -  `batch_size`: Adjusting this parameter can affect the stability and efficiency of the training process.\n",
    "    - `gamma`: The reward discount factor, which affects how important future rewards are to the agent when making decisions.\n",
    "    - `explore_start` and `explore_stop`: Adjusting these values can impact the agent's ability to discover optimal policies.\n",
    "    - `pretrain_length`: Number of experience prior to training.\n",
    "    - `memory_size`: Number of memories for experience replay.\n",
    "\n",
    "    \n",
    "- **Visualisations**: Set up pyplot to visualise the results.\n",
    "\n",
    "### How to Run:\n",
    "1. **Setup**: Ensure all required libraries and packages are installed, and the CartPole environment is properly configured. Modify the path for loading the CartPole environment and storing the model as needed. This was developed in **Python 3.10.14**.\n",
    "2. **Parameter Tuning**: Adjust the parameters in the 'Hyperparameters' section as needed.\n",
    "3. **Execution**: Run the cells sequentially to train the model. Monitor the output for performance metrics and visualisations.\n",
    "\n",
    "\n",
    "### References\n",
    "Seita, D., 2016. Frame Skipping and Pre-Processing for Deep Q-Networks on Atari 2600 Games [Online]. danieltakeshi.github.io. Available from: https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/.\n",
    "\n",
    "Simonini, T., 2018. An introduction to Deep Q-Learning: let‚Äôs play Doom [Online]. freeCodeCamp.org. Available from: https://www.freecodecamp.org/news/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8 [Accessed 7 May 2024].\n",
    "\n",
    "Simonini, T., 2018. Deep_reinforcement_learning_Course/Dueling Double DQN with PER and fixed-q targets/Dueling Deep Q Learning with Doom (+ double DQNs and Prioritized Experience Replay).ipynb at master ¬∑ simoninithomas/Deep_reinforcement_learning_Course [Online]. GitHub. Available from: https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/Dueling%20Deep%20Q%20Learning%20with%20Doom%20(%2B%20double%20DQNs%20and%20Prioritized%20Experience%20Replay).ipynb [Accessed 7 May 2024].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "\n",
    "    # Load the correct configuration\n",
    "    game.load_config('VizDoom/scenarios/defend_the_center.cfg')\n",
    "\n",
    "    # Load the correct scenario (in our case basic)\n",
    "    game.set_doom_scenario_path('VizDoom/scenarios/defend_the_center.wad')\n",
    "\n",
    "    game.set_window_visible(False) #no pop out window\n",
    "    game.init()\n",
    "\n",
    "    # Here we create an hot encoded version of our actions (3 possible actions)\n",
    "    possible_actions = np.identity(3,dtype=int).tolist()\n",
    "\n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "files_and_directories = os.listdir(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Crop the screen (remove part that contains no information)\n",
    "    # [Up: Down, Left: right]\n",
    "    cropped_frame = frame[15:-5, 20:-20]\n",
    "\n",
    "    # Check if the cropped frame has non-zero dimensions\n",
    "    if cropped_frame.size == 0:\n",
    "        # If the cropped frame has zero dimensions, return a default frame with zeros\n",
    "        return np.zeros((100, 120), dtype=np.float32)\n",
    "\n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame / 255.0\n",
    "\n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(cropped_frame, [100, 120])\n",
    "\n",
    "    return preprocessed_frame # 100x120x1 frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack_frames (Seita, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((100,120), dtype=int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    if state.size == 0:\n",
    "        # Return the existing stacked frames without modification\n",
    "        return np.stack(stacked_frames, axis=2), stacked_frames\n",
    "\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "\n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((100,120), dtype=int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
    "\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [100,120,4]      # Our input is a stack of 4 frames hence 100x120x4 (Width, height, channels)\n",
    "action_size = game.get_available_buttons_size()              # 3 possible actions\n",
    "learning_rate =  0.00025      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 1000000         # Total episodes for training\n",
    "total_timesteps = 500000     #Total timestesp for training\n",
    "max_steps = 2100             # Max possible steps in an episode\n",
    "batch_size = 64\n",
    "\n",
    "# FIXED Q TARGETS HYPERPARAMETERS\n",
    "max_tau = 10000 #Tau is the C step where we update our target network\n",
    "\n",
    "# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.001            # minimum exploration probability\n",
    "decay_rate = 0.00005            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q LEARNING hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "## If you have GPU change to 1million\n",
    "pretrain_length = 10             # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 10 ##100000                 # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = False\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Dueling Double Deep Q-learning Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQNNet:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "        random_var = 64\n",
    "        \n",
    "        \n",
    "        # We use tf.variable_scope here to know which network we're using (DQN or target_net)\n",
    "        # it will be useful when we will update our w- parameters (by copy the DQN parameters)\n",
    "        with tf.compat.v1.variable_scope(self.name):\n",
    "            \n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            # [None, 100, 120, 4]\n",
    "            self.inputs_ = tf.compat.v1.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            \n",
    "            #\n",
    "            self.ISWeights_ = tf.compat.v1.placeholder(tf.float32, [None,1], name='IS_weights')\n",
    "            \n",
    "            self.actions_ = tf.placeholder(tf.float32, [random_var, action_size], name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.compat.v1.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            # Input is 100x120x4\n",
    "            self.conv1 = Conv2D(\n",
    "                    filters=32,\n",
    "                    kernel_size=[8, 8],\n",
    "                    strides=[4, 4],\n",
    "                    padding=\"VALID\",\n",
    "                    kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "                    name=\"conv1\")(self.inputs_)\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Second convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv2 = Conv2D(\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "                                 name = \"conv2\")(self.conv1_out)\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\")\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Third convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv3 = Conv2D(\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "                                 name = \"conv3\")(self.conv2_out)\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
    "            \n",
    "            \n",
    "            self.flatten = Flatten(data_format='channels_last')(self.conv3)\n",
    "            \n",
    "            \n",
    "            ## Here we separate into two streams\n",
    "            # The one that calculate V(s)\n",
    "            self.value_fc = Dense(\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "                                name=\"value_fc\")(self.flatten)\n",
    "            \n",
    "            self.value = Dense(\n",
    "                                        units = 1,\n",
    "                                        activation = None,\n",
    "                                        kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "                                name=\"value\")(self.value_fc)\n",
    "            \n",
    "            # The one that calculate A(s,a)\n",
    "            self.advantage_fc = Dense(\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "                                name=\"advantage_fc\")(self.flatten)\n",
    "            \n",
    "            self.advantage = Dense(\n",
    "                                        units = self.action_size,\n",
    "                                        activation = None,\n",
    "                                        kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"),\n",
    "                                name=\"advantages\")(self.advantage_fc)\n",
    "            \n",
    "            # Agregating layer\n",
    "            # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "            self.output = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "              \n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            # The loss is modified because of PER \n",
    "            self.absolute_errors = tf.abs(self.target_Q - self.Q)# for updating Sumtree\n",
    "            \n",
    "            self.loss = tf.reduce_mean(self.ISWeights_ * tf.math.squared_difference(self.target_Q, self.Q))\n",
    "            \n",
    "            self.optimizer = tf.compat.v1.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"DQNetwork\")\n",
    "\n",
    "# Instantiate the target network\n",
    "TargetNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"TargetNetwork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou:\n",
    "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "\n",
    "    \"\"\"\n",
    "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
    "\n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "\n",
    "\n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "\n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "\n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "\n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "\n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propagate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "\n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        parent_index = 0\n",
    "\n",
    "        while True: # the while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "\n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "\n",
    "            else: # downward search, always search for a higher priority node\n",
    "\n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "\n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don't use deque anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "\n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "\n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree\n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "\n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "\n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "\n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "\n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "\n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "\n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "\n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "\n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "\n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "\n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "\n",
    "            b_idx[i]= index\n",
    "\n",
    "            experience = [data]\n",
    "\n",
    "            memory_b.append(experience)\n",
    "\n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "\n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dwaling withthe empty memory problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "\n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "\n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "\n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "\n",
    "        # Add experience to memory\n",
    "        #experience = np.hstack((state, [action, reward], next_state, done))\n",
    "\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "\n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "\n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "        # Add experience to memory\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "\n",
    "        # Our state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Set up Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"./tensorboard/dddqn/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train our Agent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With œµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "\n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "\n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "\n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "\n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps us to copy one set of variables to another\n",
    "# In our case we use it when we want to copy the parameters of DQN to Target_network\n",
    "# Thanks of the very good implementation of Arthur Juliani https://github.com/awjuliani\n",
    "def update_target_graph():\n",
    "\n",
    "    # Get the parameters of our DQNNetwork\n",
    "    from_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, \"DQNetwork\")\n",
    "\n",
    "    # Get the parameters of our Target_network\n",
    "    to_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, \"TargetNetwork\")\n",
    "\n",
    "    op_holder = []\n",
    "\n",
    "    # Update our target_network parameters with DQNNetwork parameters\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "avg_episode_lengths = 0\n",
    "avg_episode_rewards = 0\n",
    "acc_timesteps = 0\n",
    "episode_lengths = []\n",
    "eepisode_rewards = []\n",
    "\n",
    "plot_dir = './logs/dddqn/plots'\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "figure_file = os.path.join(plot_dir, 'test.png')\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "        \n",
    "        # Set tau = 0\n",
    "        tau = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "        \n",
    "        # Update the parameters of our TargetNetwork with DQN_weights\n",
    "        update_target = update_target_graph()\n",
    "        sess.run(update_target)\n",
    "        while acc_timesteps < total_timesteps:\n",
    "            for episode in range(total_episodes):\n",
    "                try:\n",
    "                    if acc_timesteps > total_timesteps:\n",
    "                        break\n",
    "                    # Set step to 0\n",
    "                    step = 0\n",
    "                    \n",
    "                    # Initialize the rewards of the episode\n",
    "                    episode_rewards = []\n",
    "                    \n",
    "                    # Make a new episode and observe the first state\n",
    "                    game.new_episode()\n",
    "                    \n",
    "                    state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    # Remember that stack frame function also call our preprocess function.\n",
    "                    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "                \n",
    "                    while step < max_steps:\n",
    "                        if acc_timesteps > total_timesteps:\n",
    "                            break\n",
    "                        step += 1\n",
    "                        acc_timesteps += 1\n",
    "                        \n",
    "                        # Increase the C step\n",
    "                        tau += 1\n",
    "                        \n",
    "                        # Increase decay_step\n",
    "                        decay_step +=1\n",
    "                        \n",
    "                        # With œµ select a random action atat, otherwise select a = argmaxQ(st,a)\n",
    "                        action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                        # Do the action\n",
    "                        reward = game.make_action(action)\n",
    "\n",
    "                        # Look if the episode is finished\n",
    "                        done = game.is_episode_finished()\n",
    "                        \n",
    "                        # Add the reward to total reward\n",
    "                        episode_rewards.append(reward)\n",
    "\n",
    "                        # If the game is finished\n",
    "                        if done:\n",
    "                            # the episode ends so no next state\n",
    "                            next_state = np.zeros((120,140), dtype=int)\n",
    "                            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                            savestep = step\n",
    "\n",
    "                            # Set step = max_steps to end the episode\n",
    "                            step = max_steps\n",
    "\n",
    "                            # Get the total reward of the episode\n",
    "                            total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                            print('Episode: {}'.format(episode), 'Step: {}'.format(savestep), 'Acc Step: {}'.format(acc_timesteps),\n",
    "                                    'Total reward: {}'.format(total_reward),\n",
    "                                    'Training loss: {:.4f}'.format(loss),\n",
    "                                    'Explore P: {:.4f}'.format(explore_probability))\n",
    "                            \n",
    "                            episode_lengths.append(savestep)\n",
    "                            eepisode_rewards.append(total_reward)\n",
    "                            # Add experience to memory\n",
    "                            experience = state, action, reward, next_state, done\n",
    "                            memory.store(experience)\n",
    "\n",
    "                        else:\n",
    "                            # Get the next state\n",
    "                            next_state = game.get_state().screen_buffer\n",
    "                            \n",
    "                            # Stack the frame of the next_state\n",
    "                            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                            \n",
    "\n",
    "                            # Add experience to memory\n",
    "                            experience = state, action, reward, next_state, done\n",
    "                            memory.store(experience)\n",
    "                            \n",
    "                            # st+1 is now our current state\n",
    "                            state = next_state\n",
    "\n",
    "\n",
    "                        ### LEARNING PART            \n",
    "                        # Obtain random mini-batch from memory\n",
    "                        tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "                        \n",
    "                        states_mb = np.array([each[0][0] for each in batch], ndmin=3)\n",
    "                        actions_mb = np.array([each[0][1] for each in batch])\n",
    "                        rewards_mb = np.array([each[0][2] for each in batch]) \n",
    "                        next_states_mb = np.array([each[0][3] for each in batch], ndmin=3)\n",
    "                        dones_mb = np.array([each[0][4] for each in batch])\n",
    "\n",
    "                        target_Qs_batch = []\n",
    "\n",
    "                        \n",
    "                        ### DOUBLE DQN Logic\n",
    "                        # Use DQNNetwork to select the action to take at next_state (a') (action with the highest Q-value)\n",
    "                        # Use TargetNetwork to calculate the Q_val of Q(s',a')\n",
    "                        \n",
    "                        # Get Q values for next_state \n",
    "                        q_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                        \n",
    "                        # Calculate Qtarget for all actions that state\n",
    "                        q_target_next_state = sess.run(TargetNetwork.output, feed_dict = {TargetNetwork.inputs_: next_states_mb})\n",
    "                        \n",
    "                        \n",
    "                        # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma * Qtarget(s',a') \n",
    "                        for i in range(0, len(batch)):\n",
    "                            terminal = dones_mb[i]\n",
    "                            \n",
    "                            # We got a'\n",
    "                            action = np.argmax(q_next_state[i])\n",
    "\n",
    "                            # If we are in a terminal state, only equals reward\n",
    "                            if terminal:\n",
    "                                target_Qs_batch.append(rewards_mb[i])\n",
    "                                \n",
    "                            else:\n",
    "                                # Take the Qtarget for action a'\n",
    "                                target = rewards_mb[i] + gamma * q_target_next_state[i][action]\n",
    "                                target_Qs_batch.append(target)\n",
    "                                \n",
    "\n",
    "                        targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                        \n",
    "                        _, loss, absolute_errors = sess.run([DQNetwork.optimizer, DQNetwork.loss, DQNetwork.absolute_errors],\n",
    "                                            feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                    DQNetwork.target_Q: targets_mb,\n",
    "                                                    DQNetwork.actions_: actions_mb,\n",
    "                                                    DQNetwork.ISWeights_: ISWeights_mb})\n",
    "                    \n",
    "                        \n",
    "                        \n",
    "                        # Update priority\n",
    "                        memory.batch_update(tree_idx, absolute_errors)\n",
    "                        \n",
    "                        \n",
    "                        # Write TF Summaries\n",
    "                        summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                            DQNetwork.target_Q: targets_mb,\n",
    "                                                            DQNetwork.actions_: actions_mb,\n",
    "                                                        DQNetwork.ISWeights_: ISWeights_mb})\n",
    "                        writer.add_summary(summary, episode)\n",
    "                        writer.flush()\n",
    "                        \n",
    "                        if tau > max_tau:\n",
    "                            # Update the parameters of our TargetNetwork with DQN_weights\n",
    "                            update_target = update_target_graph()\n",
    "                            sess.run(update_target)\n",
    "                            tau = 0\n",
    "                            print(\"Model updated\")\n",
    "                        \n",
    "                        # Save model every 16 timesteps (500 for demostrate purpose)\n",
    "                        if acc_timesteps % 16 == 0:\n",
    "                            avg_episode_lengths = [np.mean(episode_lengths[:i + 1]) for i in range(episode + 1)]\n",
    "                            avg_episode_rewards = [np.mean(eepisode_rewards[:i + 1]) for i in range(episode + 1)]\n",
    "                            save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "\n",
    "                            # Clear previous output\n",
    "                            clear_output(wait=True)\n",
    "                            \n",
    "                            # Plotting\n",
    "                            fig, ax1 = plt.subplots()\n",
    "\n",
    "                            ax1.set_xlabel('Number of Episodes')\n",
    "                            ax1.set_ylabel('Mean Episode Length', color='tab:blue')\n",
    "                            ax1.plot(range(1, episode + 2), avg_episode_lengths, color='tab:blue',label='Episode Length')\n",
    "                            ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "                            ax2 = ax1.twinx()\n",
    "                            ax2.set_ylabel('Mean Reward per Episode', color='tab:red')\n",
    "                            ax2.plot(range(1, episode + 2), avg_episode_rewards, color='tab:red',label='Episode Reward')\n",
    "                            ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "                            fig.tight_layout()\n",
    "                            ax1.legend(loc='upper right')\n",
    "                            ax2.legend(loc='upper left')\n",
    "                            plt.title('Training Curve')\n",
    "\n",
    "                            # Save the plot with a filename based on acc_timesteps\n",
    "                            save_filename = f\"performance_{acc_timesteps:06d}.png\"\n",
    "                            save_path = os.path.join(plot_dir, save_filename)\n",
    "                            plt.savefig(save_path)\n",
    "\n",
    "\n",
    "                            plt.show()\n",
    "                except tf.errors.InvalidArgumentError as e:\n",
    "                    print(\"An error occurred:\", e)\n",
    "                    print(\"Skipping the code and continuing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration (TESTING)\n",
    "    game.load_config('VizDoom/scenarios/defend_the_center.cfg')\n",
    "    \n",
    "    # Load the correct scenario (in our case deadly_corridor scenario)\n",
    "    game.set_doom_scenario_path('VizDoom/scenarios/defend_the_center.wad')\n",
    "    \n",
    "    game.init()    \n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "        while not game.is_episode_finished():\n",
    "            ## EPSILON GREEDY STRATEGY\n",
    "            # Choose action a from state s using epsilon greedy.\n",
    "            ## First we randomize a number\n",
    "            exp_exp_tradeoff = np.random.rand()\n",
    "            \n",
    "\n",
    "            explore_probability = 0.01\n",
    "    \n",
    "            if (explore_probability > exp_exp_tradeoff):\n",
    "                # Make a random action (exploration)\n",
    "                action = random.choice(possible_actions)\n",
    "        \n",
    "            else:\n",
    "                # Get action from Q-network (exploitation)\n",
    "                # Estimate the Qs values state\n",
    "                Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "                # Take the biggest Q value (= the best action)\n",
    "                choice = np.argmax(Qs)\n",
    "                action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "        \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "        \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    \n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
