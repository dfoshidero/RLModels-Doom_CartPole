{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390adbc2",
   "metadata": {},
   "source": [
    "## 1. Getting VizDoom up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41fbd6a2-79b9-4fdb-b138-01ba25aafc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec3301f-9627-44c9-abbe-ff3605e68762",
   "metadata": {},
   "source": [
    "## 2. Converting it to a Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae853175-8aca-4ab6-ac5f-5b0e8de82789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2\n",
    "\n",
    "# Import environment checker\n",
    "from stable_baselines3.common import env_checker\n",
    "# Discrete(3).sample() returns a number from 0, 1, 2 -> used as index to select action\n",
    "# Box(low=0, high=10, shape=(10,10)).sample() -> getting 10x10 array with low=0 and high=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86fd0b97-dfae-4ad4-98ab-7268c8db3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('VizDoom/scenarios/basic.cfg')\n",
    "        \n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 4) # get action using index -> left, right, shoot\n",
    "        \n",
    "        # Get all the other stuff we need to return \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            ammo = self.game.get_state().game_variables[0] \n",
    "            info = ammo\n",
    "        # If we dont have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "            \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer  # Apply Grayscale\n",
    "\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18351768-7387-4813-99d0-a2faea4dc26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d7861e-c8cc-4b61-8962-717cbfbb1ff4",
   "metadata": {},
   "source": [
    "Environment checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2e6ac7a-9294-40a3-82d5-cc9d2f6f9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82a3ba49-76d5-4c4a-817b-7f0dbf3ccc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8162f",
   "metadata": {},
   "source": [
    "## Hyperparameter Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f890390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeungivan/anaconda3/envs/Doom/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing the optimzation frame - HPO\n",
    "import optuna\n",
    "# PPO algo for RL\n",
    "from stable_baselines3 import PPO\n",
    "# Bring in the eval policy method for metric calculation\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Import the sb3 monitor for logging \n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# Import the vec wrappers to vectorize and frame stack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "# Import os to deal with filepaths\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f6c8cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/log_basic/tuning'\n",
    "OPT_DIR = './opt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12c14708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return test hyperparameters - define the object function\n",
    "def optimize_ppo(trial): \n",
    "    return {\n",
    "        'n_steps':trial.suggest_int('n_steps', 2048, 8192),\n",
    "        'learning_rate':trial.suggest_float('learning_rate', 1e-5, 1e-4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bab28920",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96dcd7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a training loop and return mean reward \n",
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        model_params = optimize_ppo(trial) \n",
    "\n",
    "        # Create environment \n",
    "        env = VizDoomGym()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "        # Create algo \n",
    "        model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        model.learn(total_timesteps=30000)\n",
    "        #model.learn(total_timesteps=100000)\n",
    "\n",
    "        # Evaluate model \n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "        env.close()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    except Exception as e:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6601c586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-08 17:41:45,738] A new study created in memory with name: no-name-153ffc2b-3c95-4455-868e-1780a06ff363\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Creating the experiment \n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent, n_trials=5, n_jobs=1)\n",
    "#study.optimize(optimize_agent, n_trials=100, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32f6b37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_steps': 6928,\n",
       " 'gamma': 0.8296119993363261,\n",
       " 'learning_rate': 2.296979511732029e-05,\n",
       " 'clip_range': 0.3688634008030219,\n",
       " 'gae_lambda': 0.8002547385297311}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6758e230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=0, state=TrialState.COMPLETE, values=[-1000.0], datetime_start=datetime.datetime(2024, 4, 8, 17, 30, 25, 419385), datetime_complete=datetime.datetime(2024, 4, 8, 17, 30, 25, 420239), params={'n_steps': 6928, 'gamma': 0.8296119993363261, 'learning_rate': 2.296979511732029e-05, 'clip_range': 0.3688634008030219, 'gae_lambda': 0.8002547385297311}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_steps': IntDistribution(high=8192, log=False, low=2048, step=1), 'gamma': FloatDistribution(high=0.9999, log=False, low=0.8, step=None), 'learning_rate': FloatDistribution(high=0.0001, log=False, low=1e-05, step=None), 'clip_range': FloatDistribution(high=0.4, log=False, low=0.1, step=None), 'gae_lambda': FloatDistribution(high=0.99, log=False, low=0.8, step=None)}, trial_id=0, value=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8503e-9e86-404e-9557-36db8c6cf6f2",
   "metadata": {},
   "source": [
    "## 3. Setup Callback\n",
    "Save model at different state of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd3a115d-fbab-449a-bd50-f19a8f3aac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for file nav\n",
    "import os\n",
    "# Import callback class from sb3\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "667e4204-553a-47eb-89d4-572b15f21eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        return True\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94544ad3-aea1-4559-95ab-8c0a30166e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_basic'\n",
    "LOG_DIR = './logs/log_basic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2a9d6be-2e1f-410f-9c40-e0e508cf86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of callback\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR) \n",
    "# after every 10000 steps of training the model, weights are saved for the pytorch agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0684619b-6997-4f27-9246-3eb05be026c9",
   "metadata": {},
   "source": [
    "## 5. Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2831ff36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_steps': 7488,\n",
       " 'gamma': 0.8296119993363261,\n",
       " 'learning_rate': 2.296979511732029e-05,\n",
       " 'clip_range': 0.3688634008030219,\n",
       " 'gae_lambda': 0.8002547385297311}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params = study.best_params\n",
    "model_params['n_steps'] = 7488  # set n_steps to 7488 or a factor of 64\n",
    "# model_params['learning_rate'] = 5e-7\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bedf5cea-9605-4739-97fa-719fe962cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PPO for training\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a93b07b6-5e9a-46d9-916c-7d918a5d4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non rendered environment\n",
    "env = VizDoomGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfb926b1-88b4-469e-973e-0d4d0f7b313b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# n_steps: How many steps/frames the agent is going to take and store in the buffer \n",
    "# before run through training of actor and critique\n",
    "# ideally not too close to end of game (300) but somewhere close\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8fa60-2db2-4183-a544-42117c76eff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/log_basic/PPO_2\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.1     |\n",
      "|    ep_rew_mean     | 19.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 11.1       |\n",
      "|    ep_rew_mean          | 48         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 21         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 193        |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03136115 |\n",
      "|    clip_fraction        | 0.315      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.795     |\n",
      "|    explained_variance   | 0.663      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 549        |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    value_loss           | 1.15e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.7        |\n",
      "|    ep_rew_mean          | 32.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 309         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018919863 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.753      |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 381         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 0.0148      |\n",
      "|    value_loss           | 862         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.36        |\n",
      "|    ep_rew_mean          | 57.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 434         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021261353 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.693      |\n",
      "|    explained_variance   | 0.684       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 328         |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00124    |\n",
      "|    value_loss           | 1.13e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.85        |\n",
      "|    ep_rew_mean          | 62.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 559         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042747393 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.671      |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 259         |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00536    |\n",
      "|    value_loss           | 448         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9           |\n",
      "|    ep_rew_mean          | 61.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 685         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040855393 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.661      |\n",
      "|    explained_variance   | 0.246       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 199         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.00397     |\n",
      "|    value_loss           | 593         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.67        |\n",
      "|    ep_rew_mean          | 63.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 812         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045351025 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.617      |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 141         |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    value_loss           | 421         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.71        |\n",
      "|    ep_rew_mean          | 68.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 942         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025766872 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 111         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.000666   |\n",
      "|    value_loss           | 412         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.56       |\n",
      "|    ep_rew_mean          | 75.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 17         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 1074       |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03965273 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.452     |\n",
      "|    explained_variance   | 0.526      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 128        |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | 0.000843   |\n",
      "|    value_loss           | 202        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.65        |\n",
      "|    ep_rew_mean          | 74.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 1206        |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025559738 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.414      |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 231         |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | 0.0186      |\n",
      "|    value_loss           | 160         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.82       |\n",
      "|    ep_rew_mean          | 73.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 1337       |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03031558 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.338     |\n",
      "|    explained_variance   | 0.493      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 32.4       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | 0.00276    |\n",
      "|    value_loss           | 141        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.52        |\n",
      "|    ep_rew_mean          | 75.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 1470        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031810477 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | 0.529       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 63.9        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | 0.0312      |\n",
      "|    value_loss           | 130         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.55        |\n",
      "|    ep_rew_mean          | 74.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 1607        |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040945582 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.246      |\n",
      "|    explained_variance   | 0.554       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 35.3        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 0.00806     |\n",
      "|    value_loss           | 115         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.49        |\n",
      "|    ep_rew_mean          | 75.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 1741        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019396544 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.226      |\n",
      "|    explained_variance   | 0.587       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 62.5        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.00726     |\n",
      "|    value_loss           | 93.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.17        |\n",
      "|    ep_rew_mean          | 77.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 1880        |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028692415 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.195      |\n",
      "|    explained_variance   | 0.541       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 70          |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | 0.00707     |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1         |\n",
      "|    ep_rew_mean          | 77.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 2017        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014054595 |\n",
      "|    clip_fraction        | 0.0903      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.16       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 42.9        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | 0.0115      |\n",
      "|    value_loss           | 67.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.87        |\n",
      "|    ep_rew_mean          | 79.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 2158        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019112844 |\n",
      "|    clip_fraction        | 0.0894      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.179      |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 43.3        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | 0.0139      |\n",
      "|    value_loss           | 90.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.07        |\n",
      "|    ep_rew_mean          | 78.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 2296        |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055685878 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.199      |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 8.89        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | 0.0273      |\n",
      "|    value_loss           | 37.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.16        |\n",
      "|    ep_rew_mean          | 77.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 2432        |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020560186 |\n",
      "|    clip_fraction        | 0.0816      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.172      |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 4.38        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | 0.00443     |\n",
      "|    value_loss           | 31.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.02      |\n",
      "|    ep_rew_mean          | 78        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 15        |\n",
      "|    iterations           | 20        |\n",
      "|    time_elapsed         | 2568      |\n",
      "|    total_timesteps      | 40960     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0587975 |\n",
      "|    clip_fraction        | 0.123     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.17     |\n",
      "|    explained_variance   | 0.767     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 6.94      |\n",
      "|    n_updates            | 300       |\n",
      "|    policy_gradient_loss | 0.022     |\n",
      "|    value_loss           | 36.9      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.85        |\n",
      "|    ep_rew_mean          | 78.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 2703        |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069684915 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.13       |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    value_loss           | 68.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.76       |\n",
      "|    ep_rew_mean          | 79.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 15         |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 2839       |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03872658 |\n",
      "|    clip_fraction        | 0.0782     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.123     |\n",
      "|    explained_variance   | 0.666      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 37.2       |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | 0.00969    |\n",
      "|    value_loss           | 51.3       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.42       |\n",
      "|    ep_rew_mean          | 75.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 15         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 2974       |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05854378 |\n",
      "|    clip_fraction        | 0.0864     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.112     |\n",
      "|    explained_variance   | 0.669      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 26.4       |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | 0.0176     |\n",
      "|    value_loss           | 59.8       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.12       |\n",
      "|    ep_rew_mean          | 81.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 15         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 3111       |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21400237 |\n",
      "|    clip_fraction        | 0.17       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.106     |\n",
      "|    explained_variance   | 0.401      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 77.1       |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | 0.00255    |\n",
      "|    value_loss           | 137        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.05      |\n",
      "|    ep_rew_mean          | 78.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 15        |\n",
      "|    iterations           | 25        |\n",
      "|    time_elapsed         | 3244      |\n",
      "|    total_timesteps      | 51200     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0811172 |\n",
      "|    clip_fraction        | 0.0931    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.119    |\n",
      "|    explained_variance   | 0.712     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 12.4      |\n",
      "|    n_updates            | 350       |\n",
      "|    policy_gradient_loss | 0.022     |\n",
      "|    value_loss           | 35.2      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.23       |\n",
      "|    ep_rew_mean          | 81.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 15         |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 3379       |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03779048 |\n",
      "|    clip_fraction        | 0.0799     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.126     |\n",
      "|    explained_variance   | 0.727      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 12.4       |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | 0.0163     |\n",
      "|    value_loss           | 43.3       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.72       |\n",
      "|    ep_rew_mean          | 79.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 15         |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 3513       |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08893949 |\n",
      "|    clip_fraction        | 0.0957     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.114     |\n",
      "|    explained_variance   | 0.524      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 47         |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    value_loss           | 77.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.12        |\n",
      "|    ep_rew_mean          | 77.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 3648        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038211904 |\n",
      "|    clip_fraction        | 0.0614      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0953     |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 5.34        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | 0.00438     |\n",
      "|    value_loss           | 28.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.92       |\n",
      "|    ep_rew_mean          | 78.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 15         |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 3784       |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05048291 |\n",
      "|    clip_fraction        | 0.0751     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0971    |\n",
      "|    explained_variance   | 0.701      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 12.9       |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | 0.0142     |\n",
      "|    value_loss           | 51.3       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.42       |\n",
      "|    ep_rew_mean          | 81         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 15         |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 3919       |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03774234 |\n",
      "|    clip_fraction        | 0.0799     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.105     |\n",
      "|    explained_variance   | 0.833      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 4.85       |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | 0.0161     |\n",
      "|    value_loss           | 18.5       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.8        |\n",
      "|    ep_rew_mean          | 79.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 15         |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 4055       |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02639904 |\n",
      "|    clip_fraction        | 0.0516     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0919    |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 2.9        |\n",
      "|    n_updates            | 410        |\n",
      "|    policy_gradient_loss | 0.0149     |\n",
      "|    value_loss           | 15.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.82        |\n",
      "|    ep_rew_mean          | 79.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 4191        |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013960399 |\n",
      "|    clip_fraction        | 0.0691      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0989     |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 26.4        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | 0.0109      |\n",
      "|    value_loss           | 23          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.22        |\n",
      "|    ep_rew_mean          | 77.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 4333        |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031469412 |\n",
      "|    clip_fraction        | 0.0703      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0784     |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 7.75        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | 0.00912     |\n",
      "|    value_loss           | 8.76        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.56        |\n",
      "|    ep_rew_mean          | 80.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 4469        |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030198297 |\n",
      "|    clip_fraction        | 0.0566      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0764     |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.74        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | 0.0243      |\n",
      "|    value_loss           | 9           |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.74        |\n",
      "|    ep_rew_mean          | 79.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 4608        |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009160726 |\n",
      "|    clip_fraction        | 0.0397      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0544     |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 4.56        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | 0.0111      |\n",
      "|    value_loss           | 22.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.13        |\n",
      "|    ep_rew_mean          | 78.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 4735        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019623447 |\n",
      "|    clip_fraction        | 0.0441      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0588     |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 9.31        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | 0.00379     |\n",
      "|    value_loss           | 27.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.56        |\n",
      "|    ep_rew_mean          | 70.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 4856        |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055567876 |\n",
      "|    clip_fraction        | 0.0707      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0694     |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.73        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | 0.0147      |\n",
      "|    value_loss           | 3.4         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.37       |\n",
      "|    ep_rew_mean          | 65.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 15         |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 4975       |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07101053 |\n",
      "|    clip_fraction        | 0.0827     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0654    |\n",
      "|    explained_variance   | 0.282      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 59.2       |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | 0.0137     |\n",
      "|    value_loss           | 130        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 5.73      |\n",
      "|    ep_rew_mean          | 79.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 15        |\n",
      "|    iterations           | 39        |\n",
      "|    time_elapsed         | 5103      |\n",
      "|    total_timesteps      | 79872     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0462906 |\n",
      "|    clip_fraction        | 0.0654    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.05     |\n",
      "|    explained_variance   | 0.828     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 79.5      |\n",
      "|    n_updates            | 490       |\n",
      "|    policy_gradient_loss | 0.00981   |\n",
      "|    value_loss           | 208       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.76        |\n",
      "|    ep_rew_mean          | 79.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 5231        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034566946 |\n",
      "|    clip_fraction        | 0.0523      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0431     |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 33.9        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.000967   |\n",
      "|    value_loss           | 149         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.7         |\n",
      "|    ep_rew_mean          | 79.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 5361        |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059216935 |\n",
      "|    clip_fraction        | 0.0551      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0475     |\n",
      "|    explained_variance   | 0.63        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 9.56        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | 0.00849     |\n",
      "|    value_loss           | 19.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.64       |\n",
      "|    ep_rew_mean          | 80.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 15         |\n",
      "|    iterations           | 42         |\n",
      "|    time_elapsed         | 5489       |\n",
      "|    total_timesteps      | 86016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07303353 |\n",
      "|    clip_fraction        | 0.0572     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0414    |\n",
      "|    explained_variance   | 0.671      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 15.3       |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | 0.00716    |\n",
      "|    value_loss           | 53.2       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:313\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:217\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mreset_noise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m--> 217\u001b[0m values, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/stable_baselines3/common/policies.py:730\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03mEvaluate actions according to the current policy,\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03mgiven the observations.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;124;03m    and entropy of the action distribution.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[0;32m--> 730\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m    732\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor(features)\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/stable_baselines3/common/policies.py:672\u001b[0m, in \u001b[0;36mActorCriticPolicy.extract_features\u001b[0;34m(self, obs, features_extractor)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m    features for the actor and the features for the critic.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[0;32m--> 672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/stable_baselines3/common/policies.py:131\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[0;34m(self, obs, features_extractor)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m:return: The extracted features\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m preprocessed_obs \u001b[38;5;241m=\u001b[39m preprocess_obs(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, normalize_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_images)\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeatures_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_obs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py:106\u001b[0m, in \u001b[0;36mNatureCNN.forward\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, observations: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Doom/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91761cd-2bce-4135-b448-769512a9ea2e",
   "metadata": {},
   "source": [
    "## 5. Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c2cdf8-4d70-48ae-a041-7a7e92a6d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import eval policy to test agent\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eef9c9-95cf-4f40-8f8e-a17e1be7c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model from disc\n",
    "model = PPO.load('./train/train_basic/best_model_110000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d13640f-2645-4a61-a3fc-228f0ad5bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rendered envrironment\n",
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c402d-6861-465f-bc9a-3e8d5125ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate mean reward for 10 games\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052916fc-1094-46f9-b13c-67dfc6bb88a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8eb004-51ee-4a3c-bd3d-8b2902d06525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 91.0 is 0\n",
      "Total Reward for episode 170.0 is 1\n",
      "Total Reward for episode 249.0 is 2\n",
      "Total Reward for episode 340.0 is 3\n",
      "Total Reward for episode 407.0 is 4\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for episode in range(5):\n",
    "    obs = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs) # Use model to predict what action to take\n",
    "        obs, reward, done, _, info = env.step(action) # take the predicted action\n",
    "        time.sleep(0.25)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(total_reward, episode))\n",
    "    time.sleep(2)\n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e316ba-b41a-4cce-861b-87f1441b4215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
