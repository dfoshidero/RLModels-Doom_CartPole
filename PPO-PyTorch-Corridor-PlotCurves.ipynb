{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0547f-535d-49a1-a0cb-2df24c584a95",
   "metadata": {},
   "source": [
    "## Initialize VizDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cddcc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary\n",
    "!pip install vizdoom\n",
    "!pip install opencv-python\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34143102-2b66-4ee2-9f76-f6db917d2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f8b51-d30c-4c9f-a8b1-8e24b891a576",
   "metadata": {},
   "source": [
    "## Make it a Gym Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5aed06-301c-43ff-a0ab-54dca32e78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2\n",
    "\n",
    "LEVEL = 'deadly_corridor'\n",
    "DOOM_SKILL = 's1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d02cc2-14b1-4eb2-a032-add0d7ed26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False, config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg'):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(config)\n",
    "        \n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(7)\n",
    "\n",
    "        # Game variables: HEALTH DAMAGE_TAKEN DAMAGECOUNT SELECTED_WEAPON_AMMO \n",
    "        ## We want the change in these variable values, rather than the PiT values\n",
    "        self.damage_taken = 0\n",
    "        self.damagecount = 0\n",
    "        self.ammo = 52\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(7, dtype=np.uint8)\n",
    "        # Movement rewards encapsulates predefined reward in the environment config\n",
    "        movement_reward = self.game.make_action(actions[action], 4) # get action using index -> left, right, shoot\n",
    "\n",
    "        reward = 0\n",
    "        # Get all the other stuff we need to return \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            # ammo = self.game.get_state().game_variables[0] \n",
    "\n",
    "            # Reward shaping\n",
    "            game_variables = self.game.get_state().game_variables # get current PiT game variables\n",
    "            health, damage_taken, damagecount, ammo = game_variables # unpack\n",
    "\n",
    "            # calculate change in damage_taken, hitcount, ammo\n",
    "            damage_taken_delta = -damage_taken + self.damage_taken # disincentivizng us to take damage\n",
    "            self.damage_taken = damage_taken\n",
    "            damagecount_delta = damagecount - self.damagecount # increments by +1: incentivizing more hitcounts (1 hitcount = 1 reward)\n",
    "            self.damagecount = damagecount\n",
    "            ammo_delta = ammo - self.ammo # increments by -1: disincentiving us to take shots that miss\n",
    "                                          # hitcount and ammo will cancel each other out\n",
    "            self.ammo = ammo\n",
    "\n",
    "            # Pack everything into reward function (tuned weights)\n",
    "            reward = movement_reward + damage_taken_delta*10 + damagecount_delta*200 + ammo_delta*5\n",
    "            \n",
    "            info = ammo\n",
    "        # If we dont have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "            \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer  # Apply Grayscale\n",
    "\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n",
    "\n",
    "    def run_model(self, observation):\n",
    "        \"\"\"\n",
    "        Run the PyTorch model on the observation to select an action.\n",
    "\n",
    "        Parameters:\n",
    "            observation (np.ndarray): The observation from the environment.\n",
    "\n",
    "        Returns:\n",
    "            int: The action selected by the model.\n",
    "        \"\"\"\n",
    "        # Preprocess the observation if necessary\n",
    "        # For example, if your model expects a specific input shape\n",
    "        \n",
    "        # Convert observation to torch tensor\n",
    "        observation = torch.tensor(observation, dtype=torch.float32)[0]\n",
    "        \n",
    "        # If necessary, move the observation to the correct device (e.g., GPU)\n",
    "        # observation = observation.to(device)\n",
    "        \n",
    "        # Run the model to get action logits\n",
    "        with torch.no_grad():\n",
    "            action_logits = model(observation)  # Assuming batch size of 1\n",
    "            \n",
    "        # Select the action with the highest probability\n",
    "        action = torch.argmax(action_logits).item()\n",
    "        \n",
    "        return action\n",
    "        \n",
    "# # Import PPO for training\n",
    "# from stable_baselines3 import PPO\n",
    "# # Non rendered environment\n",
    "# DOOM_SKILL = 's1'\n",
    "# env = VizDoomGym(config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg')\n",
    "# model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.00001, n_steps=8192, clip_range=.1, gamma=.95, gae_lambda=.9)\n",
    "\n",
    "# model.learn(total_timesteps=650000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48eacd6-a69d-4d14-b890-83f76c4a5e67",
   "metadata": {},
   "source": [
    "## Custom PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7728b4d-a680-4896-b0ea-a68a6683d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6422f-e84f-4457-99ee-81138a642536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size = 3, stride = 2, padding = 0):\n",
    "    return (size + 2 * padding - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "# Calculate output size after each convolutional layer\n",
    "h_w = [100, 160]  # initial height and width\n",
    "h_w = [conv2d_size_out(x, 8, 4) for x in h_w]  # after first conv layer\n",
    "h_w = [conv2d_size_out(x, 4, 2) for x in h_w]  # after second conv layer\n",
    "h_w = [conv2d_size_out(x, 3, 1) for x in h_w]  # after third conv layer\n",
    "\n",
    "# Calculate the total number of features before the linear layer\n",
    "feature_count = 64 * h_w[0] * h_w[1]\n",
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, n_output, gae_lambda):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Temporarily assume some output size after convolution\n",
    "        # This should ideally be calculated based on input size\n",
    "        self.feature_count = 64 * conv2d_size_out(conv2d_size_out(conv2d_size_out(100, 8, 4), 4, 2), 3, 1) * \\\n",
    "                        conv2d_size_out(conv2d_size_out(conv2d_size_out(160, 8, 4), 4, 2), 3, 1)\n",
    "\n",
    "        self.fc = nn.Linear(self.feature_count, 512)\n",
    "        self.actor = nn.Linear(512, n_output)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "        self.gae_lambda = gae_lambda \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(-1, self.feature_count)  # Use reshape instead of view\n",
    "        x = F.relu(self.fc(x))\n",
    "        action_probs = F.softmax(self.actor(x), dim=1)\n",
    "        value = self.critic(x)\n",
    "        return action_probs, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2518ed7-5d25-4e86-9a2a-936dd56cd604",
   "metadata": {},
   "source": [
    "## Implement helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce097a-06f8-4f30-9df0-cba0a3dbac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Calculate the returns using the rewards and the next state's value estimate.\n",
    "    \"\"\"\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def ppo_update(policy_net, optimizer, states, actions, log_probs_old, returns, advantages, clip_param=0.1):\n",
    "    \"\"\"\n",
    "    Perform one update step of the PPO algorithm.\n",
    "    \"\"\"\n",
    "    for _ in range(10):  # PPO epochs\n",
    "        # Get the log probabilities and state values from the model\n",
    "        log_probs, state_values = policy_net(states)\n",
    "        # Select the log probabilities for the actions taken\n",
    "        log_probs = log_probs.gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        # Calculate entropy to encourage exploration\n",
    "        entropy = -(log_probs * torch.exp(log_probs)).sum(1, keepdim=True).mean()\n",
    "\n",
    "        # Calculate the ratio (pi_theta / pi_theta_old) for the actions taken\n",
    "        ratios = torch.exp(log_probs - log_probs_old)\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1.0 - clip_param, 1.0 + clip_param) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = F.smooth_l1_loss(state_values.squeeze(-1), returns)\n",
    "        loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7664881-22e2-42a6-8ab9-693188f1d2c3",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5047951-d84a-42e4-94bf-a76a430830a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "def train(env, model, num_episodes, device, save_dir=\"./train/train_corridor\", save_interval=5):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "    model.to(device)\n",
    "    gamma = 0.95\n",
    "\n",
    "    episode_lengths = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    checkpoint_episode = []\n",
    "    checkpoint_avg_rewards = []\n",
    "    checkpoint_avg_lengths = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        print(episode)\n",
    "        state = env.reset()\n",
    "        state_array = np.array(state[0])\n",
    "        state = torch.from_numpy(state_array).float().unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        episode_length = 0\n",
    "\n",
    "        while not done:\n",
    "            policy_dist, value = model(state)\n",
    "            action = policy_dist.multinomial(num_samples=1).detach()\n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "            next_state = torch.from_numpy(next_state).float().unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "            total_reward += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            # Calculate and update loss here\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_lengths.append(episode_length)\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        # Save the model every save_interval episodes\n",
    "        if (episode + 1) % save_interval == 0:\n",
    "            print(f\"Episode {episode + 1}: Completed. Total Reward: {total_reward}\")\n",
    "            save_path = os.path.join(save_dir, f\"model_episode_{episode + 1}.pt\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Model saved at episode {episode + 1} to {save_path}\")\n",
    "\n",
    "            # Calculate running averages of episode length and reward\n",
    "            avg_episode_lengths = [np.mean(episode_lengths[:i + 1]) for i in range(episode + 1)]\n",
    "            avg_episode_rewards = [np.mean(episode_rewards[:i + 1]) for i in range(episode + 1)]\n",
    "\n",
    "            # Save data in checkpoint\n",
    "            checkpoint_episode.append(episode+1)\n",
    "            checkpoint_avg_rewards.append(avg_episode_rewards[-1])\n",
    "            checkpoint_avg_lengths.append(avg_episode_lengths[-1])\n",
    "            \n",
    "            # Clear previous output\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # Plotting\n",
    "            fig, ax1 = plt.subplots()\n",
    "\n",
    "            ax1.set_xlabel('Number of Episodes')\n",
    "            ax1.set_ylabel('Mean Episode Length', color='tab:blue')\n",
    "            ax1.plot(range(1, episode + 2), avg_episode_lengths, color='tab:blue',label='Episode Length')\n",
    "            ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.set_ylabel('Mean Reward per Episode', color='tab:red')\n",
    "            ax2.plot(range(1, episode + 2), avg_episode_rewards, color='tab:red',label='Episode Reward')\n",
    "            ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "            fig.tight_layout()\n",
    "            ax1.legend(loc='upper right')\n",
    "            ax2.legend(loc='upper left')\n",
    "            plt.title('Training Curve')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    return pd.DataFrame({\n",
    "       \"Episode\": checkpoint_episode,\n",
    "       \"Mean Reward\": checkpoint_avg_rewards,\n",
    "       \"Mean Episode Length\": checkpoint_avg_lengths\n",
    "    }) \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = VizDoomGym(render=False)\n",
    "    in_channels = 1  # Assuming grayscale input\n",
    "    n_actions = env.action_space.n\n",
    "    gae_lambda = .9\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ActorCriticNetwork(in_channels, n_actions, gae_lambda)\n",
    "    num_episodes = 10\n",
    "    training_data = train(env, model, num_episodes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1af7f09-a204-44f8-aa45-8c32dec12ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode</th>\n",
       "      <th>Mean Reward</th>\n",
       "      <th>Mean Episode Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Episode, Mean Reward, Mean Episode Length]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d58dc9-59aa-4d66-a30e-9ae41ab0fb72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
