{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0547f-535d-49a1-a0cb-2df24c584a95",
   "metadata": {},
   "source": [
    "## Initialize VizDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cddcc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary\n",
    "#!pip install vizdoom\n",
    "#!pip install opencv-python\n",
    "#!pip install pandas\n",
    "#!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34143102-2b66-4ee2-9f76-f6db917d2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f8b51-d30c-4c9f-a8b1-8e24b891a576",
   "metadata": {},
   "source": [
    "## Make it a Gym Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5aed06-301c-43ff-a0ab-54dca32e78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2\n",
    "\n",
    "LEVEL = 'defend_the_center'\n",
    "DOOM_SKILL = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d02cc2-14b1-4eb2-a032-add0d7ed26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('VizDoom/scenarios/defend_the_center.cfg')\n",
    "        \n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 4) # get action using index -> left, right, shoot\n",
    "        \n",
    "        # Get all the other stuff we need to return \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            ammo = self.game.get_state().game_variables[0] \n",
    "            info = ammo\n",
    "        # If we dont have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, reward, done, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "            \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer  # Apply Grayscale\n",
    "\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48eacd6-a69d-4d14-b890-83f76c4a5e67",
   "metadata": {},
   "source": [
    "## Custom PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7728b4d-a680-4896-b0ea-a68a6683d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39228483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    # define basic neural network layers (can also use convolution layers?)\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "\n",
    "        if isinstance(in_dim, int):\n",
    "            total_input_size = in_dim\n",
    "        else:\n",
    "            total_input_size = int(torch.prod(torch.tensor(in_dim)))  # Calculate the total number of elements in the shape tuple\n",
    "\n",
    "        self.layer1 = nn.Linear(total_input_size, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, out_dim)\n",
    "\n",
    "    # forward method to pass on neural network\n",
    "    \"\"\"\n",
    "    uses \"ReLU\" activation function?\n",
    "\n",
    "    this network module defines both the actor and the critic so will:\n",
    "    1. take in an observation\n",
    "    2. return an action OR return a value\n",
    "    \n",
    "    - observation is set as parameter\n",
    "    - network must be a tensor so should convert obs to a tensor first in case it is passed as numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Ensure the observation is a tensor\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "\n",
    "        if obs.dim() > 1 and obs.size(-1) != 16000:\n",
    "            obs = obs.view(-1)  # Only flatten if it's not already a flat vector per observation\n",
    "            \n",
    "        # Pass observation through the neural network\n",
    "        activation1 = F.relu(self.layer1(obs))\n",
    "        activation2 = F.relu(self.layer2(activation1))\n",
    "        output = self.layer3(activation2)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "TIMESTEPS_PER_BATCH = 100\n",
    "MAX_TIMESTEPS_PER_EPISODE = 1600\n",
    "GAMMA = 0.95\n",
    "N_UPDATES_PER_ITERATION = 5\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "from torch.optim import Adam\n",
    "#class\n",
    "class PPO:\n",
    "\n",
    "    \"\"\"\n",
    "    No information regarding input or output sizes, which can change depending on fed environment.\n",
    "\n",
    "    Solution: initialise it as an instance var in ppo class\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        # init hyperparameters\n",
    "        self._init_hyperparameters()\n",
    "        \n",
    "        # Extract environment information\n",
    "        self.env = env\n",
    "        self.obs_dim = env.observation_space.shape[0] if len(env.observation_space.shape) == 1 else env.observation_space.shape\n",
    "        self.act_dim = env.action_space.n\n",
    "\n",
    "        # Initialize episode counter\n",
    "        self.episode_number = 0  # This will keep track of the number of episodes\n",
    "\n",
    "        # ALG STEP 1\n",
    "        # Initialize actor and critic networks\n",
    "        self.actor = FeedForwardNN(self.obs_dim, self.act_dim)\n",
    "        self.critic = FeedForwardNN(self.obs_dim, 1)\n",
    "\n",
    "        \"\"\"\n",
    "        Need to explain what adam is doing in the report:\n",
    "        The Adam optimizer is an adaptive learning rate optimization algorithm commonly\n",
    "        used in deep learning that combines the advantages of AdaGrad and RMSprop to compute \n",
    "        and apply adaptive learning rates for each parameter during the training process.\n",
    "        (citations)\n",
    "        \"\"\"\n",
    "        #Initialise optimizer\n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=self.lr)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Define a for loop for some number of iterations. \n",
    "    ---> Should likely specify how many timesteps to train instead of counting to infinity.\n",
    "    \"\"\"\n",
    "    def learn(self, total_timesteps):\n",
    "        t_so_far = 0 # Timesteps simulated so far\n",
    "        while t_so_far < total_timesteps:              # ALG STEP 2\n",
    "           # ALG STEP 3\n",
    "            batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = self.rollout()\n",
    "            # Calculate how many timesteps we collected this batch   \n",
    "            t_so_far += np.sum(batch_lens)\n",
    "\n",
    "            \"\"\"\n",
    "            We will use the advantage function defined here:\n",
    "            https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions\n",
    "\n",
    "            --> Q^π is the Q-value of state action pair (s, a), and Vᵩₖ is the value of some observation s determined by our critic network following parameters Φ on the k-th iteration.\n",
    "\n",
    "            Though it is modified: \n",
    "            value predicted is following parameters Φ on the k-th iteration, as we'll need to recalculate V(s) following parameters Φ on the i-th epoch.\n",
    "            \"\"\"\n",
    "\n",
    "            # Calculate V_{phi, k}\n",
    "            V, _ = self.evaluate(batch_obs, batch_acts)\n",
    "        \n",
    "            # ALG STEP 5\n",
    "            # Calculate advantage\n",
    "            A_k = batch_rtgs - V.detach()  # we do V.detach() since V is a tensor with gradient required.\n",
    "        \n",
    "            # Normalize advantages\n",
    "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10) # we add 1e-10 to the standard deviation of the advantages, to avoid the possibility of dividing by 0.\n",
    "        \n",
    "            for _ in range(self.n_updates_per_iteration):\n",
    "                # Calculate V_phi and pi_theta(a_t | s_t)    \n",
    "                V, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
    "\n",
    "                # Calculate ratios\n",
    "                ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "\n",
    "                # Calculate surrogate losses\n",
    "                surr1 = ratios * A_k\n",
    "                surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
    "\n",
    "                # Calculate the actor loss \n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean() #taking the minimum between the 2 surrogate losses\n",
    "                critic_loss = nn.MSELoss()(V, batch_rtgs)      #calculate MSE of predicted values\n",
    "\n",
    "                # Calculate gradients and perform backward propagation for critic network    \n",
    "                self.critic_optim.zero_grad()    \n",
    "                critic_loss.backward()    \n",
    "                self.critic_optim.step()\n",
    "                # Calculate gradients and perform backward propagation for actor \n",
    "                # network\n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optim.step()\n",
    "\n",
    "    \"\"\"\n",
    "    need to collect data from a set of episodes by running our current actor policy\n",
    "    \n",
    "    ---> Can collect data in batches?\n",
    "\n",
    "    - To increment t_so_far in learn, the number of timesteps simulated per batch is necessary.\n",
    "    - Return the lengths of each episode run in our batch for future logging of average episodic length.\n",
    "    - Optionally, sum the episodic lengths before returning, based on preference.\n",
    "\n",
    "    Also have to:\n",
    "    - Determine the number of timesteps to run per batch, which will be treated as a hyperparameter.\n",
    "    - Create a function named to establish default hyperparameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def rollout(self):\n",
    "        \"\"\"\n",
    "        In batch we run episodes til we hit timesteps per batch. \n",
    "        Collect the observations, actions, probabilities of actions, rewards, rewards to-go, and lengths of each episode.\n",
    "        \"\"\"\n",
    "        # Batch data\n",
    "        batch_obs = []             # batch observations           (number of timesteps per batch, dimension of observation)\n",
    "        batch_acts = []            # batch actions                (number of timesteps per batch, dimension of action)\n",
    "        batch_log_probs = []       # log probs of each action     (number of timesteps per batch)\n",
    "        batch_rews = []            # batch rewards                (number of episodes, number of timesteps per episode)\n",
    "        batch_rtgs = []            # batch rewards-to-go          (number of timesteps per batch)\n",
    "        batch_lens = []            # episodic lengths in batch    (number of episodes)\n",
    "        \n",
    "        # sources explaining why we keep track of raw action probabilities:\n",
    "        # https://cs.stackexchange.com/questions/70518/why-do-we-use-the-log-in-gradient-based-reinforcement-algorithms\n",
    "        # https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient\n",
    "        # essentially, makes gradient ascient easier\n",
    "       \n",
    "\n",
    "        # Number of timesteps run so far this batch\n",
    "        t = 0 \n",
    "        while t < self.timesteps_per_batch:\n",
    "            # Rewards this episode\n",
    "            ep_rews = []\n",
    "            obs, _ = self.env.reset()\n",
    "            done = False\n",
    "            self.episode_number += 1  # Increment the class attribute for episode count\n",
    "            for ep_t in range(self.max_timesteps_per_episode):\n",
    "\n",
    "                # Increment timesteps ran this batch so far\n",
    "                t += 1\n",
    "                \n",
    "\n",
    "                # Collect observation\n",
    "                batch_obs.append(obs)\n",
    "\n",
    "                action, log_prob = self.get_action(obs)\n",
    "                obs, rew, done, _ = self.env.step(action)\n",
    "  \n",
    "                # Collect reward, action, and log prob\n",
    "                ep_rews.append(rew)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            # Collect episodic length and rewards\n",
    "            print(f\"Episode {self.episode_number}: Total Reward = {sum(ep_rews)}, Length = {len(ep_rews)}\")\n",
    "\n",
    "            batch_lens.append(ep_t + 1) # plus 1 because timestep starts at 0\n",
    "            batch_rews.append(ep_rews) \n",
    "\n",
    "        \"\"\"\n",
    "        convert our batch_obs, batch_acts, batch_log_probs, and batch_rtgs to tensors since we’ll need them in that form later to draw our computation graphs\n",
    "\n",
    "        also create some function that will compute the rewards to go of the batch rewards (step 4 of algo)\n",
    "        \"\"\"\n",
    "\n",
    "        # Reshape data as tensors in the shape specified before returning\n",
    "\n",
    "        batch_obs = [obs.reshape(-1) for obs in batch_obs]  # Flatten each observation\n",
    "        batch_obs_tensor = torch.tensor(batch_obs, dtype=torch.float)  # Convert list to tensor\n",
    "\n",
    "\n",
    "        batch_acts = torch.tensor(batch_acts, dtype=torch.float)\n",
    "        batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float)\n",
    "\n",
    "        # ALG STEP #4\n",
    "        batch_rtgs = self.compute_rtgs(batch_rews)\n",
    "        # Return the batch data\n",
    "        return batch_obs_tensor, batch_acts, batch_log_probs, batch_rtgs, batch_lens\n",
    "    \n",
    "    \"\"\"\n",
    "    Next we need to get an action.\n",
    "\n",
    "    This uses MULTIVARIATE NORMAL DISTRIBUTION.\n",
    "    ---> Essentially, actor will output a \"mean\" action on a forward pass, then create a covariance matrix with standard deviation.\n",
    "    ---> Mean is then used to generate a MND and then sample an action close to the mean.\n",
    "    \"\"\"\n",
    "    # source on multivariance normal distribution: https://cs229.stanford.edu/notes2021fall/cs229-notes2.pdf\n",
    "\n",
    "\n",
    "    # NOTE: actions will be deterministic when testing, meaning that the “mean” action will be our actual action during testing.\n",
    "    # NOTE: However, during training we need an exploratory factor, which this distribution can help us with.\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        # Get logits from the actor network\n",
    "        logits = self.actor(obs)\n",
    "\n",
    "        # Apply softmax to convert logits into probabilities\n",
    "        probs = F.softmax(logits, dim=-1)  # Ensure this matches the dimension of logits output\n",
    "        \n",
    "        # Create a categorical distribution and sample an action\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        # Get the log probability of the sampled action\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        # Return the sampled action and the log prob of that action\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    \n",
    "    def compute_rtgs(self, batch_rews):\n",
    "        # The rewards-to-go (rtg) per episode per batch to return.\n",
    "        # The shape will be (num timesteps per episode)\n",
    "        batch_rtgs = []\n",
    "        # Iterate through each episode backwards to maintain same order\n",
    "        # in batch_rtgs\n",
    "        for ep_rews in reversed(batch_rews):\n",
    "            discounted_reward = 0 # The discounted reward so far\n",
    "            for rew in reversed(ep_rews):\n",
    "                discounted_reward = rew + discounted_reward * self.gamma\n",
    "                batch_rtgs.insert(0, discounted_reward)\n",
    "        # Convert the rewards-to-go into a tensor\n",
    "        return torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "\n",
    "    \n",
    "\n",
    "    # default values for hyperparameters. can change in config\n",
    "    def _init_hyperparameters(self):\n",
    "        self.timesteps_per_batch = TIMESTEPS_PER_BATCH               # timesteps per batch\n",
    "        self.max_timesteps_per_episode = MAX_TIMESTEPS_PER_EPISODE   # timesteps per episode\n",
    "        self.gamma = GAMMA\n",
    "        self.n_updates_per_iteration = N_UPDATES_PER_ITERATION\n",
    "        self.clip = 0.2\n",
    "        self.lr = LEARNING_RATE\n",
    "\n",
    "    # function to evaluate V(s)\n",
    "    def evaluate(self, batch_obs, batch_acts):\n",
    "        # Query critic network for a value V for each obs in batch_obs.\n",
    "        V = self.critic(batch_obs).squeeze()\n",
    "\n",
    "        # Calculate the log probabilities of batch actions using most recent actor network.\n",
    "        logits = self.actor(batch_obs)\n",
    "        probs = F.softmax(logits, dim=-1)  # Apply softmax to convert logits into probabilities\n",
    "        dist = Categorical(probs)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "\n",
    "        # Return predicted values V and log probs\n",
    "        return V, log_probs\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7664881-22e2-42a6-8ab9-693188f1d2c3",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c273940",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)\n",
    "model = PPO(env)\n",
    "model.learn(10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
