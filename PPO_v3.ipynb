{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0547f-535d-49a1-a0cb-2df24c584a95",
   "metadata": {},
   "source": [
    "## Initialize VizDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cddcc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cuda-python\n",
      "  Using cached cuda_python-12.4.0-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Using cached cuda_python-12.4.0-cp310-cp310-win_amd64.whl (10.5 MB)\n",
      "Installing collected packages: cuda-python\n",
      "Successfully installed cuda-python-12.4.0\n"
     ]
    }
   ],
   "source": [
    "#necessary\n",
    "!pip install vizdoom\n",
    "!pip install opencv-python\n",
    "!pip install pandas\n",
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34143102-2b66-4ee2-9f76-f6db917d2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import VizDoom for game env\n",
    "from vizdoom import *\n",
    "# Import random for action sampling\n",
    "import random\n",
    "# Import time for sleeping\n",
    "import time\n",
    "# import numpy for identity matrix\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f8b51-d30c-4c9f-a8b1-8e24b891a576",
   "metadata": {},
   "source": [
    "## Make it a Gym Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df5aed06-301c-43ff-a0ab-54dca32e78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class from OpenAI Gym\n",
    "from gymnasium import Env\n",
    "# Import gym spaces\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "# Import Opencv for greyscaling observations\n",
    "import cv2\n",
    "\n",
    "LEVEL = 'deadly_corridor'\n",
    "DOOM_SKILL = 's1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d02cc2-14b1-4eb2-a032-add0d7ed26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VizDoom OpenAI Gym Environment\n",
    "class VizDoomGym(Env): \n",
    "    def __init__(self, render=False, config=f'VizDoom/scenarios/{LEVEL}_{DOOM_SKILL}.cfg'):\n",
    "        \"\"\"\n",
    "        Function called when we start the env.\n",
    "        \"\"\"\n",
    "\n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up game\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(config)\n",
    "        \n",
    "\n",
    "        # Whether we want to render the game \n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        # Start the game\n",
    "        self.game.init()\n",
    "        \n",
    "        # Create action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(7)\n",
    "\n",
    "        # Game variables: HEALTH DAMAGE_TAKEN DAMAGECOUNT SELECTED_WEAPON_AMMO \n",
    "        ## We want the change in these variable values, rather than the PiT values\n",
    "        self.damage_taken = 0\n",
    "        self.damagecount = 0\n",
    "        self.ammo = 52\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        How we take a step in the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify action and take step\n",
    "        actions = np.identity(7, dtype=np.uint8)\n",
    "        # Movement rewards encapsulates predefined reward in the environment config\n",
    "        movement_reward = self.game.make_action(actions[action], 4) # get action using index -> left, right, shoot\n",
    "\n",
    "        reward = 0\n",
    "        # Get all the other stuff we need to return \n",
    "        if self.game.get_state():  # if nothing is\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)  # Apply Grayscale\n",
    "            # ammo = self.game.get_state().game_variables[0] \n",
    "\n",
    "            # Reward shaping\n",
    "            game_variables = self.game.get_state().game_variables # get current PiT game variables\n",
    "            health, damage_taken, damagecount, ammo = game_variables # unpack\n",
    "\n",
    "            # calculate change in damage_taken, hitcount, ammo\n",
    "            damage_taken_delta = -damage_taken + self.damage_taken # disincentivizng us to take damage\n",
    "            self.damage_taken = damage_taken\n",
    "            damagecount_delta = damagecount - self.damagecount # increments by +1: incentivizing more hitcounts (1 hitcount = 1 reward)\n",
    "            self.damagecount = damagecount\n",
    "            ammo_delta = ammo - self.ammo # increments by -1: disincentiving us to take shots that miss\n",
    "                                          # hitcount and ammo will cancel each other out\n",
    "            self.ammo = ammo\n",
    "\n",
    "            # Pack everything into reward function (tuned weights)\n",
    "            reward = movement_reward + damage_taken_delta*10 + damagecount_delta*200 + ammo_delta*5\n",
    "            \n",
    "            info = ammo\n",
    "        # If we dont have anything turned from game.get_state\n",
    "        else:\n",
    "            # Return a numpy zero array\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            # Return info (game variables) as zero\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        truncated = False  # Assuming it's not truncated, modify if applicable\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Define how to render the game environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Function for defining what happens when we start a new game.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "            \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer  # Apply Grayscale\n",
    "\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    \n",
    "    def grayscale(self, observation):\n",
    "        \"\"\"\n",
    "        Function to grayscale the game frame and resize it.\n",
    "        observation: gameframe\n",
    "        \"\"\"\n",
    "        # Change colour channels \n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Reduce image pixel size for faster training\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100, 160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call to close down the game.\n",
    "        \"\"\"\n",
    "        self.game.close()\n",
    "\n",
    "    def run_model(self, observation):\n",
    "        \"\"\"\n",
    "        Run the PyTorch model on the observation to select an action.\n",
    "\n",
    "        Parameters:\n",
    "            observation (np.ndarray): The observation from the environment.\n",
    "\n",
    "        Returns:\n",
    "            int: The action selected by the model.\n",
    "        \"\"\"\n",
    "        # Preprocess the observation if necessary\n",
    "        # For example, if your model expects a specific input shape\n",
    "        \n",
    "        # Convert observation to torch tensor\n",
    "        observation = torch.tensor(observation, dtype=torch.float32)[0]\n",
    "        \n",
    "        # If necessary, move the observation to the correct device (e.g., GPU)\n",
    "        # observation = observation.to(device)\n",
    "        \n",
    "        # Run the model to get action logits\n",
    "        with torch.no_grad():\n",
    "            action_logits = model(observation)  # Assuming batch size of 1\n",
    "            \n",
    "        # Select the action with the highest probability\n",
    "        action = torch.argmax(action_logits).item()\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48eacd6-a69d-4d14-b890-83f76c4a5e67",
   "metadata": {},
   "source": [
    "## Custom PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7728b4d-a680-4896-b0ea-a68a6683d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24b6422f-e84f-4457-99ee-81138a642536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    # define basic neural network layers (can also use convolution layers?)\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "\n",
    "        if isinstance(in_dim, int):\n",
    "            total_input_size = in_dim\n",
    "        else:\n",
    "            total_input_size = int(torch.prod(torch.tensor(in_dim)))  # Calculate the total number of elements in the shape tuple\n",
    "\n",
    "        self.layer1 = nn.Linear(total_input_size, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, out_dim)\n",
    "\n",
    "    # forward method to pass on neural network\n",
    "    \"\"\"\n",
    "    uses \"ReLU\" activation function?\n",
    "\n",
    "    this network module defines both the actor and the critic so will:\n",
    "    1. take in an observation\n",
    "    2. return an action OR return a value\n",
    "    \n",
    "    - observation is set as parameter\n",
    "    - network must be a tensor so should convert obs to a tensor first in case it is passed as numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Ensure the observation is a tensor\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "\n",
    "        print(\"OBS Dim:\" + str(obs.dim()))\n",
    "        print(\"OBS Shape[0]:\" + str(obs.shape[0]))\n",
    "        print(\"OBS Shape[1]:\" + str(obs.shape[1]))\n",
    "        print(\"OBS Shape[2]:\" + str(obs.shape[2]))\n",
    "\n",
    "        # Check the dimensions and flatten if necessary\n",
    "        if obs.dim() == 3:  # Expected shape [height, width, channels]\n",
    "            obs = obs.view(-1)  # Flatten the tensor to a single dimension\n",
    "\n",
    "        print(\"OBS Dim:\" + str(obs.dim()))\n",
    "        print(\"OBS Shape[0]:\" + str(obs.shape[0]))\n",
    "\n",
    "        # Pass observation through the neural network\n",
    "        activation1 = F.relu(self.layer1(obs))\n",
    "        activation2 = F.relu(self.layer2(activation1))\n",
    "        output = self.layer3(activation2)\n",
    "        return output\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39228483",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 154 (19288602.py, line 155)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 155\u001b[1;36m\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'if' statement on line 154\n"
     ]
    }
   ],
   "source": [
    "TIMESTEPS_PER_BATCH = 4800\n",
    "MAX_TIMESTEPS_PER_EPISODE = 1600\n",
    "GAMMA = 0.95\n",
    "N_UPDATES_PER_ITERATION = 5\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "from torch.optim import Adam\n",
    "#class\n",
    "class PPO:\n",
    "\n",
    "    \"\"\"\n",
    "    No information regarding input or output sizes, which can change depending on fed environment.\n",
    "\n",
    "    Solution: initialise it as an instance var in ppo class\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        # init hyperparameters\n",
    "        self._init_hyperparameters()\n",
    "        \n",
    "        # Extract environment information\n",
    "        self.env = env\n",
    "        self.obs_dim = env.observation_space.shape[0] if len(env.observation_space.shape) == 1 else env.observation_space.shape\n",
    "        self.act_dim = env.action_space.n\n",
    "\n",
    "        # ALG STEP 1\n",
    "        # Initialize actor and critic networks\n",
    "        self.actor = FeedForwardNN(self.obs_dim, self.act_dim)\n",
    "        self.critic = FeedForwardNN(self.obs_dim, 1)\n",
    "\n",
    "        \"\"\"\n",
    "        Need to explain what adam is doing in the report:\n",
    "        The Adam optimizer is an adaptive learning rate optimization algorithm commonly\n",
    "        used in deep learning that combines the advantages of AdaGrad and RMSprop to compute \n",
    "        and apply adaptive learning rates for each parameter during the training process.\n",
    "        (citations)\n",
    "        \"\"\"\n",
    "        #Initialise optimizer\n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=self.lr)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Define a for loop for some number of iterations. \n",
    "    ---> Should likely specify how many timesteps to train instead of counting to infinity.\n",
    "    \"\"\"\n",
    "    def learn(self, total_timesteps):\n",
    "        t_so_far = 0 # Timesteps simulated so far\n",
    "        while t_so_far < total_timesteps:              # ALG STEP 2\n",
    "           # ALG STEP 3\n",
    "            batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = self.rollout()\n",
    "            # Calculate how many timesteps we collected this batch   \n",
    "            t_so_far += np.sum(batch_lens)\n",
    "\n",
    "            \"\"\"\n",
    "            We will use the advantage function defined here:\n",
    "            https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions\n",
    "\n",
    "            --> Q^π is the Q-value of state action pair (s, a), and Vᵩₖ is the value of some observation s determined by our critic network following parameters Φ on the k-th iteration.\n",
    "\n",
    "            Though it is modified: \n",
    "            value predicted is following parameters Φ on the k-th iteration, as we'll need to recalculate V(s) following parameters Φ on the i-th epoch.\n",
    "            \"\"\"\n",
    "\n",
    "            # Calculate V_{phi, k}\n",
    "            V, _ = self.evaluate(batch_obs, batch_acts)\n",
    "        \n",
    "            # ALG STEP 5\n",
    "            # Calculate advantage\n",
    "            A_k = batch_rtgs - V.detach()  # we do V.detach() since V is a tensor with gradient required.\n",
    "        \n",
    "            # Normalize advantages\n",
    "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10) # we add 1e-10 to the standard deviation of the advantages, to avoid the possibility of dividing by 0.\n",
    "        \n",
    "            for _ in range(self.n_updates_per_iteration):\n",
    "                # Calculate V_phi and pi_theta(a_t | s_t)    \n",
    "                V, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
    "\n",
    "                # Calculate ratios\n",
    "                ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "\n",
    "                # Calculate surrogate losses\n",
    "                surr1 = ratios * A_k\n",
    "                surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
    "\n",
    "                # Calculate the actor loss \n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean() #taking the minimum between the 2 surrogate losses\n",
    "                critic_loss = nn.MSELoss()(V, batch_rtgs)      #calculate MSE of predicted values\n",
    "\n",
    "                # Calculate gradients and perform backward propagation for critic network    \n",
    "                self.critic_optim.zero_grad()    \n",
    "                critic_loss.backward()    \n",
    "                self.critic_optim.step()\n",
    "                # Calculate gradients and perform backward propagation for actor \n",
    "                # network\n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optim.step()\n",
    "\n",
    "    \"\"\"\n",
    "    need to collect data from a set of episodes by running our current actor policy\n",
    "    \n",
    "    ---> Can collect data in batches?\n",
    "\n",
    "    - To increment t_so_far in learn, the number of timesteps simulated per batch is necessary.\n",
    "    - Return the lengths of each episode run in our batch for future logging of average episodic length.\n",
    "    - Optionally, sum the episodic lengths before returning, based on preference.\n",
    "\n",
    "    Also have to:\n",
    "    - Determine the number of timesteps to run per batch, which will be treated as a hyperparameter.\n",
    "    - Create a function named to establish default hyperparameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def rollout(self):\n",
    "        \"\"\"\n",
    "        In batch we run episodes til we hit timesteps per batch. \n",
    "        Collect the observations, actions, probabilities of actions, rewards, rewards to-go, and lengths of each episode.\n",
    "        \"\"\"\n",
    "        # Batch data\n",
    "        batch_obs = []             # batch observations           (number of timesteps per batch, dimension of observation)\n",
    "        batch_acts = []            # batch actions                (number of timesteps per batch, dimension of action)\n",
    "        batch_log_probs = []       # log probs of each action     (number of timesteps per batch)\n",
    "        batch_rews = []            # batch rewards                (number of episodes, number of timesteps per episode)\n",
    "        batch_rtgs = []            # batch rewards-to-go          (number of timesteps per batch)\n",
    "        batch_lens = []            # episodic lengths in batch    (number of episodes)\n",
    "        \n",
    "        # sources explaining why we keep track of raw action probabilities:\n",
    "        # https://cs.stackexchange.com/questions/70518/why-do-we-use-the-log-in-gradient-based-reinforcement-algorithms\n",
    "        # https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient\n",
    "        # essentially, makes gradient ascient easier\n",
    "       \n",
    "\n",
    "        # Number of timesteps run so far this batch\n",
    "        t = 0 \n",
    "        while t < self.timesteps_per_batch:\n",
    "            # Rewards this episode\n",
    "            ep_rews = []\n",
    "            obs, _ = self.env.reset()\n",
    "            done = False\n",
    "            for ep_t in range(self.max_timesteps_per_episode):\n",
    "\n",
    "                # Increment timesteps ran this batch so far\n",
    "                t += 1\n",
    "                # Collect observation\n",
    "                batch_obs.append(obs)\n",
    "\n",
    "                action, log_prob = self.get_action(obs)\n",
    "                obs, rew, done, _, _ = self.env.step(action)\n",
    "  \n",
    "                # Collect reward, action, and log prob\n",
    "                ep_rews.append(rew)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            # Collect episodic length and rewards\n",
    "            batch_lens.append(ep_t + 1) # plus 1 because timestep starts at 0\n",
    "            batch_rews.append(ep_rews) \n",
    "\n",
    "        \"\"\"\n",
    "        convert our batch_obs, batch_acts, batch_log_probs, and batch_rtgs to tensors since we’ll need them in that form later to draw our computation graphs\n",
    "\n",
    "        also create some function that will compute the rewards to go of the batch rewards (step 4 of algo)\n",
    "        \"\"\"\n",
    "        # Reshape data as tensors in the shape specified before returning\n",
    "        batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
    "        batch_acts = torch.tensor(batch_acts, dtype=torch.float)\n",
    "        batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float)\n",
    "\n",
    "        # ALG STEP #4\n",
    "        batch_rtgs = self.compute_rtgs(batch_rews)\n",
    "        # Return the batch data\n",
    "        return batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens\n",
    "    \n",
    "    \"\"\"\n",
    "    Next we need to get an action.\n",
    "\n",
    "    This uses MULTIVARIATE NORMAL DISTRIBUTION.\n",
    "    ---> Essentially, actor will output a \"mean\" action on a forward pass, then create a covariance matrix with standard deviation.\n",
    "    ---> Mean is then used to generate a MND and then sample an action close to the mean.\n",
    "    \"\"\"\n",
    "    # source on multivariance normal distribution: https://cs229.stanford.edu/notes2021fall/cs229-notes2.pdf\n",
    "\n",
    "\n",
    "    # NOTE: actions will be deterministic when testing, meaning that the “mean” action will be our actual action during testing.\n",
    "    # NOTE: However, during training we need an exploratory factor, which this distribution can help us with.\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        # Get logits from the actor network\n",
    "        logits = self.actor(obs)\n",
    "\n",
    "        # Apply softmax to convert logits into probabilities\n",
    "        probs = F.softmax(logits, dim=-1)  # Ensure this matches the dimension of logits output\n",
    "        \n",
    "        # Create a categorical distribution and sample an action\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        # Get the log probability of the sampled action\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        # Return the sampled action and the log prob of that action\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    \n",
    "    def compute_rtgs(self, batch_rews):\n",
    "        # The rewards-to-go (rtg) per episode per batch to return.\n",
    "        # The shape will be (num timesteps per episode)\n",
    "        batch_rtgs = []\n",
    "        # Iterate through each episode backwards to maintain same order\n",
    "        # in batch_rtgs\n",
    "        for ep_rews in reversed(batch_rews):\n",
    "            discounted_reward = 0 # The discounted reward so far\n",
    "            for rew in reversed(ep_rews):\n",
    "                discounted_reward = rew + discounted_reward * self.gamma\n",
    "                batch_rtgs.insert(0, discounted_reward)\n",
    "        # Convert the rewards-to-go into a tensor\n",
    "        return torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "\n",
    "    \n",
    "\n",
    "    # default values for hyperparameters. can change in config\n",
    "    def _init_hyperparameters(self):\n",
    "        self.timesteps_per_batch = TIMESTEPS_PER_BATCH               # timesteps per batch\n",
    "        self.max_timesteps_per_episode = MAX_TIMESTEPS_PER_EPISODE   # timesteps per episode\n",
    "        self.gamma = GAMMA\n",
    "        self.n_updates_per_iteration = N_UPDATES_PER_ITERATION\n",
    "        self.clip = 0.2                                              # As recommended by the paper\n",
    "        self.lr = LEARNING_RATE                                              # As recommended by the paper learning rate\n",
    "\n",
    "    # function to evaluate V(s)\n",
    "    def evaluate(self, batch_obs, batch_acts):\n",
    "        # Query critic network for a value V for each obs in batch_obs.\n",
    "        V = self.critic(batch_obs).squeeze()\n",
    "        \"\"\"\n",
    "        we perform a squeeze operation here on our returned tensor from a forward pass on our critic network to change the dimensionality of the tensor.\n",
    "\n",
    "        Since batch_obs retains the shape (timesteps per batch, dimension of observation), \n",
    "        the tensor returned from passing batch_obs into our critic network is (timesteps per batch, 1), \n",
    "        whereas the shape we want is just (timesteps per batch).\n",
    "        \"\"\"\n",
    "\n",
    "        # Instead of defining a whole new subroutine to calculate log probs, let’s do it in evaluate.\n",
    "\n",
    "        # Calculate the log probabilities of batch actions using most recent actor network.\n",
    "        # This segment of code is similar to that in get_action()\n",
    "        action_probs = self.actor(batch_obs)\n",
    "        dist = Categorical(action_probs)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "        # Return predicted values V and log probs log_probs\n",
    "        return V, log_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7664881-22e2-42a6-8ab9-693188f1d2c3",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c273940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "OBS Dim:3\n",
      "OBS Shape[0]:100\n",
      "OBS Shape[1]:160\n",
      "OBS Shape[2]:1\n",
      "OBS Dim:1\n",
      "OBS Shape[0]:16000\n",
      "[0.0, 0.0, 0.0, -6.9005279541015625, -9.0758056640625, 0.0, 0.0, -0.000732421875, 0.0, 0.0, 0.0, 0.0, 0.0, -380.00086975097656, 0.0, -80.0, 195.0, -0.0023040771484375, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, -5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.013458251953125, 0.0, -4.4116363525390625, 0.70098876953125, 0.47271728515625, -4.6812591552734375, 0.0619049072265625, 0.0, -60.722991943359375, -6.111663818359375, 6.516021728515625, 8.312591552734375, 5.88665771484375, 4.10003662109375, -2.234588623046875, 1.865203857421875, 1.750091552734375, 1.4363861083984375, 0.968780517578125, -2.264373779296875, -0.142059326171875, -141.68345642089844, -8.697830200195312, -6.1843719482421875, -2.4195098876953125, 0.305572509765625, 0.2059783935546875, -0.4546661376953125, 8.587844848632812, 2.28460693359375, 3.6333160400390625, 4.764678955078125, 8.188980102539062, 2.1078948974609375, -1.2340087890625, -0.832427978515625, -0.5615997314453125, -206.41702270507812, -13.08148193359375, -12.555007934570312, -6.363525390625, 1.5207061767578125, -7.105316162109375, -8.52398681640625, -3.64453125, 4.6453704833984375, 3.1332855224609375, 2.1133575439453125, -3.5746002197265625, 7.57232666015625, 13.503890991210938, 18.083038330078125, 15.204818725585938, 10.25579833984375, 13.390762329101562, 18.842056274414062, 0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m VizDoomGym(render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(env)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 50\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps)\u001b[0m\n\u001b[0;32m     47\u001b[0m t_so_far \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# Timesteps simulated so far\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m t_so_far \u001b[38;5;241m<\u001b[39m total_timesteps:              \u001b[38;5;66;03m# ALG STEP 2\u001b[39;00m\n\u001b[0;32m     49\u001b[0m    \u001b[38;5;66;03m# ALG STEP 3\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Calculate how many timesteps we collected this batch   \u001b[39;00m\n\u001b[0;32m     52\u001b[0m     t_so_far \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(batch_lens)\n",
      "Cell \u001b[1;32mIn[9], line 155\u001b[0m, in \u001b[0;36mPPO.rollout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Collect episodic length and rewards\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ep_rews)\n\u001b[1;32m--> 155\u001b[0m     batch_lens\u001b[38;5;241m.\u001b[39mappend(\u001b[43mep_rews\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m) \u001b[38;5;66;03m# plus 1 because timestep starts at 0\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     batch_rews\u001b[38;5;241m.\u001b[39mappend(ep_rews) \n\u001b[0;32m    158\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03mconvert our batch_obs, batch_acts, batch_log_probs, and batch_rtgs to tensors since we’ll need them in that form later to draw our computation graphs\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03malso create some function that will compute the rewards to go of the batch rewards (step 4 of algo)\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "env = VizDoomGym(render=True)\n",
    "model = PPO(env)\n",
    "model.learn(10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
